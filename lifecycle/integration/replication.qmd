# Replication {.unnumbered}



```{r}
#| echo: false
#| include: false
#| purl:  false
source("scripts/before_chapter_script.R")
```

After your study is completed, it may one day be replicated. By replication we mean collecting new data to study the same inquiry. A new model, data strategy, or answer strategy may also be proposed. 

So-called "exact" replications hold key features of *I*, *D*, and *A* fixed, but draw a new dataset from the data strategy and apply the same answer strategy *A* to the new data to produce a fresh answer. Replications are said to "succeed" when the new and old answer are similar and to "fail" when they are not. Dichotomizing replication attempts into successes and failures is usually not that helpful, and it would be better to simply characterize how similar the old and new answers are. Literally exact replication is impossible: at least some elements of $m^*$ have changed between the first study and the replication. Specifying how they might have changed, e.g., how outcomes vary with time, will help judge differences observed between old and new answers.

Replication studies can benefit enormously from the knowledge gains produced by the original studies. For example, we learn a large amount to inform the construction of *M* and we learn the value of the inquiry from the original study. The *M* of the replication study can and should incorporate this new information. For example, if we learn from the original study that the estimand is positive, but it might be small, the replication study could respond by changing *D* to increase the sample size. Design diagnosis can help you learn about how to change the replication study's design in light of the original research.

When changes to the data strategy *D* or answer strategy *A* can be made to produce more informative answers about the same inquiry *I*, exact replication may not be preferred. Holding the treatment and outcomes the same may be required to provide an answer to the same *I*, but increasing the sample size or sampling individuals rather than villages or other changes may be preferable to exact replication. Replication designs can also take advantage of new best practices in research design.

So-called "conceptual" replications alter both *M* and *D*, but keep *I* and *A* as similar as possible. That is, a conceptual replication tries to ascertain whether a relationship in one context also holds in a new context. The trouble and promise of conceptual replications lie in the designer's success at holding *I* constant. Too often, a conceptual replication fails because in changing *M*, too much changes about *I*, muddying the "concept" under replication.

A summary function is needed to interpret the difference between the original answer and the replication answer. This function might take the new one and throw out the old if design was poor in the first. It might be taking the average. It might be a precision-weighted average. Specifying this function ex ante may be useful to avoid the choice of summary depending on the replication results. This summary function will be reflected in *A* and in the discussion section of the replication paper. 

### Example

Here we have an original study design of size 1000. The original study design's true sample average treatment effect (SATE) is 0.2 because the original authors happened to study a very treatment-responsive population. We seek to replicate the original results, whatever they may be. We want to characterize the probability of concluding that we "failed" to replicate the original results. We have four alternative metrics for assessing replication failure.

1. Are the original and replication estimates statistically significantly different from each other? If the difference-in-SATEs is significant, we conclude that we failed to replicate the original results, and if not, we conclude that the study replicated.

2. Is the replication estimate within the original 95% confidence interval?

3. Is the original estimate within the replication 95% confidence interval?

4. Do we fail to affirm equivalence^[For an introduction to equivalence testing see @hartman2018equivalence] between the replication and original estimate, using a tolerance of 0.2? 

Figure @fig-replications shows that no matter how big we make the replication, we find that the rate of concluding the difference-in-SATEs is nonzero only occurs about 10% of the time. Similarly, the replication estimate is rarely outside of the original confidence interval, because it's rare to be more extreme than a wide confidence interval. The relatively high variance of the original study means that it is so uncertain, it's tough to distinguish it from any number in particular. 

Turning to the third metric (is the original outside the 95% confidence interval of the replication estimate), we that we become more and more likely to conclude that the original study fails to replicate as the quality replication study goes up. At very large sample sizes, the replication confidence intervals become extremely small, so in the limit, it will always exclude the original study estimate.

The last metric, equivalence testing, has the nice property that as the sample size grows, we get closer to the correct answer -- the true SATEs are indeed within 0.2 standard units of each other. However, again because the original study is so noisy, it is difficult to affirm its equivalence with anything, even when the replication study is quite large.

The upshot of this exercise is that, curiously, when original studies are weak (in that they generate imprecise estimates), it becomes *harder* to conclusively affirm that they did not replicate. This set of incentives is somewhat perverse: designers of original studies benefit from a lack of precision if it means they can't "fail to replicate."

![Rates of 'failure to replicate' according to four diagnosands. Original study N = 1000; True original SATE: 0.2; True replication SATE: 0.15.](/figures/figure-23-2){#fig-ch23fig2}


<!-- 

TODO: hidden 


**Further reading**.

- @Clemens2017 on distinctions between replication and reanalysis
- @aac4716 on estimating the reproducibility of psychological science, see @Gilbert1037 for a response
- @King1995



-->