# Archiving {.unnumbered}


```{r}
#| echo: false
#| include: false
#| purl:  false
source("scripts/before_chapter_script.R")
```

One of the biggest successes in the push for greater research transparency has been changing norms surrounding data sharing and analysis code after studies have been published. Many journals now require authors to post these materials at publicly-available repositories like the OSF or Dataverse. This development is undoubtedly a good thing. In older manuscripts, sometimes data or analyses are described as being "available upon request," but of course, such requests are sometimes ignored (93% were ignored in a recent attempt to request 1,792 such  datasets, @GABELICA2022). Furthermore, a century from now, study authors will no longer be with us even if they wanted to respond to such requests. Public repositories have a much better chance of preserving study information for the future, especially if they are actively maintained [@peer_orr_coppock_2021].

What belongs in a replication archive? Enough documentation, data, and design detail that those who wish to reanalyze, replicate, and meta-analyze results can do so without contacting the authors.

**Data.** First, the realized data $d$ itself. Sometimes this is the raw data. Sometimes it is only the "cleaned" data that is actually used by analysis scripts. Where ethically possible, we think it is preferable to post as much of the raw data as possible after removing information like IP addresses and geographic locations that could be used to identify subjects. The output of cleaning scripts -- the cleaned data -- should also be included in the replication archive. 

Reanalyses often reexamine and extend studies by exploring the use of alternative outcomes, varying sets of control variables, and new ways of grouping data. As a result, replication data ideally includes all data collected by the authors even if the variables are not used in the final published results. Sometimes authors exclude these to preserve their own ability to publish on these other variables or because they are worried alternative analyses will cast doubt on their results. We hope norms will change such that study authors instead want to enable future researchers to build on their research by being expansive in what information is shared.

**Analysis code.** Replication archives also include the answer strategy *A*, or the set of functions that produce results when applied to the data. We need the actual analysis code because the natural-language descriptions of *A* that are typically given in written reports are imprecise. As a small example, many articles describe their answer strategies as "ordinary least squares" but do not fully describe the set of covariates included or the particular approach to variance estimation. These choices can substantively affect the quality of the research design -- and nothing makes these choices explicit like the actual analysis code. Analysis code is needed not only for reanalysis but also replication and meta-analysis. Replication practice today involves inferring most of these details from descriptions in text. Reanalyses may directly reuse or modify analysis code and replication projects need to know the exact details of analyses to ensure they can implement the same analyses on the data they collect. Meta-analysis authors may take the estimates from the past studies directly, so understanding the exact analysis procedure conducted is important. Other times, meta-analyses reanalyze data to ensure comparability in estimation. Conducting analyses with and without covariates, with clustering when it was appropriate, or with a single statistical model when they vary across studies all require having the exact analysis code. 

To the extent possible we encourage you to think of analysis code as being a data-in data-out function: a function that takes in your dataset---or a future replication dataset---implements analysis and reports a dataset containing answers: estimates and estimates of uncertainty. 

**Data strategy materials.** Increasingly, replication archives include the materials needed to implement treatments and measurement strategies. Without the survey questionnaires in their original languages and formats, we cannot exactly replicate them in future studies, hindering our ability to build on and adapt them. The treatment stimuli used in the study should also be included. Data strategies are needed for reanalyses and meta-analyses too: answer strategies should respect data strategies, so understanding the details of sampling, treatment assignment, and measurement can shape reanalysts' decisions and meta-analysis authors' decisions about what studies to include and which estimates to synthesize. 

To the extent possible we encourage you to describe data strategies also as data-in data-out function. Functions that take in information about a known context, and use this, together with parameters that characterize your strategy, return a dataset similar in structure to the data you generated. 

**Design declaration.** While typical replication archives include the data and code, we think that future replication archives should also have a design declaration that fully describes *M*, *I*, *D*, and *A*. This declaration should be done in code and words. A diagnosis can also be included, demonstrating the properties as understood by the author and indicating the diagnosands that the author considered in judging the quality of the design. 

Design details help future scholars not only assess, but replicate, reanalyze, and extend the study. Reanalysts need to understand the answer strategy to modify or extend it and the data strategy used to ensure that their new analysis respects the details of the sampling, treatment assignment, and measurement procedures. Data and analysis sharing enables reanalysts to adopt or adapt the analysis strategy, but a declaration of the data strategy would help more. The same is true of meta-analysis authors, who need to understand the designs' details to make good decisions about which studies to include and how to analyze them. Replicators who wish to exactly replicate or even just provide an answer to the same inquiry need to understand the inquiry, data strategy, and answer strategy. 

Figure \@ref(fig:filestructure) below shows the file structure for an example replication. Our view on replication archives shares much in common with the [TIER protocol](https://www.projecttier.org) and the proposals in @alvarez2018. It includes raw data in a platform-independent format (.csv) and cleaned data in a language-specific format (.rds, a format for R data files). Data features like labels, attributes, and factor levels are preserved when imported by the analysis scripts. The analysis scripts are labeled by the outputs they create, such as figures and tables. A master script is included that runs the cleaning and analysis scripts in the correct order. The documents folder consists of the paper, the supplemental appendix, the pre-analysis plan, the populated analysis plan, and codebooks that describe the data. A README file explains each part of the replication archive. We also suggest that authors include a script that consists of a design declaration and diagnosis. @Bowers2011 offers one reason above and beyond research transparency to go to all this effort: good archiving is like collaborating with your future self.

```{r filestructure, echo = FALSE, fig.cap = "File structure for archiving", fig.width = 6.5}
imgsufx <- if(knitr::is_html_output()) "svg" else "pdf"
knitr::include_graphics(path = paste0("figures/file_structure.", imgsufx))
```

