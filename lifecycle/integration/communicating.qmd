# Communicating

```{r}
#| echo: false
#| include: false
#| purl:  false
source("scripts/before_chapter_script.R")
```

The findings from studies are communicated to other scholars through academic publications. But some of the most important audiences -- policymakers, businesses, journalists, and the public at large -- do not read academic journals. These audiences learn about the study in other in other ways, through  op-eds, blog posts, and policy reports that translate research for nonspecialist audiences.  

Too often, a casualty of translating the study from academic to other audiences is the design information. Emphasis gets placed on the study results, not on the reasons why the results of the study are to be believed. In sharing the research for nonspecialist audiences, we revert to saying *that* we think the findings are true and not *why* we think the findings are true. Explaining why requires explaining the research design, which in our view ought to be part of any public-facing communication about research.

Looking at recent studies published in *The New York Times* Well section on health and fitness, we found that two dimensions of design quality were commonly ignored. First, experimental studies on new fitness regimens with very small samples, sometimes fewer than 10 units, are commonly highlighted. When both academic journals and reporters promote tiny studies, the likely result is that the published (and public) record contains many statistical flukes; results reflecting noise rather than new discoveries. Second, very large studies that draw observational comparisons between large samples of dieters and non-dieters with millions of observations receive outsize attention. These designs are prone to bias from confounding, but these concerns are too often not described or discussed. 

How can we improve scientific communication so that we better communicate the credibility of findings? The market incentives for both journalists and authors reward striking and surprising findings, so any real solution to the problem would likely requires addressing those incentives. Short of that, we recommend that authors who wish to communicate the high quality of their designs to the media do so by providing the design information in *M*, *I*, *D*, and *A* in lay terms. Science communicators can  state the research question (*I*) and explain why applying the data and answer strategies is likely to yield a good answer to the question. The actual result is, of course, also important to communicate, but *why* it is a credible answer to the research question is just as important to share---specifically what has to be believed about *M* for the results to be on target (Principle \@ref(exm:designtoshare): Design to share).

How can we as researchers communicate about other scholars' work?  Citations can't covey the entirety of *MIDA* in one sentence, but they can give an inkling. Here's an example of how we could cite a (hypothetical) study in a way that conveys at least some design information. "<span style="color:#81AFEF">Using a randomized experiment</span>, the researchers (Authors, Year) found that <span style="color:#DC5D86">donating to a campaign causes a large increase in the number of subsequent donation requests from other candidates</span>, which is consistent with <span style="color:#E6C560">theories of party behavior that predict</span><span style="color:#8DBA4C">  intra-party cooperation."</span>. 

The citation explains that the <span style="color:#81AFEF">data strategy</span> included some kind of randomized experiment (we don't know how many treatment arms or subjects, among other details), and that the <span style="color:#DC5D86">answer strategy</span> probably compared the counts of donation requests from any campaign (email requests, or phone, we don't know) among the groups of subjects that were assigned to donate to a particular campaign. The citation mentions the <span style="color:#E6C560">models</span> described in an unspecified area of the scientific literature on party politics, which all predict cooperation like the sharing of donor lists. We can reason that, if the <span style="color:#8DBA4C">inquiry</span>, "Is the population average treatment effect effect of donating to one campaign on the number of donation requests from other campaigns positive?" were put to each of these theories, they would all respond "Yes." The citation serves as a useful shorthand for the reader of what the claim of the paper is and why they should think it's credible. By contrast, a citation like "The researchers found that party members cooperate (Author, Year)." doesn't communicate any design information at all.  

<!-- inquiry, green, #8DBA4C -->
<!-- data strategy, blue, ##81AFEF -->
<!-- model, yellow, #E6C560 -->
<!-- answer strategy, red, #DC5D86 -->
