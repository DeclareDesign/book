# Pivoting 


```{r}
#| echo: false
#| include: false
#| purl:  false
source("scripts/before_chapter_script.R")
```

When something goes wrong or you learn things work differently from what you expected, you need to pivot. You face two decisions: go/no-go, and if go, should you alter your research design to account for the new realities? Redesigning the study and diagnosing the possible new designs can help you make these decisions. 

We illustrate with three real choices we had to make, one in which criticism to a design by participants led to a simple improvement to the assignment mechanism ($D$) and two in which difficulties implementing assignments led to a change in inquiries ($I$) but for quite different reasons.

The first study was one in which one of us, in coordination with political parties in Uganda, a random selection of MPs were to be selected into a pilot political communications program. The design called for simple random assignment. Shortly before implementation, however, the political parties complained that the randomization scheme was one that could produce inequalities between the parties, with some parties getting more benefits than others just because of the luck of the draw. They asked whether it would be possible to "fix" things so that each party would have the same share participating. 

The answer of course was yes: a change in $D$ to employ blocked random assignment addressed the fairness concerns of the party but also led to a demonstrably better design. In the final design, members of each party pulled names out of a hat that contained names only of people from their own party. This is the rare pivot that both attends to an unanticipated complaint and improves the design in the offing.

The second study was one in which another of us faced a serious noncompliance problem. We launched a cluster-randomized, placebo-controlled 2x2 factorial trial in Nigeria of a film treatment and a text message blast treatment. Treatment was to be rolled out in 200 communities. A few days after treatment delivery began, we noticed that the number of replies was extremely similar in treatment and placebo communities, counter to expectations. We discovered that our research partner, the cell phone company, had delivered the treatment message to both groups! By that time, treatments had been delivered to 106 communities (about half the sample).  

We faced the choice to abandon the study or pivot and adapt the study. We quickly agreed that we could not continue research in the 106 communities, because they had received at least partial treatment. We were left with 109 from our original sample of 200 plus 15 alternates that were selected in the same random sampling process. We determined we could not retain all four treatment conditions and the pure control. We decided that at most we could have two conditions, with about 50 units in each. But which ones? We were reticent to lose the text message or the film treatments, as both tested two distinct theoretical mechanisms for how to encourage prosocial behaviors. We decided to drop the pure control group, the fifth condition, as well as the placebo text message condition. In this way, we could learn about the effect of the film (compared to placebo) and about the effect of the text messages (compared to none).^[We randomized half of the communities to receive the treatment film and half the placebo. We then used an over-time stepped-wedge design to study the effect of the text message, randomizing how many days after the film was distributed the text message was sent.] Thus we had to reduce the size of our inquiry set. Essentially we ended up salvaging a subpart of our design without having to materially change any design elements *within* this subpart.

Finally, one of us was involved with, a get-out-the-vote canvassing-experiment-gone-wrong during the 2014 Senate race in New Hampshire. We randomly assigned 4,230 of 8,530 subjects to treatment. However, approximately two weeks before the election, canvassers had only attempted 746 subjects (17.6\% of the treatment group) and delivered treatment to just 152 subjects (3.6\%). In essence, the implementer had been overly optimistic about the number of subjects they would be able to contact in time for the election. In their revised assessment the organization estimated that they would only be able to attempt to contact 900 more voters. They also told us also that they believed that their efforts would be best spent on voters with above-median vote propensities.

We faced a choice: should we (1) stick to our design and continue trying to treat 900 of the remaining subjects that were allocated to treatment, knowing that we will have many non-compliers in this set or (2) alter *D* to conduct a whole new random assignment among above-median propensity voters only. A design diagnosis reveals a clear course of action. Even though it decreases the overall sample size, restricting the study to the above-median propensity voters substantially increases the precision of the design.^[This conclusion follows the logic of the placebo-controlled design described in @sec-ch18s7.] Opting for this modification to *D* required thinking through whether we were willing to accept a change in *I* since the set of compliers for which we could generate estimates is different under the two designs.

Pivoting is sometimes hard and sometimes easy but, very often, assessing whether or how to pivot requires thinking through the full design to see which parts have to change as others change. If you do this through redesign your design becomes a living document and becomes a tool to guide you along the research path, not just as a document to write at the beginning of the study and revisit when you are writing up.
