# Conjoint experiments  {.unnumbered}

```{r}
#| echo: false
#| include: false
#| purl:  false
source("scripts/before_chapter_script.R")
```

:::: {.ddbox}
We declare a forced-choice conjoint experiment design in which respondents choose one of two profiles in three sets of tasks, each with three attributes. The design highlights the complexity of defining the inquiries for conjoints, and their lower power at common sample sizes. 
::::

Conjoint survey experiments have become hugely popular in political science and beyond for describing multidimensional preferences over profiles [@Hainmueller2014]. The designs have been used to study preferences over political candidates, types of immigrants to admit, neighborhoods to live in, policies to select, and many more questions. Conjoint experiments come in two basic varieties: the single profile design and the forced-choice design. Throughout this chapter, we'll discuss these studies in the context of hypothetical candidate experiments, in which candidates are described in terms of a number of attributes each of which can take on multiple values, known as levels. In the single profile design, subjects are asked to rate one profile at a time using, for example, a 1 - 7 support scale. In a forced-choice conjoint experiment, subjects are shown two profiles at a time, then asked to make a binary choice between them. Forced choice conjoint experiments are especially useful for studying electoral behavior because they closely mirror the real-world behavior of choosing between two candidates at the ballot box. A similar logic applies to purchasing behavior when consumers have to choose one product over another. Occasionally, forced-choice conjoint experiments are applied even when no real-world analogue for the binary choice exists. For example, we rarely face a binary choice between two immigrants or between two refugees.

We take the unorthodox position that conjoint experiments target descriptive, rather than casual inquiries. The reason can be most easily seen in the single profile design case. For concreteness, imagine subjects are presented with one profile at a time that describes the age (young, middle-aged, old), gender (woman, man), and employment sector (public, private) of a candidate for office and are asked to rate their support for the candidate on a 1-7 Likert scale. This set of attributes and levels generates 2 * 3 * 2 = 12 possible profiles. We *could* ask subjects to rate all 12, but we typically ask them instead to rate only a random subset. If our goal were to estimate the average ratings of each of the 12 profiles, clearly we would be targeting descriptive quantities.

The most common inquiry in conjoint experimentation is the Average Marginal Component Effect or AMCE, which summarizes the average difference in preference between two levels of one attribute, averaging over all of the levels of the other attributes. The AMCE for gender, for example, considers the average difference in preference for women candidates versus men candidates among young candidates who work in the private sector, among middle-aged candidates who work in the public sector, and so on for all six combinations. The overall AMCE is a weighted average of all six of these average preference differences, where the weights are given by the relative frequency of each type of candidate. Despite its name, we think of the AMCE as a descriptive quantity. We of course agree there is a sense in which the AMCE is a causal quantity, since it is the average effect on preferences of describing a hypothetical candidate as a man or a woman. But we can see this quantity as descriptive if we just imagine asking subjects about both candidates and describing the difference in their preferences. We then could aggregate these descriptive differences across profiles. The only reason we don't ask about all possible profiles is that there are far too many to get through in a typical survey, so we ask subjects about a random subset.

Just like single-profile conjoints, forced-choice conjoints also target descriptive inquiries, but the inquiry is one step removed from raw preferences over profiles. Instead, we aim to describe the fraction of pairwise contests that a profile would win, averaging over all subjects in the experiment. That is, we aim to describe a profile's average win rate. We can further describe the differences in the average win rate across profiles, for example, among young candidates who work in the private sector, what is the average difference in win rates for women versus men? Just as in the single profile case, the AMCE is a weighted average of these differences, weighted by the relative frequency of each type of candidate.

Here again, we *could* think of the AMCE as a causal effect, i.e., the average effect of describing a profile as a woman versus a man. But we can also imagine asking subjects to consider all 12 * 12 = 144 possible pairwise contests, then using those binary choices to fully describe subjects preferences over contests. A forced-choice conjoint asks subjects to rate just a random subset of those contests, since asking about all of them would be impractical.

One final wrinkle about the AMCE inquiries, in both the single-profile and forced-choice cases: they are "data-strategy-dependent" inquiries in the sense that, AMCEs average over the distribution of the other profile attributes, and that distribution is controlled by the researcher.^[The AMCE need not be data-dependent. We could write down one distribution of profiles in the model to establish the AMCE inquiry, then randomly sample the profiles shown to respondents for a different distribution. This would be a headache, because the estimator would need to be reweighted to successfully target the AMCE inquiry. Better to bring the data strategy in line with the model in the first place.] The AMCE of gender for profiles that do not include partisanship is different from the AMCE of gender for profiles that include partisanship due to masking (discussed below). Further, and more subtly, the AMCE of gender for profiles that are 75% public sector and 25% private sector is different from the AMCE of gender for profiles that are 50% public sector and 50% private sector, because those relative frequencies are part of the very definition of the inquiry. For contrast, consider a vignette-style hypothetical candidate experiment in which all or most of the other candidate features are fixed, save gender. In that design, we estimate an ATE of gender under only one set of conditions, but in the conjoint design, the AMCE averages over ATEs under many sets of conditions. There is a great benefit of doing so: our inferences are not specific to that one set of conditions. But it also means that what conditions inferences depends crucially on researcher choices about which characteristics are chosen and the randomization scheme.

The data strategy for conjoints, then, requires making these four choices, in addition to the usual measurement, sampling, and assignment concerns:

1. Which attributes to include in the profiles,
2. Which levels to include in each attribute (and in what proportion),
3. How many profiles subjects are asked to rate at a time, and
4. How many sets of profiles subjects are asked to rate in total.

The right set of attributes is governed by the "masking/satisficing" tradeoff [@bansak_hainmueller_hopkins_yamamoto_2019]. If we don't include an important attribute (like partisanship in a candidate choice experiment), we're worried that subjects will partially infer partisanship from other attributes (like race or gender). If so, partisanship is "masked", and the estimates for the effects of race or gender will be biased by these "omitted variables." But if we include too many attributes in order to avoid masking, we may induce "satisficing" among subjects, whereby they only take in a little bit of information, enough to make a "good enough" choice among the candidates.

The right set of levels to include is a tricky choice. We want to include all of the most important levels, but every additional level harms statistical precision. If an attribute has three levels, it's like we're conducting a three-arm trial, so we'll want to have enough subjects for each arm. The more levels, the lower the precision.

How many profiles to rate at the same time is also tricky. Our point of view is that this choice should be guided by the real-world analogue of the survey task. If we're learning about binary choices between options in the real world, then the forced-choice, paired design makes good sense. If we're learning about preferences over many possibilities, the single profile design may be more appropriate. That said, the paired design can yield precision gains over the single profile design in the sense that subjects rate two profiles at the same time, so we effectively generate twice as many observations for perhaps less than twice as much cognitive effort.

Finally, the right number of choice tasks usually depends on the survey budget. We can always add more conjoint tasks and the only cost is the opportunity cost of asking a different question of the survey that may serve another scientific purpose. If we're worried that respondents will get bored with the task, we can always throw out profile pairs that come later in the survey. @bansak_hainmueller_hopkins_yamamoto_2019 suggest that you can ask many tasks without much loss of data quality.

The declaration of conjoint experiments is complex, so we provide a series of helper functions specifically for forced-choice conjoint design in the `rdddr` companion software package.

We begin by establishing the number of subjects and the number of tasks they will accomplish. We then establish the attributes and their levels (this design assumes complete random assignment of all attributes with equal probabilities). Finally, we describe a utility function that governs subject preferences. This function can be simple, as we have it here, or it can be complex, building in differences in preferences by subject type or other details.

In Declaration @def-declarationch17dec5, we imagine a forced-choice candidate choice conjoint in which the attributes are gender, party, and region. We sample 500 subjects and ask them to complete three tasks each. 

:::{.definition #declarationch17dec5} 

Conjoint experiment design

```{r, file = "scripts_declarations/declaration_17.5.R"}
```

:::

:::{.lemma #diagnosisch17diag4}

Diagnosis of the conjoint experiment design

```{r}
#| eval: false

diagnosis_17.4 <- diagnose_design(declaration_17.4)
```

Figure @fig-ch17fig3 shows the sampling distribution of the five AMCE estimators. All five are unbiased, but with only 500 subjects evaluating three pairs of candidates, the power for the smaller AMCEs is less than ideal. 

![Sampling Distribution of five AMCE estimators. Statistical significance of estimates indicated by color](/figures/figure-17-3){#fig-ch17fig3}

:::

## Design examples

- @Kao2022 uses a conjoint experiment in an Iraqi city that was controlled by the Islamic State to understand residents' preferences over punishments for civilian collaborators, depending on the type of collaboration they engaged in.

- @aguilar2015 conduct a candidate choice experiment to measure the difference in how voters evaluate candidates depending on whether they are identified as a man or a woman in Brazil.
