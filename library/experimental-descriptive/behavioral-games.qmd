# Behavioral games 


```{r}
#| echo: false
#| include: false
#| purl:  false
source("scripts/before_chapter_script.R")
```

::: {.ddbox}
We declare a trust game and explore the implications of using deception in the game setup. The diagnosis highlights how the choices in the first round of a game, which are not randomized, affect our ability to study the behaviors of the player in the second round without bias.
:::

Behavioral games are often used to study difficult-to-measure characteristics of subjects like risk attitudes, altruism, prejudice, or trust. The approach involves using labs or other mechanisms to control contexts. A high level of control brings two distinct benefits. First, it can eliminate noise: we obtain estimates under a particular well-defined set of conditions rather than estimates generated from averaging over a range of possibly unknown conditions. Second, more subtly, it can prevent various forms of confounding. For instance, outside the lab we might observe how people act when they work on tasks with an outgroup member. But we only observe the responses among those that *do* work with out group members, not among those that do not. By studying behaviors in a controlled setting we can see how people *would* react when put into particular situations.   

The approach holds enormous value. But, as highlighted by @green2012statistical, it also introduces many subtle design choices. Many of these can be revealed through declaration and diagnosis. 

We illustrate using the "trust" game in which we specify three common inquiries and use a standard design in @def-ch17num6. The design is successful at generating unbiased estimates of the first inquiry but runs into problems with the other two.   

The trust game has been implemented hundreds of times to understand levels and correlates of social trust. Following the meta-analysis given in @johnson2011trust we consider a game in which one player (Player 1, the "trustor") can invest some share of $1. Whatever is invested is then doubled. A second player (Player 2, "the trustee") can then decide what share of the doubled amount to keep for themself and what share to return to the trustor.  

As described by @johnson2011trust, "trust" is commonly measured by the share given and "trustworthiness" is measured by the share returned. With the *MIDA* framework in mind, we will be more specific and define the inquiry independent of the measurement. We define "trust" as the share that *would* be invested by a trustor when confronted with a random trustee, whereas "trustworthiness" is the average share that *would* be returned over a range of possible investments. 

To motivate *M* we assume the following decision making model. We assume that each person $i$ seeks to maximize a weighted average of logged payoffs:

$$u_i = (1-a_i) \log(\pi_i) + a_i \log(\pi_{-i})$$

where $\pi_i, \pi_{-i}$ denotes the monetary payoffs to $i$, $-i$ and $a_i$ ("altruism") captures the weight players place on the (logged) payoffs of other players.

Let $x$ denote the amount sent by the trustor from the endowment $1$.  

The trustee then maximizes:

$$u_2 = (1-a_2) \log((1-\lambda)2x) + a_2 \log((1-x) + \lambda 2x)$$

where $\lambda$ denotes the share of $2x$ that the trustee returns. Maximizing with respect to $\lambda$ yields:

$$\lambda = a_2 + (1-a_2)\frac{x-1}{2x}$$

in the interior. Taking account of boundary constraints,^[$a_2 + (1-a_2)\frac{x-1}{2x}\geq 0$ requires $x  \geq \frac{1-a_2}{1+a_2}$] we have best response function:

$$\lambda(x):= \max\left(0, a_2 + (1-a_2)\frac{x-1}{2x}\right)$$

Interestingly, the share sent back is increasing in the amount sent because player 2 has greater incentive to compensate player 1 for their investment. If the full amount is sent then the share sent back is simply $a_2$.

Given this, the trustor chooses $x$ to maximize:

$$u_1 = (1-a_1) \log\left(1 - x + \lambda(x)2x\right) + a_1 \log\left(\left(1-\lambda(x)\right)2x\right)$$

In the interior this reduces to:

$$u_1 = (1-a_1) \log\left((1 + x)a_2\right) + a_1 \log\left((1-a_2)(1+x)\right)$$

with greatest returns at $x=1$.

For ranges in which no investment will be returned, utility reduces to:

$$u_1 = (1-a_1) \log\left(1 - x\right) + a_1 \log\left(2x\right)$$

which is maximized at: $x = a_1$.

The global maximum depends on which of these yields higher utility. 

@fig-ch17num4 shows the returns to the trustor from different investments given their own and the trustee's other-regarding preferences. We see that when other-regarding preferences are weak for both players, nothing is given and nothing is returned. When other regarding preferences are strong for player 1, they offer substantial amounts even when nothing is expected in return. When other-regarding preferences are sufficiently strong for player 2, player 1 invests fully in anticipation of a return. 

![Illustration of a trust game](/figures/figure-17-4){#fig-ch17num4}

The predictions of this model are then used to define the inquiry and predict outcomes in the model declaration. The model part of the design includes information on underlying preferences. For this we make use of a set of functions that characterize stipulated beliefs about behavior. 

```{r, file = "scripts_declarations/declaration_17.6_a.R"}
```

The inquiries for this design are the expected share offered to different types of trustees, the expected returns, averaged over possible offers, and the expected action by a trustee when the full amount is invested. The data strategy involves assigning players to pairs and orderings. The first half is assigned to the trustor role and matched with the second group that are assigned the trustee role. For the answer strategy, we simply measure average behavior across subjects. As a wrinkle, we include the possibility that the experimenter confronts the returners with random offers rather than the ones actually made by their partners. This aspect of the design is controlled by an argument called `deceive` and turns out to be important for inference.  

::: {#def-ch17num6 .declaration} 

Trust game design

```{r, file = "scripts_declarations/declaration_17.6_b.R"}
```

:::

A few features are worth highlighting. First, the inquiries are defined using a set of hypothetical responses under the model using a specified response function. However the inquiry is robust to the model in the sense that it remains well defined even if you stipulate very different behaviors.  Second, the declaration involves a step where we shift from a "long" data frame with a row per subject to a "wide" data frame with a row per game. Third, the design orders steps so that an estimation stage is implemented before a measurement stage; this is a little unusual but it is done in this way to allow the researchers to analyze Player 1 investment decisions before (possibly) replacing them with fabricated decisions.

Data generated by this design might look like this:

```{r}
#| tbl-cap: "Behavioral games diagnosis"
#| label: tbl-behavioralgames
#| echo: false
declaration_17.6 |>
  draw_data() |>
  arrange(pair) |>
  head() |>
  kable(
    booktabs = TRUE,
    align = "c",
    digits = 2
  )
```

We have a row for each game, we have the (unobserved) $a_i, a_j$ parameters as well as actions by both players in the data. 

Diagnosis @lem-ch17num5 illustrates the properties of the trust game design.

::: {#lem-ch17num5} 

Trust game diagnosis

```{r}
#| eval: false

diagnosis_17.5 <- 
  declaration_17.6 |>
  redesign(deceive = c(TRUE, FALSE)) |>
  diagnose_design() 
```

```{r, echo = FALSE, purl = FALSE}
diagnosis_17.5 <- get_rdddr_file_local("diagnosis_17.5")
```

![Diagnosis of bias in the analysis of trust games with and without deception](/figures/figure-17-5){#fig-ch17num5}

We see that we do well for the first inquiry whether or not deception is used. The first inquiry--*trusting*--is after all a simple measurement of choices albeit in a controlled setting. 
But we do poorly for the second and third inquiries.  

Whether we have bias in the measure of *trustworthy* depends on the use of deception, however, and so presents researchers with a serious design challenge.

There are two distinct reasons for the bias when Player 2 is confronted with the investments made by Player 1. First, the stage 2 distribution of investments differs from the distribution specified in the definition of the inquiry. Although we have assigned roles randomly, the choices confronting Player 2 are not random: they reflect the particular assignments generated by Player 1's choices. These player-generated assignments are generally higher than those specified in the definition of the inquiry, resulting in higher returns than would arise from random offers. A second source of bias is self-selection in stage 2. Even if the distribution of offers confronting the trustees in the second stage were correct, we could still suffer from a problem that the trustees that are sent larger investments are sent those investments partly *because* trustors expects them to return a large share. The bias on the third inquiry --- which conditions on the full amount being sent--- suggests that this is indeed a concern in this design, though the magnitude of the bias is substantively small.

These problems are, we think, very common in games that involve the analysis of decisions that depend on prior decisions. Switching out the offers solves both of these problems but at the cost of deception.  

Many experimental labs have developed quite strong norms against the use of deception. Some alternatives might exist that could be functionally equivalent. One approach would be to limit the information that players have about each other. We assumed in this design that players had enough information on each other to figure out $a_{-i}$. Say instead that information on players were coarsened --- for instance so that players know only each other's gender and ethnicity. In this case we might have a small set of "types" for Player 1 and Player 2. Conditional on the type pair, the variation in offers is as-if random with respect to a Player 2's characteristics and one could assess the average response of each Player 2 type to each offer received from a Player 1 type. This approach would address the selection problem. The problem of non-random offers could be sidestepped by redefining the inquiry to be responses *conditional* on particular offers (such as the return of a 100% investment).

Another approach is to in fact do a mixture of reporting and randomization and *advise* Player 2 players that with some probability (say 50%) they will be confronted with a random investment and with some probability they will see the actual investment made by Player 1. 

:::

## Design examples

- @avdeenko_gilligan_2015 use a trust game to measure outcomes in a randomized controlled trial of the effects of local public infrastructure projects in Sudan. 

- @Iyengar2015 use both dictator and trust games to measure partisan antipathy in the United States. 

