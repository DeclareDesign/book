# Audit experiments 

```{r}
#| echo: false
#| include: false
#| purl:  false
source("scripts/before_chapter_script.R")
```

::: {.ddbox}
We declare an audit experiment design in which the name of a citizen requesting service from government is randomized to be Latino-sounding or White-sounding and the government official either responds or does not. We then declare an augmented design in which a treatment to reduce discrimination is cross-randomized. The declaration highlights the behavioral assumptions that must be made to interpret the estimated treatment effect of the name as discrimination, and the diagnosis of the anti-discrimination treatment highlights how large a sample would be required for high power.
:::

Audit experiments are used to measure discrimination against one group in favor of another group. The design is used commonly to measure whether job applications that are otherwise similar but come from candidates from different genders, races, or social backgrounds receive the same rate of job interview invitations. The same approach has been applied to a very wide range of settings, including education, housing, and requests to politicians.

The audit experiment design we'll explore in this chapter has data and answer strategies that are identical as the two-arm trial for causal inference described in Section @two-arm-randomized-experiments. The difference between an audit study and the typical randomized experiment lies in the model and inquiry. In a two-arm trial, a common (causal) inquiry is the average difference between the treated and untreated potential outcomes, the ATE. In an audit experiment, by contrast, the inquiry is descriptive: what is the fraction of the sample that discriminates?

We can hear our colleagues objecting now -- the inquiry in an audit study can of course be conceived of as causal! It's the average effect of signaling membership in one social group on a resume versus signaling membership in another. We agree, of course, that this interpretation is possible and technically correct. But when we think of the inquiry as descriptive, we can better understand how the audit experiment relies on substantive assumptions about the behaviors of people who do and do not discriminate.

Consider @WHITE_NATHAN_FALLER_2015, which seeks to measure discrimination against Latinos by election officials by assessing whether election officials respond to emailed requests for information from putatively Latino or White voters. We imagine three types of election officials: those who would always respond to the request (regardless of the emailer's ethnicity), those who would never respond to the request (again regardless of the emailer's ethnicity), and officials who discriminate against Latinos. Here, discriminators are defined by their behavior: they would respond to the White voter but not to the Latino voter. These three types are given in @tbl-audittypes. 


| Type                       | $Y_i(Z_i = \textrm{White})$ | $Y_i(Z_i = \textrm{Latino})$ |
|----------------------------|-----------------------------|------------------------------|
| Always-responder           | 1                           | 1                            |
| Anti-Latino discriminator  | 1                           | 0                            |
| Never-responder            | 0                           | 0                            |

: Audit experiment response types {#tbl-audittypes} 

Our descriptive inquiry is the fraction of the sample that discriminates: $\E[\textrm{Type}_i = \textrm{Anti}~\textrm{Latino}~\textrm{discriminator}]$. Under the behavioral assumptions about these three types enumerated in @tbl-audittypes (whether these types would respond depending on the ethnicity of the sender), $\E[\textrm{Type}_i = \textrm{Anti}~\textrm{Latino}~\textrm{discriminator}] = \E[Y_i(Z_i = \textrm{White}) - Y_i(Z_i = \textrm{Latino})]$. Because this is the expected difference between two potential outcomes, we can use a randomized experiment that randomizes ethnicity to measure this descriptive quantity. In the data strategy, we randomly sample from the $Y_i(Z_i = \textrm{White})$'s and from the $Y_i(Z_i = \textrm{Latino})$'s, then in the answer strategy, we take a difference-in-means, generating an estimate of the fraction of the sample that discriminates.

Some finer points about these behavioral assumptions. First, we assume that always-responders and never-responders do not engage in discrimination. It could be that some never-responders don't respond to Latino voters out of racial animus, but do not respond to White voters out of laziness. In this model, such officials would be not be classified as anti-Latino discriminators by assumption. Second, we assume that there are no anti-White discriminators. If there were, then the difference-in-means would not be unbiased for the fraction of anti-Latino discriminators. Instead, it would be unbiased for "net" discrimination, i.e., how much more election officials discriminate against Latinos versus how much they discriminate against Whites. Anti-Latino discrimination and net discrimination are theoretically separate inquiries. To assess whether the no anti-White discriminators assumption is appropriate in a given setting, substantive knowledge is needed. It is not an assumption that can be directly tested empirically.

@def-ch17num1 connects the behavioral assumptions we make about subjects to the randomized experiment we use to infer the value of a descriptive quantity. Only never-responders fail to respond to the White request while only always-responders respond to the Latino request. The inquiry is the proportion of the sample that is an anti-Latino discriminator. The data strategy involves randomly assigning the putative ethnicity of the voter making the request and recording whether it was responded to. The answer strategy is compares average response rates by randomly assigned group.

::: {#def-ch17num1 .declaration} 

Audit experiment design

```{r, file = "scripts_declarations/declaration_17.1.R"}
```

:::

## Intervening to decrease discrimination

@butler_crabtree_2017 prompt researchers to "move beyond measurement" in audit studies. Under the model assumptions in the design, audit experiments measure the level of discrimination, but of course they do not do anything to reduce them. To move beyond measurement, we intervene in the world to reduce discrimination in a treatment group but not in a control group, then measure the level of discrimination in both arms using the audit experiment technology. 

This two-stage design is illustrated in @def-ch17num2. The first half of the design is about causal inference: we want to learn about the effect of the intervention on discrimination. The second half of the design is about descriptive inference -- **within each treatment arm**. We incorporate both stages of the design in the answer strategy, in which the coefficient on the interaction of the intervention indicator with the audit indicator is our estimator of the effect on discrimination.

::: {#def-ch17num2 .declaration} 

Audit experiment intervention study design

```{r, file = "scripts_declarations/declaration_17.2.R"}
```

:::


::: {#lem-ch17num1} 

Audit experiment intervention study diagnosis

Even at 5,000 subjects, the power to detect the effect of the intervention is quite poor, at approximately 15%. This low power stems from the small treatment effect (reducing discrimination by 50% from 5.0% to 2.5%) and from the noisy measurement strategy. 

```{r}
#| eval: false

diagnosis_17.1 <- diagnose_design(declaration_17.2)
```

```{r, echo = FALSE, purl = FALSE}
diagnosis_17.1 <- get_rdddr_file_local("diagnosis_17.1")
```

```{r}
#| label: tbl-auditdiagnosis
#| echo: false
#| tbl-cap: "Audit experiment power analysis"
diagnosis_17.1 |> 
  get_diagnosands() |>
  select(power, `se(power)`, n_sims) |> 
  kable(
  booktabs = TRUE,
  digits = 2,
  align = "c"
  )
```

:::


## Avoiding post-treatment bias

@coppock_2019b discusses how to avoid post-treatment bias when studying how the audit treatment affects the "quality" of responses, such as the tone of an email or the enthusiasm of the hiring call-back. Interestingly, this causal effect is not defined among the discriminators, because they never send emails to Latinos, so those emails never have a tone. The tone inquiry is defined only among always-responders, but estimating this effect without bias is tricky.

## Design examples

- @birkelund2022gender conduct harmonized audit experiments in six countries to measure employment discrimination on the basis of gender.

- @fang2019can "move beyond measurement" by randomizing a New York City government intervention designed to stop housing discrimination, which was then measured by an audit study design.





