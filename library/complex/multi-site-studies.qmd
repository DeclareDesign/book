# Multi-site studies {.unnumbered}

```{r}
#| echo: false
#| include: false
#| purl:  false
source("scripts/before_chapter_script.R")
```

:::: {.ddbox}
We declare a design for a coordinated research project in which multiple research teams combine data gathering and analysis strategies to address a common inquiry. Diagnosis clarifies the benefits and risks of coordination versus tailoring of treatments to contexts.    
::::

Nearly all social science is produced atomistically: individual scientists identify a question and a research strategy for answering it, apply the research strategy, and try to publish the results of that one study. Increasingly, scholars have come to realize that, though the standard research process may promote discovery, it is not optimized for the accumulation of general insights. One reason is that scientists are opportunistic in the contexts and units they study. If inquiry values are idiosyncratic to the contexts and units that scientists choose to study, then our studies will not generalize. 

Scholars have addressed themselves to the problem of generalizability in a variety of ways. One approach is the replication of past studies in new contexts or with new populations. A second has been the application of statistical methods for extrapolating from single studies, relying on variation within studies in unit characteristics and effect heterogeneity. Both of these approaches are important.

We explore a third approach here: coordinated multi-site studies. The idea here is to implement the same research design (or as close as possible) across multiple contexts at the same time, each targeting the same inquiry with similar methods with the explicit intention of meta-analyzing the eventual results. In the previous chapter, we described *retrospective* meta-analysis in which a meta-analyst gathers together previous scholarship on a research question; harmonization of the studies is achieved *ex post* by including the studies that target the same theoretical quantity well and excluding those that do not. A coordinated multi-site study is a *prospective* meta-analysis; harmonization is achieved *ex ante* through the sometimes painstaking process of ensuring comparability of treatments, outcomes, and units across contexts.

How much we can learn about the generalizability of effects depends on how many sites we study, how we *select* the sites, our beliefs about effect heterogeneity, and the level of harmonization across studies. Of these, the harmonization level is most important. Without harmonization of research designs across sites, researchers risk answering different questions in different sites. If the goal is to understand a common inquiry and how values of the inquiry vary across contexts, then each site must be providing answers to (close to) the same inquiry. To do so, researchers coordinate measurement strategies so as to collect the same outcome measures and coordinate the details of treatment procedures to ensure they are studying the effects of the same intervention. They may also wish to coordinate on details like sampling to ensure the same types of subjects are enrolled at each site and on the consent and enrollment procedures so as to avoid introducing selection or demand effects differentially across sites.

The disadvantage of harmonization is that researchers end up answering a *different* question than they started out with. In the worst case, they end up answering one common question across all sites that is interesting in none. We explore these tradeoffs by declaring a modified multi-site design in which different treatments have different effects in different sites. 

Declaration \@ref(def:declaration-19-4) describes a multi-site study with five sites. A conceptual challenge with multi-site studies is that in practice  a single design has to take account of various differences across sites, such as differences in sample sizes, assignment probabilities, or expectations. The design below shows how these multisite features can be introduced in a simple way as part of a single "meta-design." The design allows for variation across sites, implements analysis separately using data from different sites, and then combines findings for an overall conclusion. Note that the inquiries here are defined by creating site level inquiries and the averaging over these.    Implicitly each *site* is weighted equally in the inquiry. Written like this you can see a clear similarity  between  *I* and *A*. One could alternatively use different weighting schemes if you wanted each unit in the sample to to have equal weight or if you wanted each each unit in the population to have equal weight---and of course, adjust *A* accordingly.

We imagine two possible treatments that might be implemented in each site. One, with treatment effect `tau_1` (possibly different in each site), is the treatment that researchers are meant to coordinate on. The second, with treatment effect `tau_2` is the treatment that researchers could implement if they were not coordinating. We imagine that if there is not strong coordination then researchers they select which version of the treatment to implement (here, we imagine they implement the treatment they think will yield larger effects).

We specify two inquiries. The first is for the coordinated treatment, with a focus on the average effect across studies. The second is less easy to describe: it is the average effect, again across cases, of whatever treatment was implemented in those cases.  Though unusual, this inquiry is still well defined.  

Estimation proceeds in two steps: first we calculate site-level effects, just as we do for a meta-analysis, then we meta-analyze those site-level effects. We use here a random effects model, reflecting our expectation that true site effects differ [see Section \@ref(meta-analysis) for a discussion of alternative answer strategies for meta analysis]. 

:::{.definition #declaration-19-4}

Multi-site studies with and without coordination

```{r, file = "scripts_declarations/declaration_19.4.R"}
```

:::

:::{.lemma #diagnosis-19-4}

Multi-site studies diagnosis

We now diagnose the two designs, with and without coordination, and assess the bias for the average common effect treatment and the average effect of the site-specific optimal treatment inquiries. Figure @fig-figure-19-6 displays the sampling distribution of the estimates from the two designs along with the estimands from these two inquiries in each case.

![Sampling distribution of the meta-analytic estimates from a multi-site trial compared to the true average common treatment effect and the true average effect of the site-specific optimal treatment effect.](/figures/figure-19-6){#fig:figure-19-6}

In the coordinated trial, of course, the common treatment and the implemented treatment are the same. Only the coordinated study produces unbiased estimates for the coordinated inquiry.  However the diagnosis shows that *both* approaches (strongly  and weakly coordinated trials) are unbiased for the (more powerful) "selected treatment" inquiry. This inquiry then is well defined and can be estimated without bias. The difficulty with this inquiry though is that being able to make an out-of-sample predictions for this inquiry requires information you might not have: what version of a treatment is likely to be selected in that new  context. 

:::

## Design examples

- @Dunning2019 present the results of a "Metaketa" study (a term for coordinated multi site studies) of the effects of voter information campaigns on political accountability in five countries: Benin, Burkina Faso, Brazil, Uganda (two studies), and Mexico.

- Coordinated multi-site studies need not be enormously expensive undertakings. @frederiksen_2022 reports the results of the same conjoint survey experiment measuring the effects of undemocratic behavior on electoral support in five countries (the United States, the United Kingdom, the Czech Republic, Mexico, and South Korea)



