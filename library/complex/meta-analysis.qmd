# Meta-analysis {.unnumbered}

```{r}
#| echo: false
#| include: false
#| purl:  false
source("scripts/before_chapter_script.R")
```

:::: {.ddbox}
We declare a design for meta-analytic study in which a researcher seeks to combine findings from a collection of existing studies to form an overall conclusion.  Declaration helps clarify the estimand in such studies. Diagnosis highlights risks associated with a common estimator used in meta-analysis, the fixed effect estimator. 
::::

In a meta-analysis, inquiries are typically population-level quantities like the population average treatment effect (PATE). The relevant population might be the full set of units that happened to participate in past research studies or it might be a broader population, like all human adults since 1950. The data strategy for a meta-analysis involves collecting all (or a subset) of the estimates generated by past research studies on the topic and standardizing them so they can be meaningfully compared or combined. Meta-analyses are valuable because they can tell us about the empirical consensus on a particular inquiry. Because they typically aggregate a large amount of information, meta-analyses are usually more precise than individual studies. We can also learn what we *don't* know from a meta-analysis. After aggregating all the available evidence on a given inquiry, we may find that we don't know very much at all.

The PATE inquiry, however, might not be so interesting if the constituent inquiries -- the site-level ATEs -- vary greatly from context to context. The ATEs that make up the PATE might vary because of the contextual features that condition the effect. @galos_coppock_2022, for example,  reanalyze audit studies of gender-based employment discrimination to find that the ATE on callbacks of being a woman is strongly positive in women-dominated sectors and is strongly negative in men dominated sections. For this reason, meta-analyses often include inquiries about the variance of effects across studies or the covariance of effects with groups.^[Not all variation in estimates across sites is due to differences in true effects. Different studies employ different data strategies, so some differences in the treatments and outcomes are inevitable. If the differences across studies grow too large, meta-analysis on the full set of studies becomes inappropriate.]

The largest choice in the data strategy for a meta-analysis is the set of study inclusion and exclusion criteria. These criteria should be guided by the inquiries of the meta-analysis and whether the designs of the constituent studies appropriately inform the meta-analytic inquiry. If the inquiry is the population mean and standard deviation of the site level ATEs of treatment $Z$ on outcome $Y$, we want to include only studies that credibly estimate the effect of $Z$ on $Y$. This requirement means checking that all included studies use similar-enough treatments and similar-enough measurements of the outcome. It also means excluding studies that are prone to bias. For example, @Pettigrew:2006th assemble 515 studies of the contact hypothesis, but @paluck_green_green_2019 exclude all but 5\% of these studies in their updated meta-analysis for failing to randomize intergroup contact. Meta-analyses that include biased studies can compound biases, giving us falsely precise and incorrect answers. Finally, inclusion decisions should be made on the basis of the designs of the constituent studies and *not* their results. For example, we should not exclude studies that fail to reach statistical significance or yield unexpected answers.

The answer strategies for meta-analysis often amount to a weighted average of the individual study estimates. We take a weighted average instead of a simple average because we want to give different studies different amounts of pull in the overall estimate. In particular, we want to give studies that are more precise more weight and studies that are less precise less weight. In fixed-effects estimation, for example, study weights are proportional to the inverse of the estimated variance from the study. In random-effects estimation, by contrast, the weights are proportional to the inverse of the estimated variance from the study, plus an estimate of the between-study variance in effects. With this adjustment, the study weights are less extreme in random effects relative to fixed effects [for more see @borenstein2021introduction, ch. 13]. Fixed-effects meta-analysis may be appropriate in settings in which we have a strong theoretical reason to think the site-level inquiries are all equal to the PATE, but typically, we think effects vary from site-to-site, so random effects is usually the meta-analytic answer strategy of choice.

In @def-ch19num3, we declare a meta-analytic research design for 100 studies with a true PATE ($\mu$) of 0.2. We represent the standard deviation of the study-level ATEs with $\tau$, which we vary between 0 and 1. When $\tau>0$  the true effects vary across contexts. The studies each have different measurement precision, with standard errors between 0.025 and 0.6. The inquiries are $\mu$ and $\tau^2$. In the answer strategy, we use both fixed and random effects meta-analysis. 

::: {#def-ch19num3} 

## Meta-analysis design

```{r, file = "scripts_declarations/declaration_19.3.R"}
```

:::


::: {#lem-ch19num3} 

## Meta-analysis diagnosis

@fig-ch19num5 explores the bias and coverage of the two estimators under each model both for the mean effects inquiry $\mu$ and for the variance of effects inquiry $\tau^2$. We find that the random effects estimator, across both models of how effects are realized, performs best. Whether the variance of site-level ATEs is 1 or 0, random effects estimates both inquiries without bias and coverage is nominal. By contrast, the fixed effect estimator has two problems. When $\tau^2$ is 1, the estimator gets the variance in estimates wrong, because it *assumes* it is zero. Second, the estimator is extremely overconfident, generating confidence intervals that are drastically too small, as reflected in the poor coverage. This overconfidence stems from the fixed-effect assumption that the only reason study-to-study estimates differ is due to estimation noise, not true differences across sites.

```{r}
#| eval: false

diagnosis_19.3 <- diagnose_design(declaration_19.3)
```

```{r, echo = FALSE, purl = FALSE}
diagnosis_19.3 <- get_rdddr_file_local("diagnosis_19.3")
```

![Coverage and bias of the fixed and random effects under two settings, effect homogeneity and effect heterogeneity.](/figures/figure-19-5){#fig-ch19num5}

:::

## Design examples

- @blair_christensen_rudkin_2021 meta-analyze 46 difference-in-difference studies of the effects of commodity price shocks on conflict, excluding over 300 studies of the same effect that relied on weaker identification strategies.

- @schwarz_coppock_2020 collect 67 candidate choice conjoint experiments that randomized candidate gender to estimate the average effect of gender on electoral support.



