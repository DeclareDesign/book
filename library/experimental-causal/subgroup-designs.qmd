# Subgroup designs {#sec-ch18s9}

```{r}
#| echo: false
#| include: false
#| purl:  false
source("scripts/before_chapter_script.R")
```

::: {.ddbox}
We declare and diagnose a design that is targeted at understanding the difference in treatment effects between subgroups. The design combines a sampling strategy that ensures reasonable numbers within each group of interest and a blocking assignment strategy to minimize variance.  
:::

Subgroup designs are experimental designs that have been tailored to a specific inquiry, the difference-in-CATEs. A CATE is a "conditional average treatment effect," or the average treatment effect conditional on membership in some group. A difference-in-CATEs is simply the difference between two CATEs. 

For example, studies of political communication often have the *difference* in response to a party cue by subject partisanship as the main inquiry, since Republican subjects tend to respond positively to a Republican party cue, whereas Democratic subjects tend to respond negatively. 

Subgroup designs share much in common with factorial designs, discussed in detail in @sec-ch18s5. The main source of commonality is the answer strategy for the difference-in-CATEs inquiry. In subgroup designs and factorial designs, the usual approach is to inspect the interaction term from an OLS regression. The two designs differ because in the subgroup design, the difference-in-CATEs is a descriptive difference. We don't randomly assign partisanship, so we can't attribute the difference in response to treatment to partisanship, which could just be marker for the true causes of the difference in response. In the factorial design, we randomize the levels of all treatments, so the differences-in-CATEs carry with them a causal interpretation.

Since we don't randomly assign membership in subgroups, how can we optimize the design to target the difference-in-CATEs? Our main data strategy choice comes in sampling. We need to obtain sufficient numbers of both groups in order to generate sharp enough estimates of each CATE, the better to estimate their difference. For example, at the time of this writing, many sources of convenience samples (Mechanical Turk, Lucid, Prolific, and many others) appear to under-represent Republicans, so researchers sometimes need to make special efforts to increase their numbers in the eventual sample.^[@klar2019identities make a similar point when specifically advocating for oversampling minority groups in experimental studies of intersectionality.]

@def-ch18num6 describes a fixed population of 10,000 units, among whom people with `X = 1` are relatively rare (only 20%). In the `potential_outcomes` call, we build in both baseline differences in the outcome, and also responses to treatment that are oppositely-signed across the two subgroups. Those with `X = 0` have a CATE of `0.1` and those with `X = 1` have a CATE of `0.1 - 0.2 = -0.1`. The true difference-in-CATEs is therefore 20 percentage points. 

If we were to draw a sample of 1,000 at random, we would expect to yield only 200 people with `X = 1`. Here we improve upon that through stratified sampling. We deliberately sample 500 units with `X = 1` and 500 with `X = 0`, then block-random-assign the treatment within groups defined by `X`.

::: {#def-ch18num6 .declaration} 

Subgroup design declaration

```{r, file = "scripts_declarations/declaration_18.6.R"}
```

:::


::: {#lem-ch18num6} 

Subgroup design diagnosis

To show the benefits of stratified sampling for experiments, we redesign over many values of under- and over-sampling units with `X = 1`, holding the total sample size fixed at `1000`. The top panel of @fig-ch18num6 shows the distribution of difference-in-CATE estimates at each size of the `X = 1` group. When very small or very large fractions of the total sample have `X = 1`, the variance of the estimator is much larger than when the two groups the same size. 

The bottom panel of the figure shows how three diagnosands change over the oversampling design parameter. Bias is never a problem -- even small subgroups will generate unbiased difference-in-CATE estimates. As suggested by the top panel, the standard error is minimized in the middle and is largest at the extremes. Likewise, statistical power is maximized in the middle, but drops off surprisingly quickly as we move away from evenly-balanced recruitment. 

```{r}
#| eval: false

diagnosis_18.6 <- 
  declaration_18.6 |> 
  redesign(n_x1 = seq(20, 980, by = 96)) |> 
  diagnose_designs()
```

![Performance of the subgroup depending on oversampling strategy.](/figures/figure-18-6){#fig-ch18num6}

:::

## Design Examples

- @collins_2021 conducts a survey experiment that measures the effects of school board meeting style (standard, participatory, or deliberative) on willingness to attend meetings. The author hypothesized that the effects of participatory and deliberative meeting styles would be larger for nonwhite subjects, which motivated an oversample of this type of respondent. The final sample included 1,061 nonwhite subjects and 1,122 white subjects, so the design was well-poised to estimate the conditional average effects of the treatments for both groups, which ended up being quite similar.

- @Swire2017 conduct an experimental study of misinformation and corrections that compares Republican and Democratic subjects beliefs in false statements. These authors oversampled Republicans from the Mechanical Turk platform, since usual samples from the platform underrepresent Republicans. Republicans believed false claims more when they were attributed to President Trump and Democrats believed them less; this difference-in-CATEs is especially precisely estimated because of the over sample. Both partisan groups responded to *corrections* of false claims by believing them less, by similar amounts.

