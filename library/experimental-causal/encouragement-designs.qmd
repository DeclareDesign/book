# Encouragement designs  {.unnumbered}

```{r}
#| echo: false
#| include: false
#| purl:  false
source("scripts/before_chapter_script.R")
```

:::: {.ddbox}
We declare an encouragement design in which units are assigned to be encouraged to take up a treatment and the average treatment effect is measured among those who comply with the encouragement. The declaration highlights the many changes to the design that are needed to consider noncompliance and what inquiries can be estimated with the design.
::::

In many experimental settings, we cannot require units we *assign* to take treatment to actually *take* treatment. Nor can we require units assigned to the control group not to take treatment. Instead, we have to content ourselves with "encouraging" units assigned to the treatment group to take treatment and "encouraging" units assigned to the control group not to.

Encouragements are often only partially successful. Some units assigned to treatment refuse treatment and some units assigned to control find a way to obtain treatment after all. In these settings, we say that experiments encounter "noncompliance." This section will describe the most common approach to the design and analysis of encouragement trials, and will point out potential pitfalls along the way.

Any time a data strategy entails contacting subjects in order to deliver a treatment like a bundle of information or some good, noncompliance is a potential problem. Emails go undelivered, unopened, and unread. Letters get lost in the mail. Phone calls are screened, text messages get blocked, direct messages on social media are ignored. People don't come to the door when you knock, either because they aren't home or they don't trust strangers. Noncompliance can affect non-informational treatments as well: goods may be difficult to deliver to remote locations, subjects may refuse to participate in assigned experimental activities, or research staff might simply fail to respect the realized treatment schedule.

Experimenters who anticipate noncompliance should make compensating adjustments to their research designs (relative to the canonical two arm design). These adjustments ripple through *M*, *I*, *D*, and *A*. 

## Changes to the model

The biggest change to *M* is developing beliefs about compliance types, also called "principal strata" [@frangakis2002principal]. In a two-arm trial, subjects can be one of four compliance types, depending on how their treatment status responds to their treatment assignment. The four types are described in Table \@ref(tab:compliancetypes2). $D_i(Z_i = 0)$ is a potential outcome -- it is the treatment status that unit $i$ would express if assigned to control. Likewise, $D_i(Z_i = 1)$ is the treatment status that unit $i$ would express if assigned to treatment. These potential outcomes can take each take on a value of 0 or 1, so their intersection allows for four types. For Always-takers, $D_i$ is equal to 1 regardless of the value of $Z$ -- they always take treatment. Never-takers are the opposite -- $D_i$ is equal to 0 regardless of the value of $Z_i$. For Always-takers and Never-takers, assignment to treatment *does not change* whether they take treatment. 

Compliers are units that take treatment if assigned to treatment and do not take treatment if assigned to control. Their name "compliers" connotes that something about their disposition as subjects makes them "compliant" or otherwise docile, but this connotation is misleading. Compliance types are generated by the confluence of subject behavior and data strategy choices. Whether or not a subject answers the door when the canvasser comes calling is a function many things, including whether the subject is at home and whether they open the door to canvassers. Data strategies that attempt to deliver treatments in the evenings or on weekends might generate more (or different) compliers than those that attempt treatment during working hours.

| Compliance Type | $D_i(Z_i = 0)$ | $D_i(Z_i = 1)$ |
|-----------------|----------------|----------------|
| Never-taker     | 0              | 0              |
| Complier        | 0              | 1              |
| Defier          | 1              | 0              |
| Always-taker    | 1              | 1              |

Table: (\#tab:compliancetypes2) Compliance types

The last compliance type to describe are defiers. These strange birds refuse treatment when assigned to treatment, but find a way to obtain treatment when assigned to control. Whether or not "defiers" exist turns out to be a consequential assumption that must be made in the model. We have good reason to believe that defiers are rare -- assignment to treatment almost always has a positive average effect on treatment take-up.

A unit's compliance type is usually not possible to observe directly. Subjects assigned to the control group who take take treatment ($D_i(0) = 1$) could be defiers or always-takers. Subjects assigned to the treatment group who do not take treatment ($D_i(1) = 0$) could be defiers or never-takers. Our inability to be sure of compliance types is another facet of the fundamental problem of causal inference. Even though a subject's compliance type (with respect to a given design) is a stable trait, it is defined by how the subject would act in multiple counterfactual worlds. We can't tell what type a unit is because we would need to see whether they take treatment when assigned to treatment and also when assigned to control. 

## Changes to the inquiry

The inclusion of compliance types in the model also necessitate changes to the inquiry. Always-takers and never-takers present a real problem for causal inference. Even with the power to randomly assign, we can't change what treatments these units take. As a result, *we don't get to learn* about the effects of treatment among these groups. Even if our inquiry were the average effect of treatment among the never-takers, the experiment (as designed) would not be able to generate empirical estimates of it.^[We write "as designed" because compliance types are defined with respect to a particular design. If it were possible to induce the never-takers to take treatment (i.e., under a different data strategy, these units might be compliers), this inquiry would not necessarily be out of reach.] Our inquiry has to fall back to the average effects among those units that whose treatment status we can successfully encourage to change -- the compliers.

This inquiry is called the complier average causal effect (the CACE). It is defined as $\E[Y_i(1) - Y_i(0) | d_i(1) > d_i(0)]$. Just like the average treatment effect, it refers to an average over individual causal effects, but this average is taken over a specific subset of units, the compliers. Compliers are the only units for whom $d_i(1) > d_i(0)$, because for compliers, $d_i(1) = 1$ and $d_i(0) = 0$. When assignments and treatments are binary, the CACE is mathematically identical to the local average treatment effect (LATE) described in Chapter \@ref(sec: p3iv). Whether we write CACE or LATE sometimes depends on academic discipline, with LATE being more common among economists and CATE more common among political scientists. An advantage of "CACE" over "LATE" is that it is specific about which units the effect is "local" to -- it is local to the compliers. 

When experiments encounter noncompliance, the CACE is *usually* the most important inquiry for theory, since it refers to an average effect of the causal variable, at least for a subset of the units in the study. However, two other common inquiries are important to address here as well. 

The first is the intention-to-treat (ITT) inquiry, which is defined as $\E[Y_i(D_i(Z = 1), Z = 1) - Y_i(D_i(Z = 0), Z = 0)]$. The encouragement itself $Z$ has a total effect on $Y$ that is mediated in whole or in part by the treatment status. Sometimes the ITT is the policy-relevant inquiry, since it describes what would happen if a policy maker implemented the policy in the same way as the experiment, *inclusive* of noncompliance. Consider an encouragement design to study the effectiveness of a tax webinar on tax compliance. Even if the webinar is very effective among people willing to watch it (the CACE is large), the main trouble faced by the policy maker will be getting people to sit through the webinar. The ITT describes the average effect of *inviting* people to the webinar, which could be quite small if very few people are willing to join.

The second additional inquiry is the compliance rate, sometimes referred to as the $\mathrm{ITT}_{\textrm{D}}$. It describes the average effect of assignment on treatment, and is written $\E[(D_i(Z = 1) - D_i(Z = 0)]$. A small bit of algebra shows that the $\mathrm{ITT}_{\textrm{D}}$ is equal to the fraction of the sample that are compliers minus the fraction that are defiers. 

These three inquiries are tightly related. Under five very important assumptions (described below), we can write: 

\begin{align*}
\mathrm{CACE} = \frac{\mathrm{ITT}}{\mathrm{ITT}_{\mathrm{D}}}
\end{align*}

A derivation of this relationship is given in Section \@ref(instrumental-variables) on instrumental variables. The five assumptions described in that section are identical to the assumptions required here. In an experimental setting, "exogeneity of the instrument" is guaranteed by features of the data strategy. Since we use random assignment, we know for sure that the "instrument" (the encouragement) is exogenous. Excludability of the instrument refers to the idea that the effect of the encouragement on the outcome is fully mediated by the treatment. This assumption could be violated if the mere act of encouragement changes outcomes. Stated differently, if never-takers or always-takers reveal *different* potential outcomes in treatment and control ($Y_i(D_i(Z = 1), Z = 1) \neq Y_i(D_i(Z = 0), Z = 0)$), it must be because encouragement itself changes outcomes. Non-interference in this setting means that units' treatment status and outcomes do not depend on the assignment or treatment status of other units. In an experimental context, the assumption of monotonicity rules out the existence of defiers. This assumption is often made plausible by features of the data strategy (perhaps it is impossible for those who are not assigned to treatment to obtain treatment) or features of the model ("defiant" responses to encouragement are behaviorally unlikely). The final assumption -- nonzero effect of the instrument on the treatment -- can also be assured by features of the data strategy. In order to learn about the effects of treatment, data strategies must successfully encourage at least some units to take treatment.

## Changes to the data strategy

When experimenters expect that noncompliance will be a problem, they should take steps to mitigate that problem in the data strategy. Sometimes doing so just means trying harder: investigating the patterns of noncompliance, attempting to deliver treatment on multiple occasions, or offering subjects incentives for participation. "Trying harder" is about turning more subjects into compliers by choosing a data strategy that encounters less noncompliance.

A second important change to the data strategy is the explicit measurement of treatment status as distinct from treatment assignment. For some designs, measuring treatment status is easy. We just record which units were treated and which were untreated. But in some settings, measuring compliance is trickier. For example, if treatments are emailed, we might never know if subjects read the email. Perhaps our email service will track read receipts, in which case one facet of this measurement problem is solved. We won't know, however, how many subjects read the subject line -- and if the subject line contains any treatment information, then even subjects who don't click on the email may be "partially" treated. Our main advice is to measure compliance in the most conservative way: if treatment emails bounce altogether, then subjects are not treated. 
           
In multiarm trials or with continuous rather than binary instruments, noncompliance becomes a more complex problem to define and address through the data strategy and answer strategy. We must define complier types according to all of the possible treatment conditions. For multiarm trials, the complier types for the first treatment may not be the same for the second treatment; in other words, units will comply at different rates to different treatments. Apparent differences in complier average treatment effects and intent-to-treat effects, as a result, may reflect not differences in treatment effects but different rates of compliance.

## Changes to the answer strategy

Estimation of the CACE is not as straightforward subsetting the analysis to compliers. A plug-in estimator of the CACE with good properties takes the ratio of the $ITT$ estimate to the $ITT_d$ estimate. Since the $ITT_d$ must be a number between zero and one, this estimator "inflates" the $ITT$ by the compliance rate. Another way of thinking about this is that the $ITT$ is deflated by all the never-takers and always-takers, among whom the $ITT$ is by construction 0, so instead of "inflating", we are "re-inflating" the ITT to the level of the CACE. Two-stage least squares in which we instrument the treatment with the random assignment is a numerically equivalent procedure when treatment and assignments are binary. Two-stage least squares has the further advantage of being able to seamlessly incorporate covariate information to increase precision.

Two alternative answer strategies are biased and should be avoided. An "as-treated" analysis ignores the encouragement $Z$ and instead compares units by their revealed treatment status $D$. This procedure is prone to bias because those who come to be treated may differ systematically from those who do not. The "per protocol" analysis is similarly biased. It drops any unit that fails to comply with its assignment, but those who take treatment in the treatment group (compliers and always-takers) may differ systematically from those who do not take treatment in the control group (compliers and never-takers). Both the "as-treated" and "per-protocol" answer strategies suffer from a special case of post-treatment bias, wherein conditioning on a post-assignment variable (treatment status) essentially de-randomizes the study.

Declaration \@ref(def:declaration-18-9) elaborates the model to include the four compliance types, setting the share of defiers to zero to match the assumption of monotonicity. It imagines that the potential outcomes of the outcomes $Y$ with respect to the treatment $D$ are different for each compliance type, reflecting the idea that compliance type could be correlated with potential outcomes. The declaration also links compliance type to the potential outcomes of the treatment $D$ with respect to the randomized encouragement $Z$. We then move on to declaring two inquiries (the CACE and the ATE) and three answer strategies (two-stage least squares, as-treated analysis, and per-protocol analysis).

:::{.definition #declaration-18-8} 

Encouragement design

```{r, file = "scripts_declarations/declaration_18.8.R"}
```

:::

Figure \@ref(fig:figure-18-9) represents the encouragement design as a DAG. No arrows lead into $Z$, because the treatment was randomly assigned. The compliance type $C$, the assignment $Z$, and unobserved heterogeneity $U$ conspire to set the level of $D$. The outcome $Y$ is affected by the treatment $D$ of course, but also by compliance type $C$ and unobserved heterogeneity $U$. The required exclusion restriction that $Z$ only affect $Y$ through $D$ is represented by the *lack* of an arrow from $Z$ to $Y$.  The deficiencies of the as-treated and per-protocol analysis strategies can be learned from the DAG as well. $D$ is a collider, so conditioning on it would open up back-door paths between $Z$, $C$, and $U$, leading to bias of unknown direction and magnitude.

```{r figure-18-9, fig.cap = "Directed acyclic graph of the encouragement design", echo = FALSE}
imgsufx <- if(is_html_output()) "svg" else "pdf"
include_graphics(path = paste0("figures/figure_18.9.", imgsufx))
```

:::{.lemma #diagnosis-18-9}

Diagnosis of encouragement design

The design diagnosis shows the sampling distribution of the three answer strategies and compares it to two potential inquiries: the complier average causal effect and the average treatment effect. Our preferred method, two-stage least squares, is *biased* for the ATE. Because we can't learn about the effects of treatment among never-takers or always-takers, any estimate of the true ATE will be necessarily be prone to bias, except in the happy circumstance that never-takers and always-takers happen to be just like compliers in terms of their potential outcomes.

Two-stage least squares does a much better job of estimating the complier average causal effect. Even though the sampling distribution is wider than those for the per-protocol and as-treated analysis, it is at least centered on a well-defined inquiry. By contrast, the other two answer strategies are biased for either target.

```{r}
#| eval: false

diagnosis_18.9 <- diagnose_design(declaration_18.8)
```

```{r figure-18-10, echo=FALSE, fig.width = 6.5, fig.height=6.5, fig.cap="Sampling distributions of the two-stage least squares, per protocol, and as-treated answer strategies."}
imgsufx <- if(is_html_output()) "svg" else "pdf"
include_graphics(path = paste0("figures/figure_18.10.", imgsufx))
```

:::


## Design examples

- @scacco_warren_2018 randomize young men in Nigeria to participate in a vocational training program -- 84% of subjects assigned to the training participated, but the remainder did not. The authors attempted to measure outcomes for all subjects, regardless of treatment or compliance status and estimated intention-to-treat effects in all cases.

- @blair_2022 randomize communities in Colombia to receive a program aimed at improving local governance through enhanced cooperation between state and local agencies. Some communities assigned to participate in the program did not, or did not participate fully. The authors present the intention-to-treat estimates of treatment effects as well as complier average causal effect estimates, varying the definition of compliance to include or exclude partial compliance (defining compliance as "any compliance including partial compliance" is the conservative choice, as defining partial compliers as noncompliers could violate excludability.)

