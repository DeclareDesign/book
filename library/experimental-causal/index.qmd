# Experimental : causal 

An inquiry is causal if it involves a comparison of counterfactual states of the world and a data strategy is experimental if it involves explicit assignment of units to treatment conditions. Experimental designs for causal inference combine these two elements. The designs in this section aim to estimate causal effects and the procedure for doing so involves actively allocating treatments. 

Many experimental designs for causal inference in the social sciences take advantage of researcher control over the assignment of treatments to assign treatments *at random*. In the archetypal two-arm randomized trial, a group of $N$ subjects are recruited, $m$ of them are chosen at random to receive treatment and the remaining $N-m$ of them do not receive treatment and serve as controls. The inquiry is the average treatment effect, the answer strategy is the difference-in-means estimator. The strength of the design can be appreciated by analogy to random sampling. The $m$ outcomes in the treatment group represent a random sample from the treated potential outcomes among all $N$ subjects, so the sample mean in the treatment group is a good estimator of the true average treated potential outcome; an analogous claim holds for the control group.

The randomization of treatments to estimate average causal effects is a relatively recent human invention. While glimmers of the idea appeared earlier, it wasn't until at least the 1920s that explicit randomization appeared in agricultural science, medicine, education, and political science [@Jamison2019]. Only a few generations of scientists have had access to this tool. Sometimes critics of experiments will charge "you can't randomize [important causal variable]."  There are of course practical constraints on what treatments researchers can control, be they ethical, financial, or otherwise. We think the main constraint is researcher creativity. The scientific history of randomized experiments is short -- just because it hasn't been randomized *yet* doesn't mean it can't be. (By the same token, just because it *can* be randomized doesn't mean that it should be.)

Randomized experiments are rightly praised for their desirable inferential properties, but of course they can go wrong in many ways that designers of experiments should anticipate and minimize. These problems include problems in the data strategy (randomization implementation failures, excludability violations, noncompliance, attrition, and interference between units), problems in the answer strategy (conditioning on post-treatment variables, failure to account for clustering, $p$-hacking), and even problems in the inquiry (estimator-inquiry mismatches). Of course all these problems apply *a fortiori* to non-experimental studies, but they are important to emphasize for experimental studies since they are often characterized as being "unbiased" without qualification.

The designs in this chapter proceed from the simplest experimental design -- the two arm trial -- up through very complex designs like the randomized saturation design. 