# Process tracing

```{r}
#| echo: false
#| include: false
#| purl:  false
source("scripts/before_chapter_script.R")
```

:::: {.ddbox}
We declare a qualitative design in which researchers seek to learn about the effect of a cause on a single unit. The diagnosis helps evaluate the gains from different within-case data gathering strategies.
::::

In qualitative research we are often interested in learning about the causal effect for a single unit. For example, for a country unit that underwent sanctions (an "event"), we want to know the causal effect of the sanctions on government behavior. To do so, we need to know what would have happened if the event did not happen, the *counter*factual outcome that did not occur as opposed to the factual outcome that did. Due to the fundamental problem of causal inference, we cannot observe what would have happened if that counterfactual case had happened. We have to guess---or infer---what would have happened. Social scientists have developed a large array of tools for guessing missing counterfactual outcomes --- what would have happened in the counterfactual case, if the event had not happened.^[We are also interested in the opposite case sometimes: we have a unit that did not experience an event, and we want to know the causal effect of *not* having it. In this case, we need to guess what would have happened if the event did happen. The same tools apply in reverse.]

A common inquiry in this setting is whether an outcome was *due* to a cause. For instance in a case with $X=1, Y=1$, this "causal attribution" inquiry or "Cause of Effect" inquiry can be written $\mathrm{CoE}:=1-Y(0)| X=1 \mathrm{ \& } Y=1$. For a unit with  $X=1$ and $Y=1$, $\mathrm{CoE}=1$ if $Y(0)=0$.

"Process tracing" is a prominent strategy for assessing causes of effects [@BennettCheckel2015PT; @fairfield2017explicit]. Here, following for example @humphreys2017qualitative, we think of process tracing as a procedure in which researchers provide a theory in the form of a causal model that is rich enough to characterize the probability of observing ancillary data ("Causal process observations" [@brady2004data]) given underlying causal relations. When equipped with prior beliefs, such a model in turn lets one use Bayes' rule to form posterior beliefs over causal relations after observing these data. 

For intuition, say we are interested in whether a policy caused a change in economic outcomes. We theorize that for the policy to matter, it at least had to be implemented. So if we find out that the policy was not implemented we infer that it did not matter. We make theory-dependent inferences that are reasonable *insofar as* the theory itself is reasonable. In this example, if there are plausible channels through which a policy might have mattered even if not implemented, then our conclusion would not be warranted.

To illustrate design choices for a process tracing study we consider a setting in which we have already observed $X, Y$ data and we are interested in figuring out whether $Y$ takes on the value it does *because* $X$ took on the value it did. 

More specifically we imagine a model with two ancillary variables, $M$ and $W$. We posit that $X$ causes $Y$ via $M$---with negative effects of $X$ on $M$ and of $M$ and $Y$ ruled out. And we posit that $W$ is a cause of both $M$ and $Y$. Specifically, our model asserts that if $W=1$ then $X$ causes $M$ and $M$ causes $Y$ for sure. Under this model $M$ and $W$ each serve as "clues" for the causal effect of $X$ on $Y$. 

Using the language popularized by @VanEvera1997, $M$ provides a "hoop" test: if you look for data on $M$ and find that $M=0$ when $X=1$ and $Y=1$ then you infer that $X$ did not cause $Y$. If on the other hand you examine $W$ and find that $W=1$ then you have a "smoking gun" test: You infer that $X$ did indeed cause $Y$. However, if you find both $M=0$ and $W=1$, then you know your model is wrong. 

The model can be described using the `CausalQueries` package thus:

```{r, file = "scripts_declarations/declaration_16.1a.R", eval = TRUE}
```

The DAG of the causal model is shown in figure \@ref(fig:figure-16-1).

```{r figure-16-1, echo = FALSE, fig.cap="Directed acyclic graph of a process tracing model", fig.height=5,fig.width=6.5}
imgsufx <- if(is_html_output()) "svg" else "pdf"
include_graphics(path = paste0("figures/figure_16.1.", imgsufx))
```

This model definition describes the DAG but also specifies a set of restrictions on causal relations. By default, flat priors are then placed over all other possible causal relations, though of course other prior beliefs could also be specified. 

We now have all we need to assess what inferences we might make given different sorts of observations on $W$ and $M$. 

Table \@ref(tab:ptprobative) shows three types of quantities: beliefs upon observing $X=1, Y=1$, conditional inferences upon observing additional data on $M$ or $W$, and the conditional probability of seeing different outcomes for $M$ or $W$. 

```{r ptprobative, echo = FALSE}
query_model(
    causal_model,
    query = list('Prob(CoE=1|X = 1, Y=1)' = "Y[X=1] > Y[X=0]",
                 'Prob(CoE=1 | X = 1, Y = 1, M=0)' = "Y[X=1] > Y[X=0]",
                 'Prob(CoE=1 | X = 1, Y = 1, M=1)' = "Y[X=1] > Y[X=0]",
                 'Prob(CoE=1 | X = 1, Y = 1, W=0)' = "Y[X=1] > Y[X=0]",
                 'Prob(CoE=1 | X = 1, Y = 1, W=1)' = "Y[X=1] > Y[X=0]",
                 'Prob(M=1 | X = 1, Y = 1)' = "M==1",
                 'Prob(W=1 | X = 1, Y = 1)' = "W==1"),
    given = list("Y==1 & X==1",
                 "Y==1 & X==1 & M==0",
                 "Y==1 & X==1 & M==1",
                 "Y==1 & X==1 & W==0",
                 "Y==1 & X==1 & W==1",
                 "Y==1 & X==1",
                 "Y==1 & X==1"
                 )) |> 
  select(Query, Value = mean) |> 
  kable(booktabs = TRUE, 
        align = c("l","c"), 
        caption = "Beliefs about the probability $X$ caused $Y$ upon observing $X=1, Y=1$,  updated inferences upon observing additional data on $M$  or $W$, and  the conditional probability of seeing different outcomes for $M$ or $W$ given observation of $X=1, Y=1$.", 
        digits = 2) 
```

We see here that $M$ provides a hoop test since we are certain that $X$ does not cause $Y$ when $M=0$, but we are uncertain when $M=1$. (Moreover, we already expect to see $M=1$ given what we have seen for $X$ and $Y$). $W$ provides a smoking gun test since we are certain that $X$ causes $Y$ when $M=1$ but uncertain otherwise. Since we have access to both conditional inferences and the probability of seeing different types of data, we have enough data in Table \@ref(tab:ptprobative) to calculate how different strategies will perform. 

We think it useful to fold these quantities into a design declaration so that research consumers can access the data strategies and answer strategies in the same way as they would for any other problem.

Declaration \@ref(def:declaration-16-1) provides a flexible process tracing design. You can use this design with a different causal model and substituting in different causal queries and process tracing strategies. Given a background causal model, the design first draws a "causal type"---that is a case together with all its causal relations. The value of the estimand (EoC) can then be calculated and the observable data corresponding to the type revealed. The estimation uses a custom function, which simply returns the inferences on the query---like those in the Table \@ref(tab:ptprobative)---but given different possible observed values for all nodes. No data strategy is provided explicitly as this is tied up here in the estimation, that is, the estimation step describes what data will be used.

:::{.definition #declaration-16-1} 

Process tracing design declaration

```{r, file = "scripts_declarations/declaration_16.1b.R", eval = FALSE}
```

:::

:::{.lemma #diagnosis-16-1}

Process tracing diagnosis

Given such a model, a case in which $X = Y = 1$ (for instance), and limited resources, we now want to know whether we would be better gathering data on $M$ or on $W$ or both.  

The answers are given in Figure \@ref(fig:figure-16-2). Here we show the expected error from inferences given each process tracing strategy. We break up the diagnoses according to the $X$, $Y$ data already observed---thus illustrating how an unconditional model can be used to assess designs even after some data is observed. Across the four possible data patterns we see equivalent implications for settings in which we are interested in a case with $X=Y=1$ and a case with $X=Y=0$. For the cases with $X \neq Y$ we already know---because of the monotonicity restrictions in the model---that $X$ did not cause $Y$ and so we learn nothing from all strategies. 

We see only modest declines in expected errors from observation of the mediator $M$ (consistent with the manual calculation above), but large declines from observing $W$. If we already observe $W$ the gains from observing $M$ are still more slight. Broadly this confirms a more general observation that mediators can provide limited traction for learning about causes of effects relative to moderators [@dawid2022bounding].

```{r figure-16-2, echo = FALSE, fig.cap="Inferences from four different process tracing data strategies on whether $X$ caused $Y$ given four different observed $X,Y$ data patterns.", fig.height=5,fig.width=6.5}
imgsufx <- if(is_html_output()) "svg" else "pdf"
include_graphics(path = paste0("figures/figure_16.2.", imgsufx))
```

:::

## Design examples

- @REVKIN2020104981 consider the "rebel social contract" in which rebel groups offer citizens political protections and social benefits in return for citizens' allegiance. The authors use the presence of formal complaints by citizens about the Islamic State in Iraq and Syria as a hoop test for the causal model of exchange of protections for allegiance.

- @snyder_borghard_2011 attempt to apply a smoking gun test to audience cost theory, but could find no clear cut cases of settings in which "the public was opposed to military action before a threat was issued and then explicitly punished the leader for not following through with a threat with which it disagreed."


