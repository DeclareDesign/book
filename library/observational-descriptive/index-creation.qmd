# Index creation {.unnumbered}

```{r}
#| echo: false
#| include: false
#| purl:  false
source("scripts/before_chapter_script.R")
```

:::: {.ddbox}
We declare a design is which we take multiple measures and combine them to learn about a latent, unobservable quantity. The design diagnosis shows that it is possible to generate interpretable conditional estimates of this quantity and assess bias even though the metric of the unobservable quantity is unknown. The diagnosis highlights how subtle differences in scale construction generate different biases. 
::::

Models often specify a latent variable (`Y_star`) that we can't observe directly. The measurement procedures we use to produce observed variables (`Y_1`, `Y_2`, `Y_3`) are not perfect: observed values may be related to the latent variables but are often on different scales. A common strategy for addressing measurement error is to combine multiple measures of the same latent variable into an index. The basic intuition for this procedure is that when we combine multiple measures, the errors attending to each measure will tend to cancel one another out. When this happens, the index itself has lower measurement error than any of the constituent parts. 

The first difficult feature of such problems is that we do not have access to the *scale* on which `Y_star` is measured and so it may seem like a hopeless exercise to try to assess whether we have got good or bad estimates of `Y_star` when we combine the measured data. 

One way around this is to normalize the scale of both the latent variable and the measured variable so that they have a mean of zero and a standard deviation of one. But in that case, we are guaranteed that our estimate of the mean of the normalized variable will be unbiased because we will certainly estimate a mean of 0! That may be --- but as we show in the declaration below, if your model is correct this approach may still be useful for calculating other quantities (such as conditional means) that you don't just get right by construction.  

A second challenge is deciding which measurements to combine into the index. We clearly only want to create indices using measurements of the same latent variable, but it can hard to be sure which latent variable, exactly, is being measured. Just relying on whether the measurements are positively correlated or not is not sufficient, because measurements can be correlated even if they are not measurements of the same underlying variable. Ultimately we have to rely on theory to make these decisions, as uncomfortable as that may make us.

In Declaration @def-declarationch15num6, our inquiry is the average level of the latent variable among units whose binary covariate `X` is equal to one.

In the declaration below `Y_star` has a normal distribution but it is not centered on zero. The measured variables `Y_1`, `Y_2`, `Y_3` are also normally distributed but each has its own scale; they are all related to `Y_star`, though some more strongly than others. Many procedures for combining these measured variables into an index exist. Here we'll consider a few of the common approaches: 

- `Y_avg` is constructed by first scaling each of these measured variables, then averaging and then scaling again. This is akin to the approach used in @kling2007experimental. 

- `Y_avg_adjusted` is the same as `Y_avg`, but we scale the measured values by the units with `X = 0`.

- `Y_avg_rescaled` is the same as `Y_avg`, but instead of taking an equally-weighted average of the scaled components, we rescale their sum by its mean and standard deviation.

- `Y_first_factor` extracts the first factor from a 'principal components analysis' which, intuitively, seeks to find a weighting that minimizes the distance to the measured variables. 

:::{.definition #declarationch15num6} 

Latent variable design

```{r, file = "scripts_declarations/declaration_15.6.R"}
```

:::

In Figure @fig-ch15num6 we show that the correlation between all the indices and the underlying quantity is quite good, even though the strength of the correlations for some of the components is weak. Trickier however is being sure we have an interpretable  *scale*.

![Correlations between the latent variable, measured components, and indices.](/figures/figure-15-6){#fig-ch15num6}

:::{.lemma #diagnosisch15num5}

Latent variables diagnosis over alternative answer strategies

```{r}
#| eval: false

diagnosis_15.5 <- diagnose_design(declaration_15.6, make_groups = vars(outcome)) 
```

Figure @fig-ch15num7 shows the distribution of estimates from different approaches to generating indices. A few features stand out from the distribution of estimates. 

The simple averaging of the normalized scales also generates estimates that are too small. Rescaling the scaled components instead of taking the average helps only a small amount. The principal component version appears to do especially poorly, but there is a simple reason for this: the method does not presuppose knowledge of the *direction* of the scale of the latent variable and can come up with index that reverse the actual scale of interest. Avoiding this requires an interpretative step after the principal components analysis is implemented (more subtly the averaging approach also has an interpretative step *before* averaging, when components are introduced with a metric that presupposes a positive correlation with the underlying quantity). Even accounting for the different sign patterns however, the estimates are too small.

Scaling by units with `X = 1` does best in this case. The key insight here is that the total variation in the latent variable combines the variation within groups and between them: we want to measure outcomes on a scale benchmarked to the within group variation and so have to take out the between group variation when rescaling. 

![Distribution of estimates from different approaches to generating indices](/figures/figure-15-7){#fig-ch15num7}

Overall we see from the diagnosis that we *can* do quite well here in recovering the conditional mean of the standardized latent variable. Though we see risks here. We declared a design under optimistic conditions in which we knew the relevant components and these were all related to the latent variable in a simple way. Even in this case we had difficulty getting the answer right. 

:::

## Design examples

- @Jefferson2022 introduces the "Respectability Politics Scale," which is a six-item additive index. Responses on 1-7 Likert scales are rescaled to the 0-1 range, then averaged.

- @broockman2016durably conduct a randomized experiment in which the main outcome is an index of attitudes toward transgender people, which is constructed by taking the first factor from a factor analysis of multiple survey questions.



