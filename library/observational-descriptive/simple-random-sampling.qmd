# Simple random sampling 

```{r}
#| echo: false
#| include: false
#| purl:  false
source("scripts/before_chapter_script.R")
```

::: {.ddbox}
We declare a design in which a researcher takes a simple random sample of a population and uses a survey instrument to measure a latent quantity. The inquiry is the population average of the measured quantity. We show how to declare this design and an approach to incorporate concerns about non random non response in the design.
:::

Descriptive inquiries like the population mean of one variable or the population correlation between two variables are defined with respect to a specific group of units -- the population about which we want to draw inferences.

One approach to studying a population is to conduct a census in which we record data on all $N$ units (written with an upper-case $N$ to represent the population). A census has the clear advantage of being comprehensive, but it usually comes at an overwhelming and prohibitive cost. 

To avoid those costs, we collect data from only $n$ units (written with a lower-case $n$), where the sample size $n$ is smaller than the population size $N$. When the $n$ units we sample are chosen at random, we can obtain unbiased estimates of many descriptive inquiries.^[But not all. @HEDAYAT2019192 prove that that unbiased estimators of the population minimum, maximum, and even median do not exist for any sampling procedure except a census. The intuition behind this result is easiest to see for a maximum: unless the random sample happens to contain the unit with the highest value of the outcome, the estimate will necessarily fall below the maximum.] 

Imagine we seek to estimate the average political ideology of adult residents of the small town of Portola, California (population 2,100). Under our model *M*, the latent ideology $Y^*$ is drawn from a standard normal distribution.

The data strategy $D$ has two components: a survey question $Q$ and a sampling procedure $S$. The survey question asks subjects to place themselves on a left-right scale that varies from 1 (most liberal) to 7 (most conservative). We approximate this measurement procedure with a function that "cuts" the latent ideology into 7 separate groups. This survey question will introduce measurement error insofar as it does not distinguish among people with different latent ideologies who, because of our measurement tool, place themselves at the same place on the seven-point scale.^[Why do we choose this strategy? Respondents may have trouble placing them on a finer scale, and we cannot ask them on an infinitely-fine scale.] Our main hope for this measurement procedure is that all of the people who give themselves higher scores are indeed more conservative than those who give themselves lower scores. The sampling procedure is "complete" random sampling. We draw a sample of exactly $n = 100$, where every member of the population has an equal probability of inclusion in the sample, $\frac{n}{N}$. 

The model and data strategy are represented by the DAG in @fig-ch15num1. The DAG shows that the observed outcome $Y$ is a function of the latent score $Y^*$ and the survey question $Q$. The observed outcome $Y$ is only measured for sampled units, i.e., units that have $S = 1$. This simple diagram represents important design assumptions. First, no arrow leads from $Y^*$ to $S$. If such an arrow were present, then units with higher or lower latent ideologies would be more likely to be sampled. Second, an arrow does lead from $Y^*$ to $Y$, indicating that we assume the measured variable does indeed respond to the latent variable. Finally, the diagram includes an explicit role for the survey question, which helps us to consider how alternative wordings of $Q$ might change the observed variable $Y$.

![Directed acyclic graph for the simple random sampling design](/figures/figure-15-1){#fig-ch15num1}

Our inquiry $I$ is the population mean of the *measured* variable $Y$: $\frac{1}{N} \sum_1^N Y_i = \bar{Y}$, rather than the mean of the latent variable $Y^*$. In this sense, our inquiry is "data-strategy dependent", since we are interested in the average value of what we *would* measure for any member of the population were we to sample them. 

Our answer strategy is the sample mean estimator: $\widehat{\overline{Y}} = \frac{1}{n} \sum_1^n Y_i$, implemented here as an ordinary least squares regression to facilitate the easy calculation of auxiliary statistics like the standard error of the estimate and the 95% confidence interval.

We incorporate these design features into @def-ch15num1. The `portola` object is a fixed population of 2100 units with a latent ideology `Y_star`. The declaration of the measurement strategy comes before the declaration of the inquiry, showing how the inquiry is data strategy dependent.  

::: {#def-ch15num1 .declaration} 

Simple random sampling design

```{r file = "scripts_declarations/declaration_15.1.R"}
```

$~$

:::

Two main diagnosands for the simple random sampling design are bias and root mean-squared error. We want to know if we get the right answer on average and we want to know, on average, how far off from the truth we are.

::: {#lem-ch15num1} 

## Simple random sampling diagnosis

```{r}
#| eval: false

diagnosands <- declare_diagnosands(
  bias = mean(estimate - estimand),
  rmse = sqrt(mean((estimate - estimand) ^ 2))
)
diagnosis_15.1 <- diagnose_design(declaration_15.1, diagnosands = diagnosands) 
```

```{r, echo = FALSE, purl = FALSE}
diagnosis_15.1 <- get_rdddr_file_local("diagnosis_15.1")
```

```{r diagnosis-15-1, echo = FALSE}
diagnosis_15.1 |>
  reshape_diagnosis() |>
  select(Bias, RMSE) |>
  kable(
    booktabs = TRUE,
    align = "c",
    digits = 3,
    caption = "Complete random sampling design diagnosis"
  )
```

The diagnosis in table @tbl-diagnosis-15-1 indicates that under complete random sampling, the sample mean estimator of the population mean is unbiased and that the root mean squared error is manageable at `r round(diagnosis_15.1$diagnosands_df$rmse,2)`.

:::

## What can go wrong

The most serious threat to descriptive inference in a randomized sampling design is nonresponse. Missingness due to nonresponse can lead to bias if missingness is correlated with outcomes. Sometimes this bias is referred to as "selection bias" in the sense that some units select out of responding when sampled.

Depending on what information is available about the missing units, various answer strategy fix-ups are available to analysts. For example, if we have demographic or other covariate information about the missing units, we can search for similar-seeming units in the observed data, then impute their missing outcomes. This approach depends on the strong assumption that units with the same covariate profile have the same average outcome, regardless of whether they go missing. The imputation process is often done on the basis of a regression model; multiple imputation methods attempt to incorporate the additional uncertainty that accompanies the modeling technique. Answer strategies that employ inverse probability weights adjust for non response by upweighting types of units we have too few of (relative to a population target) and downweighting units we have too many of.

Avoiding -- or dynamically responding to -- missingness in the data strategy is usually preferable to the addition of modeling assumptions in the answer strategy. Avoiding missingness often means adding extra effort and expense, such as monetary incentives for participation, multiple rounds of attempted contact, and attempting contact through a variety of modes (phone, mail, email, direct message, text, in-person visit). The best way to allocate extra effort will vary from context to context, as will the payoff from doing so. Our recommendation is to reason about the plausible response rates that would result from different levels of effort, then to consider how to optimize the bias-effort tradeoff. Sometimes, achieving zero bias would be far too costly, so we would be willing to tolerate some bias because effort is too expensive.

@def-ch15num2 builds in a dependence between the latent outcome $Y*$ and the probability of responding to the survey. That probability also responds to researcher effort. The diagnosis shows how effort translates into higher response rates and lower bias:

::: {#def-ch15num2 .declaration} 

Survey nonresponse design
  
```{r, file = "scripts_declarations/declaration_15.2.R"}
```

:::

::: {#lem-ch15num2} 

## Survey nonresponse diagnosis

```{r}
#| eval: false

diagnosis_15.2 <- 
  declaration_15.2 |> 
  redesign(effort = seq(0, 5, by = 0.5)) |> 
  diagnose_designs()
```

```{r, echo = FALSE, purl = FALSE}
diagnosis_15.2 <- get_rdddr_file_local("diagnosis_15.2")
```

![Redesigning the random sampling design over researcher effort](/figures/figure-15-2){#fig-ch15num2}

:::

## Design Examples

- @Bradley:2021us compare "big data" convenience sample surveys (n = 75,000 and 250,000) of COVID-19 vaccine uptake to a 1,000 person simple random sampling design with an inverse probability weighting answer strategy, finding strong support for random sampling over "big data."

- Simple random sampling is also used when researchers need to take manual measurements of a large number of observations. @merkley_stecula_2021 hand-code a simple random sample of 3,000 newspaper articles about climate changes out of the many hundreds of thousands of articles about climate change identified by an automatic search process, allowing them to characterize their population of observations without hand-coding each one.
