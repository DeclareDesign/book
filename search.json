[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Research Design",
    "section": "",
    "text": "Forthcoming, Princeton University Press."
  },
  {
    "objectID": "acknowledgements.html",
    "href": "acknowledgements.html",
    "title": "Acknowledgements",
    "section": "",
    "text": "Each of us inflicted early versions of DeclareDesign and our ideas about research design on our students and colleagues at UCLA, Yale, Columbia, and WZB Berlin, and at many summer schools and workshops. We thank our students for their patience and feedback on the tools and approach, which deeply shaped what you find in this book.\nWe held a book conference over Zoom and are grateful for the incisive feedback of Dorothy Bishop, Don Green, Nahomi Ichino, Kosuke Imai, Gary King, Andy Gelman, Felix Elwert, Molly Roberts, Cyrus Samii, and Rocio Titiunik. For feedback at our EGAP feedback session on Part I and Part IV, we thank Adam Berinsky, Jake Bowers, David Broockman, Cesi Cruz, Ryan Enos, Alex Hartman, Morgan Holmes, Ryan Moore, Pia Raffler, Dan Rubenson, and Rebecca Wolfe. Phoenix Dalto and Elayne Stecher provided excellent research assistance. We also thank the three anonymous reviewers of the manuscript for generous and helpful feedback.\nA core part of the DeclareDesign project is its software implementation in R. We were lucky to work with a big group of talented graduate students and programmers on nearly every aspect of it. We are grateful to Neal Fultz who moved the software from prototype to professional software product and who made many big contributions to how the tools work now. Luke Sonnet is responsible for the speed and technical workings of estimatr; Aaron Rudkin for many ideas in fabricatr; Clara Bicalho, Markus Konrad, and Sisi Huang for DeclareDesignWizard; and Clara Bicalho and Lily Medina for DesignLibrary.\nWe are grateful to the many people who have intensively used the software in its early versions for their generous feedback, Tom Leavitt, Daniel Rubenson, Vartika Savarna, Tara Slough, Georgiy Syunyaev, Anna Wilke, Linan Yao. Special thanks also to Dorothy Bishop and Jake Bowers for so many thoughts and suggestions.\nWe are grateful to the Laura and John Arnold Foundation, and especially Stuart Buck, for seeing the virtues of our approach and providing early funding of the project. We also thank EGAP for seed funding that got us started. We thank Lynn Vavreck for offering Alex a Hoffenberg Visiting Fellowship at UCLA to work on the software and an early version of the manuscript.\nWe thank Bridget Flannery-McCoy for shepherding this book to fruition and for her willingness to engage with us about offering a free online version of the book. We are also grateful to Eric Crahan for our early conversations about the idea for the book.\nFinally, Graeme and Alex thank Alex’s spouse Penelope Van Grinsven for warmly putting up with us for a semester in Los Angeles and a year in New Haven and many coworking sessions in the pottery studio."
  },
  {
    "objectID": "authors.html",
    "href": "authors.html",
    "title": "About the authors",
    "section": "",
    "text": "Alexander Coppock is an assistant professor of political science at Yale University and a resident fellow of the Institution for Social Policy Studies and Center for the Study of American Politics. He received his Ph.D. in political science from Columbia University (2016). His principal research interest lies in political persuasion and its implications for the malleability of public opinion in the context of elections. His interests extend beyond persuasion to the design and analysis of randomized experiments.\nMacartan Humphreys is a professor of political science at Columbia University and director of the Institutions and Political Inequality group at the WZB Berlin. He works on the political economy of development. Ongoing research focuses on post-conflict development, ethnic politics, political authority and leadership, and democratic development with a current focus on the use of field experiments to study democratic decision-making in post-conflict and developing areas. Macartan has worked in Chad, Ghana, Haiti, Indonesia, Liberia, Mali, Sao Tome and Principe, Sierra Leone, Senegal, Uganda, and elsewhere. Recent work has appeared in the American Political Science Review, Journal of Development Economics, Science Advances, and elsewhere. He has written or coauthored books on ethnic politics, natural resource management, and game theory and politics. Humphreys is a former Trudeau fellow and scholar of the Harvard Academy."
  },
  {
    "objectID": "introduction/preface.html",
    "href": "introduction/preface.html",
    "title": "\n1  Preface\n",
    "section": "",
    "text": "This book introduces a new way of thinking about research designs in the social sciences. Our hope is that this approach will make it easier to develop and to share strong research designs.\nAt the heart of our approach is the MIDA framework, in which a research design is characterized by four elements: a model, an inquiry, a data strategy, and an answer strategy. We have to understand each of the four on their own and also how they interrelate. The design encodes your beliefs about the world, it describes your questions, and it lays out how you go about answering those questions, both in terms of what data you collect and how you analyze it. In strong designs, choices made in the model and inquiry are reflected in the data and answer strategies, and vice-versa.\nWe think of designs as objects that can be interrogated. Each of the four design elements can be “declared” in computer code and – if done right – the information provided is enough to “diagnose” the quality of the design through computer simulation. Researchers can then select the best design for their purposes by “redesigning” over alternative, feasible designs.\nThis way of thinking pays dividends at multiple points in the research design lifecycle: planning the design, implementing it, and integrating the results into the broader research literature. The declaration, diagnosis, and redesign process informs choices made from the beginning to the end of a research project."
  },
  {
    "objectID": "introduction/preface.html#how-to-read-this-book",
    "href": "introduction/preface.html#how-to-read-this-book",
    "title": "\n1  Preface\n",
    "section": "\n1.1 How to read this book",
    "text": "1.1 How to read this book\nWe had multiple audiences in mind when writing this book. First, we’re thinking of people looking for a high-level introduction to these ideas. If we only had 30 minutes with a person to try and communicate the core ideas, we would give them Part I. We’re thinking of people who are new to the practice of research design and who are embarking on their first empirical projects. The MIDA framework introduced in Part I accommodates many different empirical approaches: qualitative and quantitative, descriptive and causal, observational and experimental. Beginners starting out in any of these traditions can use our framework to consider how the design elements in those approaches fit together. We’re also thinking of researchers-in-training: graduate students in seminar courses where the main purpose is to read papers and discuss the credibility of research findings. Such discussions can sometimes feel like a laundry list of complaints, but we hope our framework can focus attention on the most relevant concerns. What, exactly, is the inquiry? Is it the right one to be posing. Are the data and answer strategies suited to the inquiry? We’re also thinking of funders and decision-makers, who often wish to assess research not in terms of its results but its design. Our approach provides a way of defining the design and diagnosing its quality.\nPart II is more involved. We provide the formal foundations of the MIDA framework. We walk through each component of a research design in detail, describe the finer points of design diagnosis, and explain how to carry out a “redesign.” Part II will resonate with several audiences of applied researchers both inside and outside of academia. We imagine it could be assigned early in a graduate course on research design in any of the social sciences. We hope data scientists and monitoring and evaluation professionals will find value in our framework for learning about research designs. Scholars will find value in declaring, diagnosing, and redesigning designs whether they are implementing randomized trials, multi-method archival studies, or calibrating structural theories with data.\nIn Part III, we apply the general framework to specific research designs. The result is a library of common designs. Many empirical research designs are included in the library, but not all. The set of entries covers a large portion of what we see in current empirical practice across social sciences, but it is not meant to be exhaustive. We don’t expect that any readers will read straight through the design library, but will instead pick-and-choose depending on their interests.\nWe are thinking of three kinds of uses for entries in the design library. Collectively, the design entries serve to illustrate the fundamental principles of design. The entries clarify the variety of ways in which models, inquiries, data strategies, and answer strategies can be connected and show how high level principles operate in common ways across very different designs. The second use is pedagogical. The library entries provide hands-on illustrations of designs in action. A researcher interested in understanding the “regression discontinuity design,” for example, can quickly see a complete implementation and learn under what conditions the standard design performs well or poorly. They can also compare the suitability of one type of design against another for a given problem. We emphasize that these descriptions of different designs provide entry points but they are not exhaustive, so we refer readers to recent methodological treatments of the different topics. The third use is as a starter kit to help readers get going on designs of their own. Each entry includes code for a basic design that can be fine-tuned to capture the specificities of particular research settings.\nThe last section of the book describes how our framework can help at different stages of the research process. Each of these sections should be readable for anyone who has read Part I. The entry on preanalysis plans, for example, can be assigned in an experiments course as guidance for students filing their first preanalysis plan. The entry on research ethics could be shared among coauthors at the start of a project. The entry on writing a research paper could be assigned to college seniors writing their first original research papers."
  },
  {
    "objectID": "introduction/preface.html#how-to-work-this-book",
    "href": "introduction/preface.html#how-to-work-this-book",
    "title": "\n1  Preface\n",
    "section": "\n1.2 How to work this book",
    "text": "1.2 How to work this book\nWe will often describe research designs not just in words, but in computer code. If you want to work through the code and exercises, fantastic. This path requires investment in R, the tidyverse, and the DeclareDesign software package. Chapter 4 helps get you started. We think working through the code is very rewarding, but we understand that there is a learning curve. You could, of course, tackle the declaration, diagnosis, and redesign processes using any computer language you like,1 but it is easier in DeclareDesign because the software guides you to articulate each of the four design elements.\nIf you want nothing to do with the code, you can skip it and just focus on the text. We have written the book so that understanding of the code is not required in order to understand research design concepts."
  },
  {
    "objectID": "introduction/preface.html#what-this-book-will-not-do",
    "href": "introduction/preface.html#what-this-book-will-not-do",
    "title": "\n1  Preface\n",
    "section": "\n1.3 What this book will not do",
    "text": "1.3 What this book will not do\nThis is a research design book, not a statistics textbook, or a cookbook with recipes applicable to all situations. We will not derive estimators, we will provide no guarantees of the general optimality of designs, and we will present no mathematical proofs. Nor will we provide all the answers to all the practical questions you might have about your design.\nWhat we do offer is a language to express research designs. We can help you learn that language so you can describe your own design in it. When you can declare your design in this language, then you can diagnose it, figure out if it works the way you think it should, and then improve it through redesign."
  },
  {
    "objectID": "introduction/what-is-a-research-design.html",
    "href": "introduction/what-is-a-research-design.html",
    "title": "\n2  What is a research design?\n",
    "section": "",
    "text": "At its heart, a research design is a procedure for generating answers to questions. Strong designs yield answers that are close to their targets, but weak designs can produce answers that are biased, imprecise, or just irrelevant. Assessing whether a design is strong requires having a clear sense of what the question to be answered is and understanding how the empirical information generated or collected by the design will lead to reliable answers. This book offers a language for describing research designs and an algorithm for selecting among them. In other words, it provides a set of tools for characterizing and evaluating the dozens of choices we make in our research activities that together determine the strength of our designs. Throughout, we keep our focus on empirical research designs—designs that seek to answer questions that are answerable with data—and we use the term “research design” as a short hand for these.\nWe show that the same basic language can be used to represent research designs whether they target causal or descriptive questions, whether they are focused on theory testing or inductive learning, and whether they use quantitative, qualitative, or a mix of methods. We can select a strong design by applying a simple algorithm: declare-diagnose-redesign. Once a design is declared in simple enough language that a computer can understand it, its properties can be diagnosed through simulation. We can then engage in redesign, or the exploration of a range of neighboring designs. The same language we use to talk to the computer can be used to talk to others. Reviewers, advisers, students, funders, journalists, and the public need to know four basic things to understand a design."
  },
  {
    "objectID": "introduction/what-is-a-research-design.html#mida-the-four-elements-of-a-research-design",
    "href": "introduction/what-is-a-research-design.html#mida-the-four-elements-of-a-research-design",
    "title": "\n2  What is a research design?\n",
    "section": "\n2.1 MIDA: The four elements of a research design",
    "text": "2.1 MIDA: The four elements of a research design\nResearch designs share in common that they all have an inquiry I, a data strategy D, and an answer strategy A. Less obviously, perhaps, these three elements presuppose a model M of how the world works. We refer to the four together as MIDA.\nWe think of MIDA as having two sides. M and I form the theoretical half, comprising your beliefs about the world and your target of inference. D and A form the empirical half, comprising your strategies for collecting and summarizing information. The theoretical side sets the research challenges for you to overcome and the empirical side captures your responses to those challenges.1\nFigure 2.1 shows how these four elements of a design relate to one another, how they relate to real world quantities, and how they relate to simulated quantities. We will unpack this figure in the remainder of this chapter, highlighting two especially important parallelisms, first between the upper and lower halves representing actual processes and simulated processes and second between the left (M, I) and right (D, A) halves representing these theoretical and empirical sides of research designs.\n\n\n\nFigure 2.1: The MIDA framework. An arrow between two points means that the point at the end of an arrow depends in some way on the one at the start of the arrow. For instance ‘the answer you’ll get’ depends on the dataset you’ll get and the answer strategy you specify.\n\n\n\n2.1.1 Model\nThe set of models in M comprises speculations about what causes what and how. It includes guesses about how important variables are generated, how they are correlated, and the sequences of events.\nThe M in MIDA does not necessarily represent our beliefs about how the world actually works. Instead, it describes a set of possible worlds in enough detail that we can assess how our design would perform if the real world worked like those in M. For this reason we sometimes refer to M as a set of “reference” models. Assessment of the quality of a design is carried out with reference to the models of the world that we provide in M. In other contexts, we might see M described as as the “data generating process.” We prefer to describe M as the “event generating process” to honor the fact that data are produced or gathered via a data strategy — and the resulting data are measurements taken of the events generated by the true causal model of the world.\nWe are conscious that the term “model” is used in many different ways by researchers and so a little disambiguation is helpful. Our use of the term when discussing M—as a representation of how the world works for the purposes of posing questions and assessing strategies—contrasts with two other usages. First, in some usages, the model is the object of inquiry: our goal in research is to select a model of the world that provides a useful representation of the world. In this sense a model might be the output of A. We will discuss such approaches and when we do so we will make clear how such models serve a function distinct from M. Second, researcher commonly use “model” to describe a representation of event generating processes used specifically for the purpose of generating estimates. For instance, researchers might use a “a linear probability model” or an “ordered probit model.” Such “statistical models” might be justified on the grounds that they reflect beliefs how the world works, but they might also be used simply because they are helpful in generating answers to questions. We think it clearer to think of these models as part of A. They are part of the method used to answer questions given data. We can then assess, for a given research question, whether the answer strategy provides good answers, whether or not the model assumed by the statistical procedure is consistent with M.\n\n\n2.1.1.1 What’s in a model?\nThe model has two responsibilities. First, the model provides a setting within which a question can be answered. The inquiry I should be answerable under the model. If the inquiry is the average difference between two potential outcomes, those two potential outcomes should be described in the model. Second, the model governs what data can be produced by any given data strategy D. The data that might be produced by a data strategy D should be foreseeable under the model. For example, if the data strategy includes random sampling of units from a population and measurement of an outcome, the model should describe the outcome variable for all units in that population.\nThese responsibilities in turn determine what needs to be in the model. In general, the model defines a set of units that we wish to study. Often, this set of units is larger than the set of units that we will actually study empirically, but we can nevertheless define this larger set about which we seek to make inferences. The units might be all of the citizens in Lagos, Nigeria, or every police beat in Delhi. The set may be restricted to the mayors of cities in California or the catchment areas of schools in rural Poland. The model also includes information about characteristics of those units: how many of each kind of unit there are and how features of the units may be correlated.\nFor descriptive and causal questions alike, we usually imagine causal models. Even if questions are fundamentally descriptive, they can be usefully posed in the context of a causal model, because causal models can explain the level of variables and not simply the nature of effects.\nCausal models (see for instance Pearl and Mackenzie 2018) include a set of exogenous and endogenous variables as well as functions that describe the values endogenous variables take depending on the values of other variables. If we think of one variable influencing another, we think of the first as a treatment variable that specifies a condition and the second as an outcome variable. Treatments might be delivered naturally by the world or may be assigned by researchers. The values that an outcome variable would take depending on the level of a treatment are called potential outcomes. In the simplest case of a binary treatment, the treated potential outcome is what would arise if the unit were treated; the untreated potential outcome if it were not. Both potential outcomes are part of the model.\nSummarizing, we can think of three functions of a model that characterize units, conditions, and outcomes: an identification of a population, a conjecture of values of exogenous variable—conditions, and a description of the values of endogenous variables—outcomes—given the values of other variables on which they depend.\n\n2.1.1.2 M as a set.\nIn Figure 2.1, we describe M as the “the worlds you’ll consider.” The reason for this is that we are uncertain about how the world works. As scientists, we are skeptical of easy assertions about what the right model is and we freely admit we don’t know the true causal model of the world. When conducting empirical research into the true model, we have to think through how our design would play out under different possible models, including ones we think more likely and those we think less likely. For instance, the correlation between two variables might be large and positive, but it could just as well be zero. We might believe that, conditional on some background variables, a treatment has been as-if randomly assigned by the world—but we might be wrong about that too. In the figure we use \\(m^*\\) to denote the true model, or the actual, unknown, event generating process. We do not have access to \\(m^*\\), but our hope is that \\(m^*\\) is sufficiently well-represented in M so that we can reasonably imagine what will happen when our design is applied in the real world.\nHow can we construct a sufficiently varied set of models of the world? For this we can draw on existing data from past studies or on new information gathered from pilot studies. Reducing uncertainty over the set of possible models is a core purpose of theoretical reflection, literature review, meta-analysis, and formative research. If there are important known features about your context it generally makes sense to include them in \\(M\\).\n\n\nExamples of models\n\n\nContact theory: When two members of different groups come into contact under specific conditions, they learn more about each other, which reduces prejudice, which in turn reduces discrimination.\nPrisoner’s dilemma. When facing a collective action problem, each of two people will choose non-cooperative actions independent of what the other will do.\nHealth intervention with externalities. When individuals receive deworming medication, school attendance rates increase for them and for their neighbors, leading to improved labor market outcomes in the long run.\n\n\n\n2.1.2 Inquiry\nThe inquiry is a research question stated in terms of the model. For example, the inquiry might be the average causal effect of one variable on another, the descriptive distribution of a third variable, or a prediction about the value of a variable in the future. We refer to “the” inquiry when talking about the main research question, but in practice we may seek to learn about many inquiries in a single research study.\nMany people use the word “estimand” to refer to an inquiry, and we do too when casually talking about research. When we are formally describing research designs, however, we distinguish between inquiries and estimands and Figure 2.1 shows why. The inquiry I is the function that operates on the events generated (or conjectured to be generated) by the real world \\(m^*\\) or a simulated world \\(m\\). The estimand is the value of that function: \\(a_{m^*}\\) or \\(a_{m}\\). In other words, we use “inquiry” to refer to the question and “estimand” to refer to the answer to the question.\nAs with models, inquiries are also defined with respect to units, conditions, and outcomes: they are summaries of outcomes of units in or across conditions. Inquiries may be causal, as in the sample average treatment effect (SATE). The SATE is the average difference in treated and untreated potential outcomes among units in a sample. Inquiries may also be descriptive, as in a population average of an outcome. While it may seem that descriptive inquiries do not involve conditions, they always do, since the realization of outcomes must take place under a particular set of circumstances, often set by the world and not the researcher.\nFigure 2.1 shows that when I is applied to a model \\(m\\), it produces an answer \\(a^m\\). This set of relationships forces discipline on both M and I: I needs to be able to return an answer using information available from M and in turn M needs to provide enough information so that I can do its job.\n\n\nExamples of inquiries\n\n\nWhat proportion of voters live with limited exposure to voters from another party in their neighborhood?\n\n\nDoes gaining political office make divorce more likely?\n\n\nWhat types of people will benefit most from a vaccine?\n\n\n\n2.1.3 Data strategy\nThe data strategy is the full set of procedures we use to gather information from the world. The three basic elements of data strategies parallel the three features of inquiries: units are selected, conditions are assigned, and outcomes are measured.\nAll data strategies require an identification of units. Many involve sampling, gathering data on a subset of units specified by a model or by an inquiry.\nData strategies also involve conditions. Most obviously, experimental interventions are used to produce controlled variation in conditions. If we present some subjects with one piece of information and other subjects with a different piece of information, we’ve generated variation on the basis of an assignment procedure. Observational approaches often seek to do something similar, selecting units so that natural variation can be exploited. In such cases units are often selected for study because of the conditions that they are in.\nMeasurement procedures are the ways in which researchers reduce the complex and multidimensional social world into a parsimonious set of empirical data. These data need not be quantitative data in the sense of being numbers or values on a pre-defined scale; qualitative data are data too. Measurement is the vexing but necessary reduction of reality to a few choice representations.\nFigure 2.1 shows how the data strategy is applied to both the imagined worlds in M and to the real world. When D is applied to the real world (\\(m^*\\)), we obtain the realized dataset \\(d^*\\). When D is applied to the worlds we imagine in M, we obtain simulated datasets, which may or may not be anything like the dataset \\(d^*\\) we would really get. When our models M more accurately represent the real world, our simulated datasets will look more like the real data we will eventually collect.\n\n\nExamples of data strategies\n\nSampling procedures.\n\nRandom digit dial sampling of 500 voters in the Netherlands\nRespondent-driven sampling of people who are HIV positive, starting from a sample of HIV-positive individuals\n“Mall intercept” convenience sampling of men and women present at the mall on a Saturday\n\nTreatment assignment procedures.\n\nRandom assignment of free legal assistance intervention for detainees held in pretrial detention\nNature’s assignment of the sex of a child at birth\n\nMeasurement procedures.\n\nVoting behavior gathered from survey responses\nAdministrative data indicating voter registration\nMeasurement of stress using Cortisol readings\n\n\n\n2.1.4 Answer strategy\nThe answer strategy is what we use to summarize the data produced by the data strategy. Just like the inquiry summarizes a part of the model, the answer strategy summarizes a part of the data. We can’t just “let the data speak” because complex, multidimensional datasets don’t speak for themselves—they need to be summarized and explained. Answer strategies are the procedures we follow to do so.\nAnswer strategies are functions that take in data and return answers. For some research designs, this is a literal function like the R function lm_robust that implements an ordinary least squares (OLS) regression with robust standard errors. For some research designs, the function is embodied by the researchers themselves when they read documents and summarize their meanings in a case study.\nThe answer strategy is more than the choice of an estimator. It includes the full set of procedures that begins with cleaning the dataset and ends with answers in words, tables, and graphs. These activities include data cleaning, data transformation, estimation, plotting, and interpretation. Not only do we define our choice of OLS as the estimator, we also specify that we will focus attention on a particular coefficient estimate, assess uncertainty using a 95% confidence interval, and construct a coefficient plot to visualize the inference. The answer strategy also includes all of the if-then procedures that researchers implicitly or explicitly follow depending on initial results and features of the data. For example, in a stepwise regression procedure, the answer strategy is not the final regression specification that results from iterative model selection, but that whole procedure.\nD and A impose a discipline on each other in the same way as we saw with M and I. Just like the model needs to provide the events that are summarized by the inquiry, the data strategy needs to provide the data that are summarized by the answer strategy. Declaring each of these parts in detail reveals the dependencies across the design elements.\nA and I also enjoy a tight connection stemming from the more general parallelism between (M, I) and (D, A). We elaborate the principle of parallel inquiries and answer strategies in Section 9.3.\nFigure 2.1 shows how the same answer strategy A is applied both to the realized data \\(d^*\\) and also to the simulated data \\(d\\). We know that in practice, however, the A applied to the real data differs somewhat from the A applied to the data we plan for via simulation. Designs sometimes drift in response to data, but too much drift and the inferences we draw can become misleading. The MIDA framework encourages researchers to think through what the real data will actually look like, and adjust A accordingly before data strategies are implemented.\n\n\nExamples of answer strategies\n\n\nMultilevel modeling and poststratification\nBayesian process tracing\nDifference-in-means estimation"
  },
  {
    "objectID": "introduction/what-is-a-research-design.html#declaration-diagnosis-redesign",
    "href": "introduction/what-is-a-research-design.html#declaration-diagnosis-redesign",
    "title": "\n2  What is a research design?\n",
    "section": "\n2.2 Declaration, diagnosis, redesign",
    "text": "2.2 Declaration, diagnosis, redesign\nWith the core elements of a design described, we are now ready to lay out the process of declaration, diagnosis, and redesign.\n\n2.2.1 Declaration\nDeclaring a design entails figuring out which parts of your design belong in M, I, D, and A. The declaration process can be a challenge because mapping our ideas and excitement about a project into MIDA is not always straightforward, but it is rewarding. When we can express a research design in terms of these four components, we are newly able to think about its properties.\nDesigns can be declared in words, but declarations often become much more specific when carried out in code. You can declare a design in any statistical programming language: Stata, R, Python, Julia, SPSS, SAS, Mathematica, or many others. Design declaration is even possible – though somewhat awkward – in Excel. We wrote the companion software, DeclareDesign, in R because of the availability of other useful tools in R and because it is free, open-source, and high-quality. We have designed the book so that you can read it even if you do not use R, but you will have to translate the code into your own language of choice. On our Web site, we have pointers for how you might declare designs in Stata, Python, and Excel. In addition, we link to a “Design wizard” that lets you declare and diagnose variations of standard designs via a point-and-click web interface. Chapter Chapter 4 provides an introduction to DeclareDesign in R.\n\n2.2.2 Diagnosis\nOnce you’ve declared your design, you can diagnose it. Design diagnosis is the process of simulating a research design in order to understand the range of ways the study could turn out. Each run of the design comes out differently because different units are sampled, or the randomization allocated different units to treatment, or outcomes were measured with different error. We let computers do the simulations for us because imagining the full set of possibilities is – to put it mildly – cognitively demanding.\nDiagnosis is the process of assessing the properties of designs, and represents an opportunity to write down what would make the study a success. For a long time, researchers have classified studies as successful or not based on statistical significance (Chopra et al. 2022). If significant, the study “worked”; if not, it is a failed “null.” Accordingly, statistical power (the probability of a statistically significant result) has been the most front-of-mind design property when researchers plan studies. As we learn more about the pathologies of relying on statistical significance, we learn that features beyond power are more important. For example, the “credibility revolution” throughout the social sciences has trained a laser-like focus on the biases that may result from omitted or “lurking” variables.\nDesign diagnosis relies on two new concepts: diagnostic statistics and diagnosands.\nA “diagnostic statistic” is a summary statistic generated from a single “run” of a design. For example, the statistic \\(e\\) (error) refers to the difference between the estimate and the estimand. The statistic \\(s\\) (significance) refers to whether the estimate was deemed statistically significant at the 0.05 level (for instance).\nA “diagnosand” is a summary of the distribution of a diagnostic statistic across many simulations of the design. The bias diagnosand is defined as the average value of the \\(e\\) statistic and the power diagnosand is defined as the average value of the \\(s\\) statistic. Other diagnosands include quantities like root-mean-squared-error (RMSE), Type I and Type II error rates, how likely is it that subjects were harmed, and average cost. We describe these diagnosands in much more detail in ?sec-ch11.\nOne especially important diagnosand is the “success rate,” which is the average value of the “success” diagnostic statistic. As the researcher, you get to decide what would make your study a success. What matters most in your research scenario? Is it statistical significance? If so, optimize your design with respect to power. Is what matters most whether the answer has the correct sign or not? Then diagnose how frequently your answer strategy yields an answer with the same sign as your inquiry. Diagnosis involves articulating what would make your study a success and then figuring out, through simulation, how often you obtain that success. Success is often a multidimensional aggregation of diagnosands, such as the joint achievement of high statistical power, manageable costs, and low ethical harms.\nWe diagnose studies over the range of possibilities in the model, since we want to learn the value of diagnosands under many possible scenarios. A clear example of this is the power diagnosand over many possible conjectures about the true effect size. For each effect size that we entertain in the model, we can calculate statistical power. The minimum detectable effect size is a summary of this power curve, usually defined as the smallest effect size at which the design reaches 80% statistical power. This idea, however, extends well beyond power. Whatever the set of important diagnosands, we want to ensure that our design performs well across all model possibilities.\nComputer simulation is not the only way to do design diagnosis. Designs can be declared in writing or mathematical notation and then diagnosed using analytic formulas. Enormous theoretical progress in the study of research design has been made with this approach. Methodologists across the social sciences have described diagnosands such as bias, power, and root-mean-squared-error for large classes of designs. Not only can this work provide closed-form mathematical expressions for many diagnosands, it can also yield insights about the pitfalls to watch out for when constructing similar designs. That said, pen-and-paper diagnosis is challenging for many social science research designs, first because many designs as actually implemented have idiosyncratic features that are hard to incorporate and second because the analytic formulas for many diagnosands have not yet been worked out by statisticians. For this reason, when we do diagnosis in this book we will usually depend on simulation.\nEven when using simulation, design diagnosis doesn’t solve every problem and like any tool, it can be misused. We outline two main concerns. The first is the worry that the diagnoses are plain wrong. Given that design declaration includes conjectures about the world, it is possible to choose inputs such that a design passes any diagnostic test set for it. For instance, a simulation-based claim of unbiasedness that incorporates all features of a design is still only good with respect to the precise conditions of the simulation. In contrast, analytic results, when available, may extend over general classes of designs. Still worse, simulation parameters might be chosen opportunistically. Power analysis is useless if implausible parameters are chosen to raise power artificially. While our framework may encourage more principled declarations, it does not guarantee good practice. As ever, garbage-in, garbage-out. The second concern is the risk that research may be evaluated on the basis of a narrow or inappropriate set of diagnosands. Statistical power is often invoked as a key design feature, but well-powered studies that are biased are of little theoretical use. The importance of particular diagnosands can depend on the values of others in complex ways, so researchers should take care to evaluate their studies along many dimensions.\n\n2.2.3 Redesign\nOnce your design has been declared, and you have diagnosed it with respect to the most important diagnosands, the last step is redesign.\nRedesign entails fine-tuning features of the data and answer strategies to understand how they change your diagnosands. Most diagnosands depend on features of the data strategy. We can redesign the study by varying the sample size to determine how big it needs to be to achieve a target diagnosand: 90% power, say, or an RMSE of 0.02. We could also vary an aspect of the answer strategy, such as the choice of covariates used to adjust a regression model. Sometimes the changes to the data and answer strategies interact. For example, if we want to use covariates that increase the precision of the estimates in the answer strategy, we have to collect that information as a part of the data strategy. The redesign question now becomes, is it better to collect pretreatment information from all subjects or is the money better spent on increasing the total number of subjects and only measuring posttreatment?\nThe redesign process is mainly about optimizing research designs given ethical, logistical, and financial constraints. If diagnosands such as total harm to subjects, total researcher hours, or total project cost exceed acceptable levels, the design is not feasible. We want to choose the best design we can among the feasible set. If the designs remaining in the feasible set are underpowered, biased, or are otherwise scientifically inadequate, the project may need to be abandoned.\nIn our experience, it’s during the redesign process that designs become simpler. We learn that our experiment has too many arms or that the expected level of heterogeneity is too small to be detected by our design. We learn that in our theoretical excitement, we’ve built a design with too many bells and too many whistles. Some of the complexity needs to be cut, or the whole design will be a muddle. The upshot of many redesign sessions is that our designs pose fewer questions but obtain better answers."
  },
  {
    "objectID": "introduction/what-is-a-research-design.html#example-a-decision-problem",
    "href": "introduction/what-is-a-research-design.html#example-a-decision-problem",
    "title": "\n2  What is a research design?\n",
    "section": "\n2.3 Example: A decision problem",
    "text": "2.3 Example: A decision problem\nImagine you have been hired by the Los Angeles County Sheriff’s Department as an independent expert to study whether a new policy — implicit bias training — changes social norms of officers or is merely window dressing. You are given a research budget of $3,000 to run a randomized experiment to test out the training program. The Sheriff commits to scale up the training program across the force if you find it shifts norms by at least 0.3 standard units and otherwise it will not be implemented more widely. They are also enamored by classical statistical testing so they will only go forward if your estimates are statistically significant. Though we describe this particular setting to fix ideas, we think this example is relevant for many decision problems in which the results of a study will inform implementation.\nYou will consider the experiment if you recommend implementing the training and it is indeed effective (which in this example we will take to mean that there is in fact an effect of at least 0.2). But if you recommend not adopting the training and it is effective, you consider it a failure because you could have reduced police abuse, and if you recommend adopting it but it was not effective that is also a failure because resources were spent that could have been used on an effective intervention.\nFor the experiment itself, you’re deciding between two designs. In one you run a study with 150 officers, randomly assign half to receive the training and half not, then compare outcomes in treatment and control using a survey about their perceived norms of reporting. In the second, you spend part of your funding gathering background information on the officers—whether they have been investigated in the past by internal affairs and were found to have discriminated against citizens—and use that information to improve both randomization and inference. Let’s suppose the two designs cost exactly the same amount. Interviewing each officer at endline costs $20, so the total cost of the larger trial is 150 * 20 = 3000. The block-randomized design costs the same for endline measurement, but measurement of the history variable from police administrative records costs $10 per individual because you have to go through the Department’s archives which are not digitized, so the total is the same: 100 * 20 + 100 * 10 = 3000.\nThe two designs cost the same but differ on the empirical side. Which strategy should you use, given your goals?\n\n2.3.1 Design 1: N = 150, complete random assignment\n\nM: We first define a model that stipulates a set of 18,000 units representing each sheriff’s deputy and an unknown treatment effect of the training lying somewhere between 0 and 0.5. This range of possible effects implies in 60% of our the models we consider, the true effect is above our threshold for a program worth implementing, 0.2. Outcomes for each individual depend on their past infractions against citizens (their history). The importance of history is captured by the parameter b. We don’t know how important the history variable is, so we will simulate over a plausible range for b. M here is a set of models as each “run” of the model will presuppose a different treatment effect for all subjects as well as distinct outcomes for all individuals.\nI: The inquiry is the difference between the average treated outcome and the average untreated outcome, which correspond to the average treatment effect. We are writing it this way (as opposed to the equivalent ATE = mean(Y_Z_1 - Y_Z_0)) to highlight the similarity between the inquiry and the difference-in-means answer strategy that we will adopt.\nD: We imagine a data strategy with three components relating to units, conditions, and outcomes: we sample 100 deputies to participate in the experiment, assign exactly half to treatment and the remainder to control, and finally measure their outcomes through a survey.\nA: The answer strategy takes the difference-in-means between the treated and untreated units. Thus the answer strategy uses a function similar to the inquiry itself.\n\nWhen we put these all together we have a design, Declaration 2.1.\n\nDeclaration 2.1 Two-arm trial design\n\nb <- 0\n\nmodel <- \n  declare_model(\n    N = 1000,\n    history = sample(c(0, 1), N, replace = TRUE),\n    potential_outcomes(Y ~ b * history + runif(1, 0, 0.5) * Z + rnorm(N))) \n\ninquiry <-\n  declare_inquiry(ATE = mean(Y_Z_1) - mean(Y_Z_0))\n\ndata_strategy <-\n  declare_sampling(S = complete_rs(N = N, n = 150), filter = S == 1) +\n  declare_assignment(Z = complete_ra(N)) +\n  declare_measurement(Y = reveal_outcomes(Y ~ Z)) \n\nanswer_strategy <-\n  declare_estimator(Y ~ Z, .method = difference_in_means, inquiry = \"ATE\")\n\ndeclaration_2.1 <- model + inquiry + data_strategy + answer_strategy\n\n\nThe design is now ready to be used, diagnosed, developed. We can generate simulated data directly from the design using draw_data(design_1). We show a snapshot of such simulated data below in Table 2.1.\n\n\n\n\n\nTable 2.1:  Simulated data from two-arm trial design \n \n ID \n    history \n    Y_Z_0 \n    Y_Z_1 \n    S \n    Z \n    Y \n  \n\n\n 0003 \n    0 \n    -2.01 \n    1.30 \n    1 \n    1 \n    1.30 \n  \n\n 0015 \n    0 \n    1.33 \n    0.77 \n    1 \n    1 \n    0.77 \n  \n\n 0017 \n    1 \n    -1.07 \n    -0.86 \n    1 \n    1 \n    -0.86 \n  \n\n 0021 \n    0 \n    -0.39 \n    3.09 \n    1 \n    0 \n    -0.39 \n  \n\n 0024 \n    0 \n    1.01 \n    1.45 \n    1 \n    1 \n    1.45 \n  \n\n 0034 \n    1 \n    -0.69 \n    0.77 \n    1 \n    1 \n    0.77 \n  \n\n\n\n\n\nTo evaluate the design, we need to specify our criteria for what counts as a good design. We could assess the design in terms of its statistical power, whether estimation is unbiased and so on. For now though we will focus on a specific design characteristic, its “success rate.” Here we call our study a “success” if we choose to scale-up the training across the force and its true effect is at least 0.2 (meaning it is in fact effective). If the true effect is below 0.2 or we if decide not to implement, we’ll deem the study “not a success.” Our decision to implement the program or not is based on the empirical results for the experiment. If we obtain a statistically significant estimate that is greater than 0.3 (the empirical threshold we agreed to with the Sheriff), we implement and we don’t otherwise.2 The reason we choose a higher standard of evidence, 0.3, than the effectiveness is that the training is costly so we want to be sure we have enough evidence it works.\nWe specify the criteria for success in this call to declare_diagnosands:\n\nprogram_diagnosands  <- \n  declare_diagnosands(\n    success = mean(estimate > 0.3 & p.value < 0.05 & estimand > 0.2)\n  )\n\n\n2.3.2 Design 2: N = 100, baseline measurement, block random assignment\nThe alternative design that differs on the empirical side in three ways. First fewer subjects are sampled. Second, information about the subjects’ background information (their “history”) is used to implement a block randomization that conditions assignment on history. Third the subjects’ history is taken into account in the analysis. This choice is an instance of adjusting the answer strategy in light of a change to the data strategy.\nIn Declaration 2.2, we can leave the model and inquiry intact and but we have to work on the data and answer strategies.\n\nDeclaration 2.2 A design that exploits background information\n\ndata_strategy_2 <-\n  declare_sampling(S = complete_rs(N = N, n = 100), \n                   filter = S == 1) +\n  declare_assignment(Z = block_ra(blocks = history)) +\n  declare_measurement(Y = reveal_outcomes(Y ~ Z)) \n\nanswer_strategy_2 <-\n  declare_estimator(Y ~ Z, .method = difference_in_means, \n                    blocks = history, inquiry = \"ATE\")\n\ndeclaration_2.2 <- \n  model + inquiry + data_strategy_2 + answer_strategy_2\n\n\n\n2.3.3 Diagnosis and comparison\nWe can then diagnose both designs over a series of conjectured values for the importance of history (b) and see how they perform on our specified criterion for success.\n\ndeclaration_2.1 |> \n  redesign(b = seq(0,3,0.25)) |> \n  diagnose_design(diagnosands = program_diagnosands)\n\ndeclaration_2.2 |> \n  redesign(b = seq(0,3,0.25)) |> \n  diagnose_design(diagnosands = program_diagnosands)\n\nThe results are shown in the next figure.\n\nDiagnosis 2.1 (Diagnosis of declaration_2.1 and declaration_2.2) \n\n\nFigure 2.2: How success depends on choice of D and A given different possibilities for M\n\n\n\nWhen background factors don’t make much of a difference for the social norms outcome, the first design outperforms the second: after all, the first design has a sample size of 150 compared with the second design’s 100. We’re successful over 30% of the time when using the first design, compare with about 25% of the time using the second. These rates seem low, but recall the treatment effect variation we built into the model implies that the program is only worth implementing 60% of the time, because the other 40% of the time, the true effects are smaller than 0.2.\nAs subject history has a bigger impact on the outcome variable, however, the first design does worse and worse. In essence, the additional variation due to background factors makes it more difficult to separate signal from noise, making it more likely that our estimates are nonsigificant and therefore more likely that we decline to implement the program.\nHere is where the smaller design that blocks on subject history shines: this variation is conditioned on in two places, in the assignment strategy and in the estimator. The result is a more precise procedure that is better able to separate signal from noise. Ultimately, the blocked design has the same success rate regardless of the importance of the background factors.\nThe overall result of this declaration, diagnosis, and redesign process is that which design you choose depends on beliefs about the importance of background conditions for outcomes. Now the design question hinges on something you can go learn about: how much variation is explained by subject history?\n\n2.3.4 Three principles\nWe see from this example the gains from entertaining a diverse model set rather than presupposing we already know M. We also see an example of design parts tailored to each other, most importantly the adjustment of answer strategies in light of data strategies. And we see that design choices are informed by a clear specification of a success criterion. In the next chapter we develop these three features as broader principles, referring to them Principle 3.1: Design holistically, Principle 3.2: Design agnostically, and Principle 3.3: Design for purpose."
  },
  {
    "objectID": "introduction/what-is-a-research-design.html#putting-designs-to-use",
    "href": "introduction/what-is-a-research-design.html#putting-designs-to-use",
    "title": "\n2  What is a research design?\n",
    "section": "\n2.4 Putting designs to use",
    "text": "2.4 Putting designs to use\nThe two pillars of our approach are the language for describing research designs (MIDA) and the algorithm for selecting high-quality designs (declare, diagnose, redesign). Together, these two ideas can shape research design decisions throughout the lifecycle of a project. The full set of implications is drawn out in Part IV but we emphasize the most important ones here.\nBroadly speaking, the lifecycle of an empirical research project has three phases: planning, realization, and integration. Having a clear characterization of your design in terms of MIDA is helpful in all three of these stages.\n\n2.4.1 Planning, realization, integration\nPlanning entails some or all of the following steps, depending on the design: conducting an ethical review, seeking human subjects approval, gathering criticism from colleagues and mentors, running pilot studies, and preparing preanalysis documents. The design as encapsulated by MIDA will go through many iterations and refinements during this period, but the goal is simple: to assess whether your data strategy and answer strategy are capable of providing reliable answers to your inquiry given different models that you might entertain. Planning is the time when frequent re-application of the declare, diagnose, redesign algorithm will pay the highest dividends. How should we investigate the ethics of a study? By casting the ethical costs and benefits as diagnosands. How should we respond to criticism, constructive or not? By re-interpreting the feedback in terms of M, I, D, and A. How can we convince funders and partners that our research project is worth investing in? By credibly communicating our study’s diagnosands: its statistical power, its unbiasedness, and its high chance of success, however the partner or funder defines it. What belongs in a pre-analysis plan? You guessed it – a specification of the model, inquiry, data strategy, and answer strategy.\nRealization is the phase of research in which all those plans are executed. We implement the data strategy in order to gather information from the world. Once that’s done, we follow the answer strategy in order to finally generate answers to the inquiry. Of course, that’s only if things go exactly according to plan, which they never do. Survey questions don’t work as we imagine, partner organizations lose interest in our study, subjects move or become otherwise unreachable. A critic or a reviewer may insist we change our answer strategy, or may think a different inquiry altogether is theoretically appropriate. We may ourselves change how we think of the design as we embark on writing up the research project. It is likely that some features of MIDA will change during the realization phase in which case you can again use diagnosis to assess whether changes to MIDA are for good or for bad. Some design changes have very bad properties, like sifting through the data ex-post, finding a statistically significant result, then back-fitting a new I to match the new A. Indeed, if we declare and diagnose this actual answer strategy (sifting through data ex-post), we can show through design diagnosis that it is badly biased. Other changes made along the way may help the design quite a bit. If the planned design did not include covariate adjustment, but a friendly critic suggests adjusting for the pre-treatment measure of the outcome, the “standard error” diagnosand might drop nicely. The point here is that design changes during the implementation process, whether necessitated by unforeseen logistical constraints or required by the review process, can be understood in terms of M, I, D, and A by reconciling the planned design with the design as implemented.\nA happy realization phase concludes with the publication of results. But the research design lifecycle is not finished: the study and its results should be integrated into the broader community of scientists, decision-makers, and the public. Studies should be archived, along with design information, to prepare for reanalysis. Future scholars may well want to reanalyze your data in order to learn more than is represented in the published article or book. Good reanalysis of study data requires a full understanding of the design as implemented, so archiving design information along with code and data is critical. Not only may your design be reanalyzed, it may also be replicated with fresh data. Ensuring that replication studies answer the same theoretical questions as original studies requires explicit design information without which replicators and original study authors may simply talk past one another. Indeed, as our studies are integrated into the scientific literature and beyond, we should anticipate disagreement over our claims. Resolving disputes is very difficult if parties do not share a common understanding of the research design. We might also anticipate that our results will be formally synthesized with others’ work via meta-analysis. Meta-analysts need design information in order to be sure they aren’t inappropriately mixing together studies that ask different questions or answer them too poorly to be of use. Finally with look your designs will be a model for others. Having an analytically complete representation of your design at hand will make it that much easier to use redesign to build on what you have done.\n\n2.4.2 Three more principles\nThis discussion motivates three more principles: Principle 3.4: Design early to reap the benefits of clarity; Principle 3.5: Design often so that you can correct course; and Principle 3.6: Design to share so that you maximize transparency and contribute maximally to knowledge creation.\n\n\n\n\nChopra, Felix, Ingar Haaland, Christopher Roth, and Andreas Stegmann. 2022. “The Null Result Penalty.” Unpublished Manuscript.\n\n\nPearl, Judea, and Dana Mackenzie. 2018. The Book of Why: The New Science of Cause and Effect. New York: Basic Books."
  },
  {
    "objectID": "introduction/research-design-principles.html",
    "href": "introduction/research-design-principles.html",
    "title": "\n3  Research design principles\n",
    "section": "",
    "text": "This section offers succinct discussions of each principle. We will expand on the implications of these principles for specific design choices throughout the book.\n\n\nDesign principles\n\n\nDesign holistically\nDesign agnostically\nDesign for purpose\nDesign early\nDesign often\nDesign to share\n\n\n\nPrinciple 3.1 Design holistically\nThis is perhaps the most important of our principles. Designs are good not because they have good components but because the components work together to get a good result. Too often, researchers develop and evaluate parts of their designs in isolation: is this a good question? Is this a good estimator? What’s the best way to sample? But if you design with a view to diagnosis you are forced to focus on how how each part of the design fits together. An estimator might be appropriate if you used one assignment scheme but not another. The evaluation of data and answer strategies depends on whether your model and inquiry call for descriptive, causal, or generalization (or perhaps, all three at once).If we ask, “What’s your research design?” and you respond “It’s a regression discontinuity design,” we’ve learned something about your answer strategy might fall into, but we don’t have enough information to decide whether it’s a strong design until we learn about the model, inquiry, data strategy, and other parts of the answer strategy. Ultimately design evaluation comes not from assessment of the parts but from diagnosis of the full design.\nWhen we consider whole designs rather than just thinking about one aspect at a time, we notice that how designs that have “parallel” theoretical and empirical sides tent to be strong. We develop this idea in Section 9.3. If you want your estimate \\(a_{d^*} = A(D)\\) to be close to the estimand \\(a_{m^*} = I(M)\\), it’s often best to choose data strategies that parallel models and answer strategies that parallel inquiries, i.e., when this rough analogy holds: M:I::D:A.\n\n\nPrinciple 3.2 Design agnostically\nWhen we design a research study, we have in mind a model of how the world works. But a good design should work, and work well, even when the world is different than what we expect. One implication is that we should entertain many models, seeking not just to ensure the design produces good results for models that we think likely but trying to expand the set of possible models for which the design delivers good results. A second implication is that inquiries and answer strategies should still work when the world looks different to what we expect. Inquiries should have answers even when event generating processes are different to how you imagine them. In the same way, the ability to apply an answer strategy should depend as little as possible on strong expectations of how the data you will get will look.\nA corollary to “Design agnostically” is that we should know for which models our design performs well and for which models it performs poorly. We want to diagnose over many models to find where designs break. All designs break under some models, so the fact that a design ever breaks is no criticism. As research designers, we just want to know which models pose problems and which do not.\n\n\nPrinciple 3.3 Design for purpose\nWhen we say a design is good we mean it is good for some specific purpose. That purpose should be captured by the diagnosands used to assess design quality and design decisions should then be taken with respect to the specified purpose. Too often, researchers focus on a narrow set of diagnosands, and often consider them in isolation. Is the estimator unbiased? Do I have statistical power? The evaluation of a design nearly always requires balancing multiple criteria: scientific precision, logistical constraints, policy goals, as well ethical considerations. And oftentimes these might come into conflict with each other. Thus one design might be best if the goal is to assessing whether a treatment has any effect, another if the goal is to assess the size of an effect. One design might be optimal if the goal is to contribute to general knowledge about how processes work, but another if the goal is to make a decision about whether to move forward with a policy in a given context.\nIn the MIDA framework, the goals of a design are not formally a part of a design. They enter at the diagnosis stage, and, of course, a single design might be assessed for performance for different purposes.\n\n\nPrinciple 3.4 Design early\nDesigning an empirical project entails declaring, diagnosing, and redesigning the components of a research design: its model, inquiry, data strategy, and answer strategy. The design phase yields the biggest gains when we design early. By frontloading design decisions, we can learn about the properties of a design while there is still time to improve them. Once data strategies are implemented — units sampled, treatments assigned, and outcomes measured — there’s no going back. While applying the answer strategy to the revealed dataset, you might well wish you’d gathered data differently, or asked different questions. Post-hoc, we always wish our previous selves had planned ahead.\nThe deeper reason than regret for designing early is that the declaration, diagnosis, and redesign process inevitably changes designs, almost always for the better. Revealing how each of the four design elements are interconnected yields improvements to each. These choices are almost always better made before any data are collected or analyzed.\n\n\nPrinciple 3.5 Design often\nDesigning early does not mean being inflexible. In practice, unforeseen circumstances may change the set of feasible data and answer strategies. Implementation failures due to nonresponse, noncompliance, spillovers, inability to link datasets, funding contractions, or logistical errors are common ways the set of feasible designs might contract. The set of feasible designs might expand if new data sources are discovered, additional funding is secured, or if you learn about a new piece of software. Whether the set expands or contracts, we benefit from declaring, diagnosing, and redesigning given the new realities.\nIn Part IV on the research design lifecycle, we push this principle to the limit, encouraging you to keep on designing even after research is completed, arguing that ex post design can help you assess the robustness of your claims and help decide how to respond to criticism of your work.\n\n\nPrinciple 3.6 Design to share\nThe MIDA framework and the declaration, diagnosis, and redesign algorithm can improve the quality of your research designs. They can also help you communicate your work, justify your decisions, and contribute to the scientific enterprise. Formalizing design declaration makes this sharing easier. By coding up a design as an object that can be run, diagnosed, and redesigned, you help other researchers see, understand, and question the logic of your research.\nWe urge you to keep this sharing function in mind as you write code, explore alternatives, and optimize over designs. An answer strategy that is hard-coded to capture your final decisions might break when researchers try to modify parts. Alternatively, designs can be created specifically to make it easier to explore neighboring designs, let others see why you chose the design you chose, and give them a leg up in their own work. In our ideal world, when you create a design, you contribute it to a design library so others can check it out and build on your good work."
  },
  {
    "objectID": "introduction/getting-started.html",
    "href": "introduction/getting-started.html",
    "title": "4  Getting started",
    "section": "",
    "text": "This chapter serves as quick start guide for the code used throughout this book, and in particular the DeclareDesign package for the R programming language. DeclareDesign is a software implementation of every step of the declare-diagnose-redesign process. While you can declare, diagnose, and redesign using nearly any programming language, DeclareDesign is structured to make it easy to mix-and-match design elements while handling the tedious simulation bookkeeping behind the scenes.\nFirst, we provide instructions for getting started in R and RStudio. We then introduce the code structure for the three steps of the research planning process: declaration, diagnose, and redesign. After this introduction, readers should be able to use the code, but perhaps not write it themselves yet. We devote a longer section to getting started writing the code (Chapter Chapter 13)."
  },
  {
    "objectID": "introduction/getting-started.html#installing-r",
    "href": "introduction/getting-started.html#installing-r",
    "title": "4  Getting started",
    "section": "\n4.1 Installing R",
    "text": "4.1 Installing R\nYou can download R for free from CRAN. We also recommend the free program RStudio, which provides a friendly interface to R. Both R and RStudio are available on Windows, Mac, and Linux.\nOnce you have R and RStudio installed, open up RStudio and install DeclareDesign and its related packages. These include three packages that enable specific steps in the research process: fabricatr for simulating social science data, randomizr for random sampling and random assignment, and estimatr for design-based estimators. You can also install rdddr, which includes datasets and helper functions used in the book. To install them all, copy the following code into your R console:\n\ninstall.packages(c(\"DeclareDesign\", \"rdddr\"))\n\nWe also recommend that you install and get to know the tidyverse set of packages for data analysis, which we will use throughout:\n\ninstall.packages(\"tidyverse\")\n\nFor introductions to R and the tidyverse we especially recommend the free resource R for Data Science.\nAll of the code in this book assumes that the DeclareDesign family of packages, the tidyverse suite, and the companion package to the book rdddr have been loaded. Once they have been installed on your computer, you can load them with the following code:\n\nlibrary(DeclareDesign)\nlibrary(tidyverse)\nlibrary(rdddr)"
  },
  {
    "objectID": "introduction/getting-started.html#declaration",
    "href": "introduction/getting-started.html#declaration",
    "title": "4  Getting started",
    "section": "\n4.2 Declaration",
    "text": "4.2 Declaration\nDesigns are constructed from design elements: models, inquiries, data strategies, and answer strategies.\nIn DeclareDesign, each design element is made with a function that starts with the word declare. For example, we can declare an assignment procedure using declare_assignment as follows:\n\nsimple_random_assignment <- \n  declare_assignment(Z = simple_ra(N = N, prob = 0.6))\n\nEach element created by a declare_* function, perhaps surprisingly, is itself a function. The object simple_random_assignment is not a particular assignment — instead, it is a function that conducts assignment when called. Each time we call simple_random_assignment we get a different random assignment:\n\nparticipants <- data.frame(ID = 1:100)\n\nassignment_1 <- simple_random_assignment(participants)\nassignment_2 <- simple_random_assignment(participants)\nassignment_3 <- simple_random_assignment(participants)\n\nbind_cols(assignment_1, assignment_2, assignment_3)\n\n\n#> New names:\n#> • `ID` -> `ID...1`\n#> • `Z` -> `Z...2`\n#> • `ID` -> `ID...3`\n#> • `Z` -> `Z...4`\n#> • `ID` -> `ID...5`\n#> • `Z` -> `Z...6`\n\n\n\nThree random assignments from the same random assignment step.\n \n ID 1 \n    Z 1 \n    ID 2 \n    Z 2 \n    ID 3 \n    Z 3 \n  \n\n\n 1 \n    0 \n    1 \n    1 \n    1 \n    0 \n  \n\n 2 \n    0 \n    2 \n    0 \n    2 \n    0 \n  \n\n 3 \n    0 \n    3 \n    1 \n    3 \n    1 \n  \n\n 4 \n    1 \n    4 \n    0 \n    4 \n    1 \n  \n\n 5 \n    0 \n    5 \n    1 \n    5 \n    0 \n  \n\n\n\n\nEvery step in a research design can be declared using one of the declare_* functions. Table ?tbl-declarationfunctions collects these according to the four elements of a research design. In Chapter 13, we detail how to build each kind of step.\n\n(#tab:declarationfunctions) Declaration functions in DeclareDesign\n\n\n\n\n\n\nDesign component\nFunction\nDescription\n\n\n\nModel\ndeclare_model()\nbackground variables and potential outcomes\n\n\nInquiry\ndeclare_inquiry()\nresearch questions\n\n\nData strategy\ndeclare_sampling()\nsampling procedures\n\n\n\ndeclare_assignment()\nassignment procedures\n\n\n\ndeclare_measurement()\nmeasurement procedures\n\n\nAnswer strategy\ndeclare_estimator()\nestimation procedures\n\n\n\ndeclare_test()\ntesting procedures\n\n\n\nWe use the + operator to build from elements of a design to a design. Declaration 4.1 shows the format of most declarations throughout the book. This declaration represents a two-arm randomized experiment with 100 units from which we aim to estimate the average treatment effect.\n\nDeclaration 4.1 Two-arm randomized experiment\n\n\nTwo-arm randomized experiment declaration\n\n\n\\(~\\)"
  },
  {
    "objectID": "introduction/getting-started.html#diagnose-design-function",
    "href": "introduction/getting-started.html#diagnose-design-function",
    "title": "4  Getting started",
    "section": "\n4.3 Diagnosis",
    "text": "4.3 Diagnosis\nDiagnosis is the process of simulating the design many times and calculating summary statistics about the design that describe its properties, which we call diagnosands. Once a design is declared, diagnosis is as simple as using the diagnose_design function on it.\n\n\nDiagnosis 4.1 (Example design diagnosis) \ndiagnose_design(declaration_4.1, sims = 100)\n\n\n\n\n\nDesign diagnosis.\n \n Bias \n    RMSE \n    Power \n  \n\n\n -0.02 \n    0.31 \n    0.11 \n  \n\n (0.03) \n    (0.02) \n    (0.03) \n  \n\n\n\n\n\nThe output of the diagnosis includes the diagnosand values (top row), such as bias of \\(-0.01\\), and our uncertainty about the diagnosand value (bootstrapped standard error in parentheses in the bottom row). The uncertainty estimates tell us whether we have conducted enough simulations to precisely estimate the diagnosands. The fact that that the estimate of bias is \\(-0.01\\) and the standard error is \\(0.02\\) means that we cannot distinguish the amount of bias from no bias at all."
  },
  {
    "objectID": "introduction/getting-started.html#redesign-function",
    "href": "introduction/getting-started.html#redesign-function",
    "title": "4  Getting started",
    "section": "\n4.4 Redesign",
    "text": "4.4 Redesign\nWe redesign to learn how the diagnosands change as design features change. We can do this using the redesign function over a range of sample sizes, which produces a list of designs.\n\ndesigns <- redesign(declaration_4.1, N = c(100, 200, 300, 400, 500))\n\nOur simulation and diagnosis tools can operate directly on this list of designs:\n\ndiagnose_design(designs)"
  },
  {
    "objectID": "introduction/getting-started.html#library-of-designs",
    "href": "introduction/getting-started.html#library-of-designs",
    "title": "4  Getting started",
    "section": "\n4.5 Library of designs",
    "text": "4.5 Library of designs\nIn our DesignLibrary package, we have created a set of common designs as designers (functions that create designs from just a few parameters), so you can get started quickly.\n\nlibrary(DesignLibrary)\n\nblock_cluster_design <- \n  block_cluster_two_arm_designer(N = 1000, N_blocks = 10)"
  },
  {
    "objectID": "introduction/getting-started.html#long-term-code-usability",
    "href": "introduction/getting-started.html#long-term-code-usability",
    "title": "4  Getting started",
    "section": "\n4.6 Long term code usability",
    "text": "4.6 Long term code usability\nWe have written the code examples with DeclareDesign version 1.0.0, the package version we released along with the book. We are committed to the long-term maintenance of this software, but inevitably, the evolution of the R ecosystem and further package development will mean that some of the printed code will break in the future. When this happens, we ask readers to refer to the book website, which we will maintain with up-to-date versions of the code and software.\nHowever, a virtue of writing out designs in code is that they are explicit: the entire design is encoded in the declarations we provide. Even if the code itself won’t run, you can still use it to understand the design and to draw insights from the diagnosis."
  },
  {
    "objectID": "declaration-diagnosis-redesign/declaring-designs.html",
    "href": "declaration-diagnosis-redesign/declaring-designs.html",
    "title": "\n5  Declaring designs\n",
    "section": "",
    "text": "In Chapter 2, we gave a high-level overview of our framework for describing research designs in terms of their models, inquiries, data strategies, and answer strategies, our process for diagnosing their properties, and a general purpose approach for improving them to better fit research goals. Now in this chapter, we place our approach on a firmer formal footing. We employ elements from Pearl’s (2009) approach to causal modeling, which provides a syntax for mapping design inputs to design outputs. We also use the potential outcomes framework as presented, for example, in Imbens and Rubin (2015), which many social scientists use to clarify their inferential targets.\nDescribing a research design in the MIDA framework allows us to see the fundamental symmetries across the theoretical (M and I) and empirical (D and A) halves of a research design. A recurring theme of our book is that research designs tend to be stronger when the relationship of M to I is mirrored by the relationship of D to A; the aim of this chapter is to make this somewhat abstract claim more concrete.\n\nResearch designs are defined by four elements: a model M, an inquiry I, a data strategy D, and an answer strategy A. Describing a research design entails “declaring” each of these four elements.\nM is a set of possible models of how the world works. Following Pearl’s definition of a probabilistic causal model, a model in M contains three core elements. The first is the “signature” (Halpern 2000), or the specification of the variables \\(X\\) about which research is being conducted, including the endogenous and exogenous variables (\\(V\\) and \\(U\\) respectively) and their ranges. The second element (\\(F\\)) is a specification of how each endogenous variable depends on other variables. These dependencies can be considered functional relations or, as in Imbens and Rubin (2015), potential outcomes because they describe what would happen under different possible conditions. The third and final element is a probability distribution over exogenous variables, written as \\(P(U)\\). Sometimes it is useful to think of the draws from \\(U\\) as implying distinct models of their own, in which case we might think of M as a family of models that fully specifies what would happen under all conditions and a particular model \\(m\\) as an element of M that describes one of those conditions. We eschew the phrase “data generating process” when referring to M (since data are generated by the data strategy) and instead use the phrase “event generating process.”\nThe inquiry I is a summary of the variables \\(X\\). All inquiries are either descriptive or causal. Descriptive inquiries are those that do not involve comparisons across counterfactual worlds, for example, the average value of an outcome Y over all N units in the population: \\(\\frac{\\sum_i^N Y_i}{N}\\). Causal inquiries do involve comparisons across counterfactuals, as in the average treatment effect: \\(\\frac{\\sum_i^N Y_i(Z_i = 1) -Y_i(Z_i = 0)}{N}\\).\nWe let \\(a_m\\) denote the answer to I under the model. Conditional on the model, \\(a_m\\) is the value of the estimand, the quantity that the researcher wants to learn about, or would want to learn about if the world were like the model. The connection of \\(a_m\\) to the model is given by: \\(a_m = I(m)\\).\nAs the saying goes, models are wrong but some may be useful. We denote the true causal process as \\(m^*\\): the process that generates events in the real world. The right answer, then, is \\(a_{m^*} = I(m^*)\\). The answer under a reference model \\(a_m\\) may be close or far from the true value \\(a_{m^*}\\), which is to say it could be wrong. If the model \\(m\\) is far from \\(m^*\\), then of course \\(a_m\\) need not be correct. Moreover \\(a_{m^*}\\) might even be undefined, since inquiries can only be stated in terms of theoretical models. If the theoretical model is wrong enough—for instance it conditions on events that could not arise—then the inquiry might be nonsensical. For example, “what is the ideological slant of a speech that is not given” is an inquiry that is undefined.\nA data strategy D generates data \\(d\\). Data \\(d\\) arises under model M with probability \\(P_M(d|D)\\). The data strategy includes sampling, assignment, and measurement strategies. Nearly all data strategies sample and measure, but not all assign treatments. Whether or not the data strategy includes assignment is the defining distinction between experimental and observational studies. When applied in the real world, the data strategy operates on \\(m^*\\) to produce the realized data: \\(D(m^*) = d^*\\). When we simulate research designs, the data strategy operates on a simulated model draw \\(m\\) to produce fabricated data: \\(D(m) = d\\).\nFinally, the answer strategy A generates answers using data. When applied to realized data, the answer strategy returns the empirical answer: \\(A(d^*) = a_{d^*}\\). When applied to simulated data, it returns a simulated answer: \\(A(d) = a_{d}\\).\nTable ?tbl-elementsofresearchdesign provides a concise description of each element of a research design and relates them to some common terms. We flag here that the term estimand has a slightly different meaning in our framework than elsewhere. We say that an estimand \\(a_m\\) is the value of an inquiry \\(I\\), whereas in some traditions “estimand” can refer to the inquiry \\(I\\) or to an intermediate parameter that happens to be targeted by an estimator.\n\n(#tab:elementsofresearchdesign) Elements of research design.\n\n\n\n\n\n\nNotation\nDescription\nRelated terms\n\n\n\nM\na stipulated collection of causal models\n\n\n\n\\(m\\)\na single model in \\(M\\), represented by events\na hypothetical data generating process\n\n\n\\(m^*\\)\nthe true model\ntrue data generating process\n\n\nI\nthe inquiry\nestimand, quantity of interest\n\n\n\\(a_m = I(m)\\)\nthe answer under the model, an estimand\n\n\n\n\\(a_{m^*} = I(m^*)\\)\nthe true answer, the estimand\nquantity of interest\n\n\nD\nthe data strategy\n\n\n\n\\(d = D(m)\\)\nfabricated data; simulated data\n\n\n\n\\(d^* = D(m^*)\\)\nrealized data\n\n\n\nA\nthe answer strategy\ndata analysis, estimator, method\n\n\n\\(a_{d} = A(d)\\)\na simulated answer, an estimate\na hypothetical estimate\n\n\n\\(a_{d^*} = A(d^*)\\)\nthe empirical answer, the estimate\nthe observed estimate\n\n\n\nThe full set of causal relationships between M, I, D, and A, with respect to \\(m\\) and \\(m^*\\), \\(a_m\\) and \\(a_{m^*}\\), \\(d\\) and \\(d^*\\), and \\(a_d\\) and \\(a_{d^*}\\) can be seen in the schematic representation of a research design given in Figure 6.1 The figure illustrates how a research design involves a correspondence between \\(I(m) = a_m\\) and \\(A(d) = a_d\\). The theoretical half of a research design produces an answer to the inquiry in theory. The empirical half of a research design produces an empirical estimate of the answer to the inquiry. Neither answer is necessarily close to the truth \\(a_{m^*}\\), of course. And, as shown in the figure, the truth is not directly accessible either to us in theory or in empirics. Our gamble in empirical research, however, is that our theoretical models are close enough to the truth; that the truth is like the set of models we imagine. If the models in \\(M\\) do not contain \\(m^*\\) or are too different from it, then even seemingly strong research designs could yield incorrect answers.\n\n\nFigure 6.1: The MIDA framework\n\n\nFigure 6.1 reveals a striking analogy between the M, I relationship and the D, A relationship. The answer we aim for (the estimand) is obtained by applying I to a draw from M. The answer we have access to (the estimate) is obtained by applying A to a draw from D. Our hope, usually, is that these two answers are quite similar. In some cases, this parallelism suggests that the function A should be “like” the function I. For instance, if we are interested in the mean of a population and we have access to a random sample, the data available to us from D is like the ideal data we would have if we could observe the nodes and edges in M directly.\nFinally, in Figure 6.1 no arrows go into M, I, D, or A, since they are not caused by any of the other nodes. We could have included a node for the research designer, who deliberately sets the details of M, I, D, and A, but we omit it for clarity.\n\nTable ?tbl-midarealizations illustrates these different quantities through DeclareDesign. We stipulate a model, \\(M\\), in which \\(Y\\) depends on \\(X\\). We define a inquiry, \\(I\\): what is the average value of \\(Y\\) when \\(X=1\\)? We calculate what the value of our inquiry (the estimand) would be under one of our simulated models, \\(I(m)\\). We also imagine we could describe how the world in fact is, \\(m^*\\), and calculate what the right answer would be in that case, \\(a_{m^*}\\). We then apply the data strategy \\(D\\) to produce realized data \\(d^*\\) and use an answer strategy \\(A\\) and use it to calculate the answer \\(a_d^*\\) we would get given \\(d^*\\).\nFor each of these steps we show DeclareDesign code in the first column. In the second column, we show the simulated \\(m\\), \\(m^*\\), and \\(d^*\\) datasets, along with the values of \\(a_m\\), \\(a_{m^*}\\), and \\(a_{d^*}\\) for one run of the simulation.\n\n(#tab:midarealizations) Elements of research design in code.\n\n\n\n\n\nDescription\nDraw\n\n\n\nM <- declare_model(N = 1000,\n  U = rnorm(N),\n  X = rbinom(N, 1, prob = pnorm(U)),\n  Y = rbinom(N, 1, prob = pnorm(U + X)))\nm <- M()\n\n\n\nI <- declare_inquiry(Ybar = mean(Y[X==1]))\na_m <- I(m)\n\n\n\nmstar <- fabricate(N = 1000,\n  U = rnorm(N),\n  X = rbinom(N, 1, prob = pnorm(U)),\n  Y = rbinom(N, 1, prob = pnorm(U)))\n\n\n\na_mstar <- I(mstar)\n\n\n\nD <- declare_sampling(\n  S = simple_rs(N, prob = 0.1))\ndstar <- D(mstar)\n\n\n\nA <- declare_estimator(\n Y ~ 1, .method = lm_robust,\n subset = X == 1, inquiry = \"Ybar\")\na_dstar <- A(dstar)\n\n\n\n\nAs described in the getting started guide in Chapter 4, we concatenate the design steps into a full design declaration using the + operator:\n\nDeclaration 6.1 Example declaration\n\ndeclaration_5.1 <-\n  declare_model(\n    N = 1000,\n    U = rnorm(N),\n    X = rbinom(N, 1, prob = pnorm(U)),\n    Y = rbinom(N, 1, prob = pnorm(U + X))\n  ) +\n  declare_inquiry(Ybar = mean(Y[X == 1])) +\n  declare_sampling(S = simple_rs(N, prob = 0.1)) +\n  declare_estimator(Y ~ 1,\n                    .method = lm_robust,\n                    subset = X == 1,\n                    inquiry = \"Ybar\")\n\n\nThis design declaration includes a specification of all four design elements: model, inquiry, data strategy, and answer strategy. The next four chapters will describe each of these four design elements in great detail. For now, notice that the declaration does not include a specification of \\(m^*\\) (the true causal model), only \\(M\\), a model we entertain for research planning purposes.\n\n\n\n\nHalpern, Joseph Y. 2000. “Axiomatizing Causal Reasoning.” Journal of Artificial Intelligence Research 12: 317–37.\n\n\nImbens, Guido W., and Donald B. Rubin. 2015. Causal Inference in Statistics, Social, and Biomedical Sciences. Cambridge: Cambridge University Press.\n\n\nPearl, Judea. 2009. Causality: Models, Reasoning and Inference. Second Edition. Cambridge: Cambridge University Press."
  },
  {
    "objectID": "declaration-diagnosis-redesign/specifying-model.html",
    "href": "declaration-diagnosis-redesign/specifying-model.html",
    "title": "\n6  Specifying the model\n",
    "section": "",
    "text": "Models are theoretical abstractions we use to make sense of the world and organize our understanding of it. They play many critical roles in research design. First and foremost, models describe the units, conditions, and outcomes that define inquiries. Without well-specified models, we cannot pose well-specified inquiries. Second, models provide a framework to evaluate the sampling, assignment, and measurement procedures that form the data strategy. Models encode our beliefs about the kinds of information that might result when we conduct empirical observations. Third, they guide the selection of answer strategies: what variables should we condition on, what variables should we not condition on, how flexible or rigid should our estimation procedure be? Whenever we rely on assumptions in the model—for example, normality of errors, conditional independencies, or latent scores—we are betting that the real causal model \\(m^*\\) has these properties.\nWe need to imagine models in order to declare and diagnose research designs. This need often generates discomfort among students and researchers who are new to thinking about research design this way. In order to compute the root mean squared error, bias, or statistical power of a design, we need to write down more than we know for sure in the model. We have to describe joint distributions of covariates, treatments, and outcomes, which entails making guesses about the very means, covariances, and effect sizes (among many other things) that the empirical research design is supposed to measure. “What do you mean, write down the potential outcomes—that’s what I’m trying to learn about!”\nThe discomfort arises because we do not know the true causal model of the world—what we referred to as \\(m^*\\) in Figure 6.1 We are uncertain about which of the many plausible models of the world we entertain is the correct one. In fact we can be fairly certain that none of them is really correct.\nThe good news is that they do not have to be correct. The M in MIDA refers to these possible models, which we call “reference models.” M is a set of reference models. Their role is to provide a stipulation of how the world works, which allows us to answer some questions about our research design. If the reference model were true, what then would the value of the inquiry be? Would the estimator generate unbiased estimates? How many units would we need to achieve an RMSE of 0.5? Critically, whether a design is good or bad depends on the reference model. A data and analysis strategy might fare very well under one model of the world but poorly under another. Thus to get to the point where we can assess a design we need to make the family of reference models explicit. Our hope is that when we apply A and I to the real event generating processes we will have a similar relation between the answer we seek and the answers we get as we have between conjectured estimands and simulated estimates. Beyond that we don’t have to actually believe any of them models in M."
  },
  {
    "objectID": "declaration-diagnosis-redesign/specifying-model.html#elements-of-models",
    "href": "declaration-diagnosis-redesign/specifying-model.html#elements-of-models",
    "title": "\n6  Specifying the model\n",
    "section": "\n6.1 Elements of models",
    "text": "6.1 Elements of models\nModels are characterized by three elements: the signature, the functional relationships, and a probability distribution over exogenous variables. We’ll describe each in turn.\n\n6.1.1 Signature\nThe signature of the model describes the variables in the model and their ranges. The signature comprises two basic kinds of variables: exogenous variables and endogenous variables. Exogenous means “generated from without” and endogenous means “generated from within.” Stated more plainly, exogenous variables are not caused by other variables in the model because they are randomly assigned by nature or by human intervention. Endogenous variables result as a consequence of exogenous variables; they are causally downstream from exogenous variables.\nWhat kinds of variables are exogenous? Typically, we think of explicitly randomly assigned variables as exogenous: the treatment assignment variable in a randomized experiment is exogenous. We’ll often use the variable letter \\(Z\\) to refer to assignments that were explicitly randomized. We also often characterize the set of unobserved causes of observed variables as exogenous. We summarize the set of unobserved causes of an observed variable with the letter \\(U\\). These unobserved causes are exogenous in the sense that, whatever the causes of \\(U\\) may be, they do not cause other endogenous variables in a model.\nWhat kinds of variables are endogenous? Everything else: covariates, mediators, moderators, and outcome variables. We’ll often use the letter \\(X\\) when describing covariates or moderators, the letter \\(M\\) when describing mediators, and the letter \\(Y\\) when describing outcome variables. Each of these kinds of variables is the downstream consequence of exogenous variables, whether those exogenous variables are observed or not.\nCritically the signature of a model is itself a part of the design: we as designers must choose the variables of interest. We do not, however, get to decide the functional relations between variables—those are set according to \\(m^*\\).\n\n6.1.2 Functional relations\nThe second element of the model is the set of functions that produce endogenous variables. The output of these functions are always endogenous variables and the inputs can be either exogenous variables or other endogenous variables. We embrace two different, but ultimately compatible, ways of thinking about these functional relationships: structural causal models and the potential outcomes model.\nThe structural casual model account of causality is often associated with directed acyclic graphs (DAGs). Each node on a graph is a variable and the edges that connect them represent possible causal effects. An arrow from a “parent” node to a “child” node indicates that the value of the parent sometimes influences the outcome of the child. More formally: the parent’s value is an argument in a functional equation determining the child’s outcome. DAGs emphasize a mechanistic notion of causality. When the exposure variable changes, the outcome variable changes as a result, possibly in different ways for different units.\nDAGs represent nonparametric structural causal models. The qualifier “nonparametric” means that DAGs don’t show how variables are related, just that they are related. This is no criticism of DAGs — they just don’t encode all of our causal beliefs about a system. We illustrate these ideas using a DAG to describe a model for an abstract research design in which we will collect information about \\(N\\) units. We will assign a treatment \\(Z\\) at random, and collect an outcome \\(Y\\). We know there are other determinants of the outcome beyond \\(Z\\), but we don’t know much about them. All we’ll say about those other determinants \\(U\\) is that they are causally related to \\(Y\\), but not to \\(Z\\), since \\(Z\\) will be randomly assigned by us.\nThis nonparametric structural causal model can be written like this:\n\\[\\begin{align*}\nY &= f_Y(Z, U)\n\\end{align*}\\]\nHere, the outcome \\(Y\\) is related to \\(Z\\) and \\(U\\) by some function \\(f_Y\\), but the details of what the function \\(f_Y\\) is—whether \\(Z\\) has a positive or negative effect on \\(Y\\), for example—are left unstated in this nonparametric model. The DAG in Figure 6.1 encodes this model in graphical form. We use a blue circle around the treatment assignment to indicate that \\(Z\\) is randomly assigned as part of the data strategy.\n\n\nFigure 6.1: Directed acyclic graph of a randomized experiment\n\n\nTo assess many properties of a research design we often need to make the leap from nonparametric models to parametric structural causal models. We need to enumerate beliefs about effect sizes, correlations between variables, intra-class correlations (ICCs), specific functional forms, and so forth. Since any particular choice for these parameters could be close or far from the truth, we will typically consider a range of plausible values for each model parameter.\nOne possible parametric model is given by the following:\n\\[\\begin{align*}\nY &= 0.5 \\times Z + U\n\\end{align*}\\]\nHere, we have specified the details of the function that relates \\(Z\\) and \\(U\\) to \\(Y\\). In particular, it is a linear function in which \\(Y\\) is equal to the unobserved characteristic \\(U\\) in the control condition, but is 0.5 higher in the treatment condition. We could also consider a more complicated parametric model in which the relationship between \\(Z\\) and \\(Y\\) depends on an interaction with the unobservables in \\(U\\):\n\\[\\begin{align*}\nY &= -0.25 \\times Z - 0.05\\times Z \\times U + U\n\\end{align*}\\]\nBoth of these parameterizations are equally consistent with the DAG in Figure 6.1, which underlines the powerful simplicity of DAGs but also their theoretical sparsity. The two parametric models are theoretically quite different from one another. In the first, the effects of the treatment \\(Z\\) are positive and the same for all units; in the second, the effects are negative and quite different from unit to unit, depending on the value of \\(U\\). If both of these reference models are plausible, we’ll want to include them both in M, to ensure that our design is robust to both possibilities. Here we have a small instance of Principle 3.2: Design agnostically. We want to consider a wide range of plausible parameterizations since we are ignorant of the true causal model (\\(m^*\\)).\nBy contrast with structural causal models, the potential outcomes formalization emphasizes a counterfactual notion of causality. \\(Y_i(Z_i = 0)\\) is the outcome for unit \\(i\\) that would occur were the causal variable \\(Z_i\\) were set to zero and \\(Y_i(Z_i = 1)\\) is the outcome that would occur if \\(Z_i\\) were set to one. The difference between them defines the effect of the treatment on the outcome for unit \\(i\\). Since at most only one potential outcome can ever be revealed, at least one of the two potential outcomes is necessarily counterfactual. Usually, the potential outcomes notation \\(Y_i(Z_i)\\) reports how outcomes depend on one feature, \\(Z_i\\), ignoring all other determinants of outcomes. It’s not to say those other causes don’t matter—they do—they are just not the focus. In a sense, they are contained in the subscript \\(i\\) since the units carry with them all relevant features other than \\(Z_i\\). We can generalize to settings where we want to consider more than one cause, in which case we use expressions of the form \\(Y_i(Z_i = 0, X_i = 0)\\) or \\(Y_i(Z_i = 0, X_i = 1)\\).\nThe potential outcomes version of the first structural causal model might be written, for \\(i \\in \\{1,2,\\dots, n\\}\\) as:\n\\[\\begin{align*}\nY_i(0) &= U_i  \\\\\nY_i(1) &= 0.5 + U_i\n\\end{align*}\\]\nThe potential outcomes under the second model would be written:\n\\[\\begin{align*}\nY_i(0) &= U_i  \\\\\nY_i(1) &= -0.25 - 0.05 \\times U_i\n\\end{align*}\\]\nDespite what might be inferred from the sometimes heated disagreements between scholars who prefer one formalization to the other, structural causal models and potential outcomes are compatible systems for thinking about causality. Potential outcome distributions can also be described using Pearl’s \\(\\pearldo()\\) operator: \\(\\Pr(Y|\\pearldo(Z = 1))\\) is the probability distribution of the treated potential outcome. We could use only the language of structural causal models or we could use only the language of potential outcomes, since a theorem in one is theorem in the other (Pearl 2009, 243). We choose to use both languages because they are useful for expressing different facets of research design. We use structural causal models to describe the web of causal interrelations in a concise way (writing out the potential outcomes for every relationship in the model is tedious). We use potential outcomes when the inquiry involves comparisons across conditions and to make fine distinctions between inquiries that apply to different sets of units.\n\n6.1.3 Probability distribution over exogenous variables\nThe final element of a model is a description of the probability distribution of exogenous variables. For example, we might describe the distribution of the treatment assignment as a Bernoulli distribution with \\(p\\) = 0.1 to describe “coin flip” random assignment with a 10% chance of a unit being assigned to treatment. We might stipulate that the unobserved characteristics \\(U\\) are normally distributed with a mean of 1 and a standard deviation of 2. The distributions of the exogenous variables then ramify through to the distributions of the endogenous variables through the functional relations.\nIn general multiple distributions can behave equivalently in a model. For instance if we specify that \\(u\\) is distributed normally with mean 0 and standard deviation \\(\\sigma\\) and that \\(Y = 1\\) if and only if \\(u \\geq 0\\), then the distribution induced on \\(Y\\) will be the same regardless of the choice of \\(\\sigma\\). Indeed the same distribution on \\(Y\\) could be generated by countless other distributions on \\(u\\). The focus then is not on getting these distributions “right,” but in selecting distributions in light of their implications for the distributions of endogeneous nodes."
  },
  {
    "objectID": "declaration-diagnosis-redesign/specifying-model.html#types-of-variables-in-models",
    "href": "declaration-diagnosis-redesign/specifying-model.html#types-of-variables-in-models",
    "title": "\n6  Specifying the model\n",
    "section": "\n6.2 Types of variables in models",
    "text": "6.2 Types of variables in models\nAny particular causal model will be a complex web of exogenous and endogenous variables woven together via a set of functional relationships. Despite the heterogeneity across models, we can describe the roles variables play in a research design with reference to the roles they play in structural causal models. There are seven roles:\n\n\nOutcomes: Variables whose level or responses we want to understand, generally referred to as \\(Y\\), as in Figure 6.2. Variously described as “dependent variables,” “endogenous variables,” “left-hand side variables,” or “response variables.”\n\nTreatments: Variables that affect outcomes. We will most often use \\(D\\) to refer to the main causal variable of interest in a particular study. Sometimes labeled as “independent variables,” or “right-hand side variables.”\n\nModerators: Variables that condition the effects of treatment variables on outcomes: depending on the level of a moderator, treatments might have higher or lower effects on outcomes. Nonparametric structural causal models (like those represented in DAGs) do not encode whether a variable is a moderator or not. See for example \\(X2\\) in Figure 6.2. The figure indicates that \\(X2\\) is a cause of \\(Y\\) but does not explicitly indicate whether it moderates the effect of \\(D\\) on \\(Y\\).\n\nMediators: Variables “along the path” from treatment variables to outcomes. \\(M\\) is an example of a mediator in this figure. Mediators are often studied to assess “how” or “why” \\(D\\) causes \\(Y\\).\n\nConfounders: Variables that introduce a non-causal dependence between \\(D\\) and \\(Y\\). In the figure, \\(X1\\) is a confounder because it causes both \\(D\\) and \\(Y\\) and could introduce a dependence between them even if \\(D\\) did not cause \\(Y\\).\n\nInstruments: An instrumental variable is an exogenous variable that affects a treatment variable which itself causes an an outcome variable. We give a much more detailed treatment of these variables in ?sec-ch13s4. We reserve the letter \\(Z\\) for instruments. Random assignments are instruments in the sense that the assignment is the instrument and the actual treatment received is the treatment variable.\n\nColliders: Colliders are variables that are caused by two other variables. Colliders can be important because conditioning on a collider introduces a non-causal relationship between all parents of the collider. The intuition is that if learn that a child is tall then learn that one parent is small makes you more likely to believe that the other parent is tall. In ?fig-vartypes, \\(K\\) is a collider that can create a non-causal dependence between \\(D\\) and \\(Y\\) (via \\(U\\)) if conditioned upon.\n\nThese labels reflect the researcher’s interest as much as their position in a model. Another researcher examining the same graph might, for instance, label \\(M\\) as their treatment variable or \\(K\\) as their outcome of interest.\n\n\nFigure 6.2: A directed acyclic graph with an explanatory variable of interest (D), an outcome of interest (Y), a mediator (M), a confounder (X1), a moderator (X2), an instrument (Z), and a collider (K).\n\n\n\n6.2.1 What variables are needed?\nOur models of the world can be more or less complex, or at least articulated at higher or lower levels of generality. How specific and detailed we need to be in our specification of possible models depends on the other features of the research design: the inquiry, the data strategy, and the answer strategy. At a minimum, we need to describe the variables required for each of these research design elements.\nInquiry: In order to reason about whether the model is sufficient to define the inquiry, we need to define the units, conditions, and outcomes used to construct our inquiry. If the inquiry is an average causal effect among a subgroup, we need to specify the relevant potential outcomes and the covariate that describes the subgroup.\nData strategy: Sampling procedures often involve stratification or clustering, so in the model, we need to define the variables that will be used to stratify and cluster. Similarly, treatment assignment might be blocked or clustered; correspondingly, the variables that are used to construct blocks or clusters must be defined in the model. Finally, all of the variables that will be measured should also be defined in the model. When we measure latent variables imperfectly, the model describes the latent trait and how measured responses may deviate from it. If you expect to encounter complexities in D that you need to take account of in A, such as missing data, non compliance or attritition, then possible drivers of these should also be included in M.\nAnswer strategy: Any measured variable that will be used in the answer strategy should be included in the model. This requirement clearly includes the observed outcomes and treatments, but also the covariates that are used to address confounding or to increase precision.\nThe variables required by the inquiry, data strategy, and answer strategy are necessary components of the model, but they are not always sufficient. For example, we might be worried about an unobserved confounder. Such a confounder would not be obviously included in any of the other research design elements but is clearly important to include in the model. Ultimately, we need to specify all variables that are required for “diagnosand completeness” (see Chapter (diagnosis?)), which is achieved when research designs are described in sufficient detail that diagnosands can be calculated."
  },
  {
    "objectID": "declaration-diagnosis-redesign/specifying-model.html#how-to-specify-models",
    "href": "declaration-diagnosis-redesign/specifying-model.html#how-to-specify-models",
    "title": "\n6  Specifying the model\n",
    "section": "\n6.3 How to specify models",
    "text": "6.3 How to specify models\nTo this point, we have described formal considerations but we have not described substantive considerations for including particular variables or stipulating particular relations between them. The justification for the choice of reference models will depend on the purpose of the design. Broadly we distinguish two desiderata for selecting reference models: reality tracking and agnosticism.\n\n6.3.1 Reality tracking\nIn stipulating reference models we have incentives to focus on models that we think reasonably track reality (\\(m^*\\)) as well as possible. Why waste time and effort stipulating processes we don’t think will happen?\nThe justification for reality tracking typically comes from two places: reading the past literature and qualitative research. Past theoretical work can guide the set of variables that are relevant and how they relate to one another. Past empirical work can provide further insight into the distributions and dependencies across variables. However, when past research is thin, there is no substitute for insights gained through qualitative data collection: focus groups and interviews with key informants who know aspects of the model that are hidden from the researcher, archival investigations to understand a causal process, or immersive participant observation to see with your own eyes how social actors behave. Fenno (1978) memorably describes this process as “soaking and poking.” This mode of model development is separate from the qualitative research designs that provide answers to well-specified inquiries (see the process tracing entry in the design library for an example of such a design). Instead, qualitative insights such as this, which Lieberman (2005) labels “model-building” case studies, do not aim to answer a question but rather yield a new theoretical model. Quantitative research is often seen as distinct from qualitative research, but the model building phase in both is itself qualitative.\nThe next step — selecting statistical distributions and their parameters to describe exogenous variables and the functional forms of endogenous variables — is often more uncomfortable. We do not know the magnitude of the effect of an intervention or the correlation between two outcomes before we do the research, that’s why we are conducting the study. However, we are not fully in the dark in most cases and can make educated guesses about most parameters.\nWe can conduct meta-analyses of past relevant studies on the same topic to identify the range of plausible effect sizes, intra-cluster correlations, correlations between variables, and other model parameters. Conducting such a meta-analysis might be as simple as collecting the three papers that measured similar outcomes in the past and calculating the intra-cluster correlations across the three. How informative past studies are for your research setting depends on the similarity of units, treatments, and outcomes. Except in the case of pure replication studies, we are typically studying a (possibly new) treatment in a new setting, with new participants, or with new outcomes, so there will not be perfect overlap. However, the variation in effects across contexts and these other dimensions will help structure the range of our guesses specified in the model.\nWhen there are past studies that are especially close to our own, we may want our model to match the observed empirical distribution from that past study as closely as possible. To do so, we can resample or bootstrap from the past data in order to simulate realistic data. Where there are no past studies that are sufficiently similar in some dimensions, we can collect new data through pilot studies (see Section (piloting?)) or baseline surveys to serve a similar purpose.\nIt is also possible to use the distribution over quantities in a model to represent our uncertainty over some quantities and in this way let our diagnostics integrate over our uncertainty. We did this already in Declaration 2.1 where we let the treatment effect be a random draw from a stipulated distribution. In doing this the success function represents expected success with respect to this distribution over treatment effects.\nSince it excludes cases we deem improbable, a focus on reality tracking models seems to contradict Principle 3.2: Design agnostically. However, by focusing on reality-tracking models, we aim to contain the smallest set of plausible models that are needed to capture the essentials of the true process. In practice of course we might never include \\(m^*\\). For instance we might contemplate a set of worlds in which an effect lies between 0 and 1 yet not include the true value of 2. This is not necessarily a cause for concern. The lessons learned from a diagnosis do not depend on the realized world \\(m^*\\) being among the set of possible draws of M, the relevant question is only whether the kinds of inferences one might draw given stipulated reference models would also hold reasonably well for the true event generating process. For instance, if our aim is to assess whether an analysis strategy generates an unbiased estimate of a treatment effect we may go to pains to make sure to model treatment assignment carefully but modeling the size of a treatment effect correctly may not be important. The idea is that what you learn from the models that you do study is sufficient for inferences about a broader class of models within which the true event generating process might lie.\n\n6.3.2 Agnosticism\nFor some purposes, the reference model might be developed not to track reality, as you see it, but rather to reflect assumptions in a scholarly debate. For instance, the purpose might be to question whether a given conclusion is valid under the assumptions maintained by some scholarly community. Indeed it is possible that a reference model is used specifically because the researcher thinks it is inaccurate, allowing them to show that even if they are wrong about some assumptions about the world in M, their analysis will produce useful answers.\nIn a directed acyclic graph, every arrow indicates a possible relation between a cause and an outcome. The big assumptions in these models, however, are not seen in the arrows but the absence of arrows: every missing arrow represents a claim that an outcome is not affected by a possible cause. Answer strategies often depend upon such assumptions. Even when arrows are included, functional relations might presuppose particular features that are important for inference. For instance, a researcher using instrumental variables analysis (see Section (instrumental-variables?)) will generally assume that \\(Z\\) causes \\(Y\\) through \\(D\\) but not through other paths. This “excludability” assumption is about absent arrows. The same analysis might also assume that \\(Z\\) never affects \\(D\\) negatively. That “monotonicity” assumption is about functional forms. An agnostic reference model might loosen these assumptions to allow for possible violations of the excludability or monotonicity assumptions.\nWhen we are agnostic, we admit we don’t know whether the truth is in the set of models we consider reasonable—so we entertain a wider set than we might think plausible. We suggest three guides for choosing these ranges: the logical minimum and maximum bounds of a parameter, a meta-analytic summary of past studies, or best- and worst-case bounds, based on the substantive interpretations of previous work. A design that performs well in terms of power and bias under many such ranges might be labeled “robust to multiple models.”\nA separate goal is assessing the performance of a research design under different models implied by alternative theories. A good design will provide probative evidence about which model is correct no matter the underlying state. A poor design might only affirm one model when it is true but fail to provide support for an alternative when it is true.\nAn important example is assessing the performance of a research design under a “null model” where the true effect size is zero. A good research design should report with a high probability that there is insufficient evidence to reject a null effect. That same research design, under an alternative model with a large effect size, should with a high probability return evidence rejecting the null hypothesis of zero effect. The example makes clear that in order to understand whether the research design is strong, we need to understand how it performs under the models implied by alternative theoretical understandings of the world."
  },
  {
    "objectID": "declaration-diagnosis-redesign/specifying-model.html#summary",
    "href": "declaration-diagnosis-redesign/specifying-model.html#summary",
    "title": "\n6  Specifying the model\n",
    "section": "\n6.4 Summary",
    "text": "6.4 Summary\nIf this section left you spinning from the array of choices we have to make in declaring a model, in some ways that was our goal. Inside every power calculator and bespoke design simulation code is an array of assumptions. Some crucially determine design quality and others are unimportant. The salve to the dizziness is in Principle 3.2: Design agnostically. Where you are uncertain, explore whether both options produce the same diagnosands. The goal is for our data and answer strategies to hold up under many models.\n\n\n\n\nFenno, Richard F. 1978. Home Style: House Members in Their Districts. New York: Longman.\n\n\nLieberman, Evan S. 2005. “Nested Analysis as a Mixed-Method Strategy for Comparative Research.” American Political Science Review 99 (3): 435–52.\n\n\nPearl, Judea. 2009. Causality: Models, Reasoning and Inference. Second Edition. Cambridge: Cambridge University Press."
  },
  {
    "objectID": "declaration-diagnosis-redesign/defining-inquiry.html",
    "href": "declaration-diagnosis-redesign/defining-inquiry.html",
    "title": "\n7  Defining the inquiry\n",
    "section": "",
    "text": "An inquiry is a question we ask of a model. If we stipulate a reference model, \\(m\\), then our inquiry is a summary of \\(m\\). Suppose in some reference model that \\(X\\) affects \\(Y\\). One inquiry might be descriptive: what is the average level of \\(Y\\) when \\(X=1\\)? A second might be causal: what is the average treatment effect of \\(X\\) on \\(Y\\)? A third is about counterfactuals: for what share of units would \\(Y\\) have been different if \\(X\\) were different? If a theory involves more variables, many more questions open up, for instance regarding how the effect of one variable passes through, or is modified by, another.\nWhen designing research, we should have our inquiries front of mind. Amazingly, very many research projects do not specify the target of inference, focusing instead of the specification of estimation procedure. At some stages of research it is not possible to specify the inquiry with great precision. In early stages you may need to do research in order to find out what the right question is. But once you are at the stage of thinking through inferential strategies you need an inquiry in mind in order to select among options. For the same reason readers need to know your inquiry in order to evaluate your choices.\nFormally, an inquiry is a summary function I that operates on an instance of a model \\(m \\in M\\). When we summarize the model with the inquiry, we obtain an “answer under the model.” We formalize this idea as \\(I(m) = a_m\\). The difference between I and \\(a_m\\) is the difference between a question and its answer: I is the question we ask about the model and \\(a_m\\) is the answer.\nIn this book when we talk about inquiries, we will usually be referring to single-number summaries of models. Some common inquiries are descriptive, such as the means, conditional means, correlations, partial correlations, quantiles, and truth statements about variables in the model. Others are causal, such as the average difference in one variable when a second variable is set to two different values. We can think of a single-number inquiry as the atom of a research question.\nWhile most inquiries are “atomic” in this way, some inquiries are more complex than a single-number summary. For example, the best linear predictor of \\(Y\\) given \\(X\\) is a two-number summary: it is the pair of numbers (the slope and intercept) that minimizes the total squared distance between the line and each value of \\(Y\\). No need to stop at two-number summaries though. We could imagine the best quadratic predictor of \\(Y\\) given \\(X\\) (a three-number summary), and so on. We could have an inquiry that is the full conditional expectation function of \\(Y\\) given \\(X\\), no matter how wiggly, nonlinear, and nuanced the shape of that function. In could in principle be a 1,000 number summary of the model, or much more.\nThe inquiry could be constituted by a series of interrelated questions about the model. For instance, a researcher might articulate a handful of important questions about the model that all have to come out a certain way or the model itself should be rejected. These complex inquires are made up of a series of atomic inquiries. We’re interested in the sub-inquiries only insofar as they help us understand the real inquiry—is this model of the world a good one or not."
  },
  {
    "objectID": "declaration-diagnosis-redesign/defining-inquiry.html#elements-of-inquiries",
    "href": "declaration-diagnosis-redesign/defining-inquiry.html#elements-of-inquiries",
    "title": "\n7  Defining the inquiry\n",
    "section": "\n7.1 Elements of inquiries",
    "text": "7.1 Elements of inquiries\nEvery inquiry operates on the events generated by the model. We can think of the events as the data set that describes the units, treatment conditions, and outcome variables over which inquiries can be defined. This definition is closely connected to the common UTOS (units, treatments, outcomes, and settings) framework (Shadish, Cook, and Campbell 2002). The units are the set of units within the model that the inquiry refers to, either all or a subset. The treatment conditions represent the set chosen for study. A descriptive inquiry is a summary of a single condition (reality), whereas a causal inquiry is a summary of multiple conditions. The outcomes are the set of nodes in the model that the inquiry concerns. Finally, the inquiry operates on the model events via a summary function. For example, the “population average” inquiry summarizes the outcome for all units in the population with the mean function. We discuss each element of inquiries in turn.\n\n7.1.1 Units\nThe units of an inquiry are the set of people, places, or things that we are interested in studying. They may refer to all the units in a study or just a subset of them. For example, we can distinguish between many different average causal effect inquiries on the basis of their units: the average treatment effect (ATE) refers to all units in the study, the average treatment effect on the treated (ATT) refers to the units who actually are treated, the average treatment effect on the untretaed (ATU) refers to those who actually are not treated, and the complier average causal effect (CACE) refers to those who would take treatment if assigned to be treated but not otherwise.\nThe reason we need to specify the units of an inquiry is inquiry values (estimands) may differ across units. If the units that are included in the sample live in easier-to-reach areas and people who live in easier-to-reach areas are wealthier than others, the sample average will differ from the population average — and also from the average among those in hard-to-reach places.\nThe choice of which set of units to focus on is a theoretical one. To whom does the theoretical expectation apply? As a general matter, seeking insights that apply across many individuals is the goal of many social scientists. We are not typically interested in the effect of a treatment or the average outcome in a random sample of 100 units because we care about those units in particular, but because we wish to understand the treatment effect or outcomes in a broader population. Our theories often have so-called scope conditions, which define the types of units for which our theory is operative. A mechanism might operate only for coethnics of a country’s president, small-to-medium towns, blue collar workers, or the mothers of daughters. The units of an inquiry should be defined by these theoretical expectations, not by what inquiries our data and answer strategies can target easily.\nDistinctions among inquiries often arise in debates over instrumental variables designs, which target local average treatment effects (LATEs), meaning the average treatment among a subset. The effect these designs estimate is the average treatment effect among those units who are “compliers”. Compliers are the subset of units who take treatment if assigned and don’t take treatment if not assigned. The effect among compliers may or may not be like the effect among the whole sample or the population from which the sample was drawn. The debate between Deaton (2010) and Imbens (2010) centers precisely on which inquiry is the appropriate one, the LATE among compliers or the ATE in the whole sample. In many settings, the LATE may be the only inquiry we can reliably estimate, so the question becomes—is the LATE a theoretical relevant inquiry?\nIf the inquiry is defined with respect to the units sampled by the data strategy, then we do not have to engage in generalization inference—we learn directly about the sample from the sample. But if the inquiry is defined at the population level, then we need to generalize from the sample to the population. We also need to engage in generalization inference when we want to generalize study results to other populations that we did not explicitly sample from. Whether an inquiry requires generalization inference depends on the data strategy in this way. If the data strategy samples the units that define the inquiry, we do not need to generalize beyond the study. If the data strategy explicitly samples from a well-defined population, we can generalize from sample to population using canonical sampling theory. But if we want to generalize to an inquiry defined over some other set of units (for example, Brazilian citizens ten years in the future), we need to engage in generalization inference (See Egami and Hartman (2022)).\n\n7.1.2 Outcomes\nEvery inquiry is also defined by what outcomes are considered for each of the units. The choice of outcome is again a theoretical one: what outcomes are to be described, or with which outcomes do we want to measure the effects of treatment? An inquiry might be about a single outcome or multiple outcomes. The average belief that climate change is real would be a single-outcome inquiry, and the difference between that belief and support for government rebates for purchasing electric vehicles a multiple-outcome inquiry.\nIn some cases, an inquiry will be about a latent outcome that we cannot directly measure, such as preferences, attitudes, or emotions. We can construct data strategies that measure these latent outcomes by asking questions or observing behavior, but we cannot directly measure them. Even though these constructs may be difficult or impossible to measure well, it is often preferable to define the inquiry in terms of the latent outcome of interest rather than in terms of the measured outcome.\n\n7.1.3 Treatment conditions\nThe final element of an inquiry is the treatment conditions under consideration and, in the case of more than one, compared.\nDescriptive inquiries are defined with respect to one single treatment condition. That treatment condition is often the “unmanipulated” condition in which the researcher exposes units to no additional causal agents. Here the goal is not to learn about the summaries of the distributions of outcomes as we observe them. Table ?tbl-descriptiveinquiries (top panel) enumerates some common descriptive estimands. These estimands have in common that you do not need any counterfactual quantities in order to define them. The covariance (similarly, the correlation) between \\(X\\) and \\(Y\\) enters as a descriptive estimand, so too does the line of best fit for \\(Y\\) given \\(X\\).\nIn Table ?tbl-descriptiveinquiries, we enumerate several common types of descriptive inquiries, listing the units, treatment conditions, and outcomes that define them. We also provide R code snippets for each.\n\n(#tab:descriptiveinquiries) Examples of descriptive inquiries and their three elements: units, treatment conditions, and outcomes.\n\n\n\n\n\n\n\n\nInquiry\nUnits\nTreatment conditions\nOutcomes\nCode\n\n\n\nAverage value of variable Y in a finite population\nUnits in the population\nUnmanipulated\nY\nmean(Y)\n\n\nAverage value of variable Y in a sample\nSampled units\nUnmanipulated\nY\nmean(Y[S == 1])\n\n\nConditional average value of Y given X = 1\nUnits for whom X = 1\nUnmanipulated\nY\nmean(Y[X == 1])\n\n\nThe variance of Y\nUnits in the population\nUnmanipulated\nY\npop.var(Y)\n\n\nThe covariance of X and Y\nUnits in the population\nUnmanipulated\nX, Y\npop.cov(X, Y)\n\n\nThe best linear predictor of Y given X\nUnits in the population\nUnmanipulated\nY\ncov(Y, X) / var(X)\n\n\nConditional expectation function of Y given X\nUnits in the population\nUnmanipulated\nY\ncef(Y, X)\n\n\n\nCausal inquiries involve a comparison of at least two possible treatment conditions. For example, an inquiry might be the causal effect of \\(X\\) on \\(Y\\) for a single unit. In order to infer that causal effect, we would need to know the value of \\(Y\\) in two worlds: one world in which \\(X\\) is set to 1 and one in which \\(X\\) is set to 0. Table ?tbl-causalinquiries (middle panel) enumerates some common causal estimands. These estimands vary in the units they refer to. For instance, some are questions about samples (SATEs) and others about populations (PATEs). Inquiries can also be defined for units of a particular covariate class (CATEs). Finally, they may be summaries of more than one potential outcome. For instance, the interaction effect is defined here at the individual level as the effect of one treatment on the effect of another treatment.\n\n(#tab:causalinquiries) Examples of causal inquiries and their three elements: units, treatment conditions, and outcomes.\n\n\n\n\n\n\n\n\nInquiry\nUnits\nTreatment conditions\nOutcomes\nCode\n\n\n\nAverage treatment effect in a finite population (PATE)\nUnits in the population\nD = 0, D = 1\nY\nmean(Y_D_1 - Y_D_0)\n\n\nConditional average treatment effect (CATE) for X = 1\nUnits for whom X = 1\nD = 0, D = 1\nY\nmean(Y_D_1[X == 1] - Y_D_0[X == 1])\n\n\nComplier average causal effect (CACE)\nComplier units\nD = 0, D = 1\nY\nmean(Y_D_1[D_Z_1 > D_Z_0] - Y_D_0[D_Z_1 > D_Z_0])\n\n\nCausal interactions of \\(D_1\\) and \\(D_2\\)\n\nUnits in the population\nD1 = 1, D1 = 0, D2 = 1, D2 = 0\nY\nmean((Y_D1_1_D2_1 - Y_D1_0_D2_1) - (Y_D1_1_D2_0 - Y_D1_0_D2_0))\n\n\n\nGenerations of students have been told to excise words that connote causality from their empirical writing. “Affects” becomes “is associated with” and “impacts” becomes “moves with.” Being careful about causal language is of course very important (it’s really true that correlation does not imply causation!). But this change in language is not usually accompanied by a change in inquiry. Many times we are faced with drawing causal inferences from less than ideal data—but the deficiencies of the data strategy should not lead us too far away from our inferential targets. If the inquiry is a causal inquiry, then the move from “causes” to “is correlated with” might be a useful rhetorical tactic, but it doesn’t move us closer to providing an answer to the inquiry.\n\n7.1.4 Summary functions\nWith the units, treatments, and outcomes specified, the last element of the inquiry is the summary function that is applied to them. For a great many inquiries, this function is the mean function: the ATE, the CATE, the LATE, the SATE, the population mean—these are all averages. These and other inquiries are “decomposable” in the sense that you can think of an average effect for a large group as being the average of a set of average effects of smaller groups.\nHowever, not all inquiries are of this form. For example, the line of best fit is defined as the covariance of X and Y divided by the variance of X. This inquiry is a complex summary of all the units in the model.\nThe inquiry that the regression discontinuity design shoots at is also non-decomposable. In the RDD model (see Section (regression-discontinuity-designs?)), we imagine units with \\(Y_i(1)\\), \\(Y_i(0)\\). Each \\(i\\) also has a value on a “running variable”, \\(X_i\\), and units receive treatment if and only if \\(X_i>0\\). In this case the “effect at the point of discontinuity” might be written:\n\\[E_{i|X_i = 0}(Y_i(Z_i=1) - Y_i(Z_i=0))\\]\nCuriously, however, there may be no units for whom \\(X_i\\) equals exactly zero (a candidate who wins exactly 50% of the vote happens, but it is rare), so we cannot easily think of the inquiry as being a summary of individual potential outcomes. Instead, we construct a conditional expectation function for both potential outcome functions withe respect to \\(X_i\\) and evaluate the difference between these when \\(X_i = 0\\). Though not an average of individual effects, this difference is nevertheless a summary of the potential outcomes."
  },
  {
    "objectID": "declaration-diagnosis-redesign/defining-inquiry.html#types-of-inquiries",
    "href": "declaration-diagnosis-redesign/defining-inquiry.html#types-of-inquiries",
    "title": "\n7  Defining the inquiry\n",
    "section": "\n7.2 Types of inquiries",
    "text": "7.2 Types of inquiries\nThe largest division in the typology of inquiries is between descriptive and causal inquiries. It is for this reason that Part III, the design library, is organized into descriptive and causal chapters, separated by whether the data strategy is observational or experimental. In this section, we describe other important ways inquiries vary and how to think about declaring them.\n\n7.2.1 Data-dependent inquiries\nThe inquiries we have introduced thus far depend on variables in the model, but not on features of the data and answer strategies. However, common inquiries do depend on realizations of the research design.\nThe first type depends on realizations of the data \\(d\\): inquiries about units within a sample depend on which units enter the sample; inquiries about treated units depend on which are treated. For example, the average treatment effect on the treated (ATT) is a data-dependent inquiry in the sense that it is the average effect of treatment among the particular set of units that happened to be randomly assigned to treatment. The value of that particular ATT doesn’t change depending on the data strategy, of course, but which ATT we end up estimating depends on the realization of the data strategy. Table ?tbl-datadependentinquiry describes three data dependent inquiries\n\n(#tab:datadependentinquiry) Examples of data-dependent inquiries and their three elements: units, treatment conditions, and outcomes.\n\n\n\n\n\n\n\n\nInquiry\nUnits\nTreatment conditions\nOutcomes\nCode\n\n\n\nAverage treatment effect in a sample (SATE)\nSampled units\nD = 0, D = 1\nY\nmean(Y_D_1[S == 1] - Y_D_0[S == 1])\n\n\nAverage treatment effect on the treated (ATT)\nTreated units\nD = 0, D = 1\nY\nmean(Y_D_1[D == 1] - Y_D_0[D == 1])\n\n\nAverage treatment effect on the untreated (ATU)\nUntreated units\nD = 0, D = 1\nY\nmean(Y_D_1[D == 0] - Y_D_0[D == 0])\n\n\n\n7.2.2 Causal attribution inquiries\nA causal attribution is a different kind of data-dependent inquiry. Whereas a causal effect inquiry focuses on the change in an outcome that would be induced by a change in the causal variable, irrespective of the values that the outcome takes in the realized data. By contrast, causal attribution inquiries focus on inquiries that condition on realized outcomes, such as the “the absence of the outcome in the hypothetical absence of the treatment (\\(Y_i(0) = 0\\)) given the actual presence of both (\\(D_i = Y_i = 1\\))” (Yamamoto 2012, 240–41). In other words, had this feature been different would the outcome have been different? Goertz and Mahoney (2012) and others refer to causal attribution inquiries as cause-of-effects questions because they start with an outcome (an effect) and seek to validate a hypothesis about its cause.\nThe dependence of these inquiries on actual outcomes makes them harder (though not impossible!) to answer with the tools of quantitative science, though they are often of central interest to scientific and policy agendas and have occupied a large number of qualitative studies. Questions like “Was economic crisis necessary for democratization in the Southern Cone of Latin America?” or “Were high levels of foreign investment in combination with soft authoritarianism and export-oriented policies sufficient for the economic miracles in South Korea and Taiwan?” are examples of such inquiries (Goertz and Mahoney 2012). Though they bear a resemblance to causal effect inquiries that focus on observed subsets (such as the average treatment effect on the treated, or ATT)1 it is important not to confuse the two kinds of inquiries.\nWhile it is increasingly common to explicitly formalize causal effect inquiries, it is less common to formalize causal attribution inquiries. Doing so, however, can provide the specificity required to diagnose a design. Pearl (1999) provides formal definitions for these inquiries using the language of causal necessity and sufficiency, depicted in table ?tbl-causalattributioninquiries. To put these inquiries in the context of the democratic peace hypothesis, for example, in a given country dyad-year, \\(Y_i = 1\\) and \\(D_i = 1\\) could represent “Peace” and “Both democracies” and \\(Y_i = 0\\) and \\(D_i = 0\\) could represent “War” and “Not both democracies.”2 Then \\(\\Pr(Y_i(D_i = 0) = 0 \\mid D_i = Y_i = 1)\\) asks, among peaceful, fully democratic dyads, what is the proportion that would have had wars were they not both democracies—that is, in what proportion of dyad-years was democracy a necessary cause of peace? Similarly, \\(\\Pr(Y_i(D_i = 1)=1 \\mid D_i = Y_i = 0)\\) asks, among dyads that had a war and at least one non-democracy in a given year, what is the proportion that would have experienced peace if both countries were democracies—in other words, in what proportion of cases would democracy have been sufficient to cause peace? Yamamoto (2012) extends on this account to focus on causal attribution inquiries that focus on particular subsets, such as compilers.\n\n(#tab:causalattributioninquiries) Examples of causal attribution inquiries and their three elements: units, treatment conditions, and outcomes.\n\n\n\n\n\n\n\n\nInquiry\nUnits\nTreatment conditions\nOutcomes\nCode\n\n\n\nProbability D necessary for \\(Y\\)\n\nUnits for whom D = 1 and Y = 1\nD = 0\nY\nmean(Y_D_0[D == 1 & Y == 1] == 0)\n\n\nProbability D sufficient for \\(Y\\)\n\nUnits for whom D = 0 and Y = 0\nD = 1\nY\nmean(Y_D_1[D == 0 & Y == 0] == 1)\n\n\nComplier probability D necessary for \\(Y\\)\n\nUnits for whom D = 1 and Y = 1 who are compliers\nD = 0, Z = 1, Z = 0\nY\nmean(Y_D_0[D == 1 & Y == 1 & D_Z_1 == 1 & D_Z_0 == 0] == 0)\n\n\n\n7.2.3 Complex counterfactual inquiries\nThe causal inquiries we have considered thus far have involved comparisons of the counterfactual values an outcome could take, depending on the value of one or more treatment variables. These inquiries are mind-bending in that we have to imagine two counterfactual states at the same time. Complex counterfactual inquiries require more mind bending still.\nAn example of a complex counterfactual inquiry is the “controlled direct effect.” Suppose our model contains a treatment \\(Z_i\\), a mediator \\(M_i\\), and outcome \\(Y_i\\). The controlled direct effect of the treatment is defined as: \\[\\mathrm{CDE} = Y_i(Z=1, M_i=1) - Y_i(Z_i=0, M_i=1)\\] So far so good. Suppose now we stipulate that at least for some units \\(M_i\\) = 1 only when \\(Z\\) = 1, but it equals 0 when \\(Z_i\\) = 0 In order to imagine the CDE, we have to hold in our minds the complex counterfactual: what is the level of \\(Y_i\\), when \\(Z_i\\) equals 0, but \\(M\\) is at the value it would take if \\(Z_i\\) equaled one?\n\n7.2.4 Inquiries with continuous causal variables\nThe forgoing causal inquiries have focused on contrasts between discrete levels of a treatment variable. We can also imagine inquiries defined in continuous treatment spaces. For example, we could think of the effects of any level of salary from 5 dollars an hour to 500 dollars an hour on workplace satisfaction. We could “discretize” these continuous treatment in bins, in which case we are back to defining inquiries with discrete treatment conditions. Another possibility is to describe the inquiry as the average of the slopes from many lines of best fit. For each subject, we describe the line of best fit of the outcome with respect to the treatment. Our inquiry is then the average of the resulting slopes."
  },
  {
    "objectID": "declaration-diagnosis-redesign/defining-inquiry.html#how-to-define-inquiries",
    "href": "declaration-diagnosis-redesign/defining-inquiry.html#how-to-define-inquiries",
    "title": "\n7  Defining the inquiry\n",
    "section": "\n7.3 How to define inquiries",
    "text": "7.3 How to define inquiries\nThere are multiple criteria for choosing an inquiry. We want to pick one that is interesting in its own right or one that would facilitate a real-world decision. We want to pick research questions that we can learn the answer to someday, possibly with a lot of effort. We want to avoid unfeasible research questions. Among feasible research questions, we want to select ones that we are likely to obtain the most informative answers, in terms of moving our priors the most.\nSometimes, advisers tell students to follow a “theory-first” route to picking a research question. Read the literature, find an unsolved puzzle, then start choosing among the methodological approaches that might answer the problem. Others are more skeptical of starting with questions that might not be answerable and encourage students to first master tools that can answer particular types of questions and then find places to apply them. You don’t have to subscribe to either of these positions but you do have to keep an eye simultaneously on the substantive importance of questions and the scope for generating informative answers.\nThe first criterion for a good inquiry is then the subjective importance of a question. The answer may be important for science (building a theoretical understanding of the world) or for decision-making (choosing which policies to implement). Even so, the scientific enterprise is designed around the idea that importance is in the eye of the beholder and is not some objective quantity. This is for two reasons. First, the scientific or practical importance of a discovery may not be understood until decades later, when other pieces of the causal model are put together or the world faces new problems. Moreover, “importance” differs for different segments of society, and scientists must be able to study questions not judged important by groups in power in order to discover new ways to solve problems faced by the left-out groups.\nThe second important criterion for a good inquiry is that it should be answerable. How could an inquiry not be answerable? The main way an inquiry might not be answerable is we can’t find a feasible data or answer strategy. When for ethical, legal, logistic, or financial constraints, we simply can’t conduct the study, the inquiry is not answerable.\nThere are subtler ways in which an inquiry might not be answerable. For example, it might be undefined. Inquiries are undefined when I returns \\(I(m) = a_m = \\mathrm{NA}\\). For example, sometimes audit studies consider the effect of treatment on responding to an email and on the tone of the email. However, in conditions where the email is never sent, it has no tone. As a result, we can’t learn about the average effect of treatment on tone, we can only learn about the effect in a subgroup: those units who always respond to email, regardless of condition. This new inquiry is well-defined, but hard to estimate (see Coppock (2019))\nAn inquiry is also not answerable if it is not “identified.” Identification means: a question is at least partly answerable if there are at least two different sets of data you might observe that would lead you to make two different inferences. In the best case, one might imagine that you have lots of data and each possible data pattern you see is consistent with only one possible answer. You might then say that your model, or inquiry, is identified. Failing that you might imagine that different data patterns at least let you rule out some answers even though you can’t be sure of the right answer. In this case we have “partial identification.” Some inquiries might not even be partially identifiable. For instance if we have a model that says an outcome \\(Y\\) is defined by the equation \\(Y=(a+b)X\\), no amount of data can tell us the exact values of \\(a\\) and \\(b\\). Indeed without limits on the values of \\(a\\) and \\(b\\) (such as \\(a\\geq0\\)), no amount of data can even narrow down the ranges of \\(a\\) and \\(b\\). The basic problem is that for any value of \\(a\\) we can choose a \\(b\\) that keeps the sum of \\(a+b\\) constant. In this setting, even though there is an answer to our inquiry (\\(a\\)) in theory it is not one we can ever answer in practice. Many other types of inquiries, such as mediation inquiries, are not identifiable. There are some circumstances in which we can provide a partial answer to the inquiry, such as learning a range of values within which the parameter lives. At a minimum, we urge you to pose inquiries that are at least partially answerable with possible data.\nOne place in which a tradeoff between substantive importance and answerability can come to a head is in selecting the population for which an inquiry is defined. One common approach is to define inferences with respect to a “finite population.” For instance, all US states. You might then sample from this finite population in the data strategy in such a way that you can use the sample to draw inferences about the population. The probability distribution over the exogenous variables simply enumerates the values that these variables take on in the population. Any randomness in the design is generated by the sampling and, perhaps, by assignment procedures, not in the values of the exogenous variables.\nA second, and closely related, approach is to define inferences for a finite sample. This is like population inference when you sample the whole population. In a sense the sample is the population. Finite sample inference is common in research designs that involve random assignment of treatments. The only source of randomness in the finite sample setting is the random assignment itself.\nA third approach is to think in terms of “superpopulations”, in which we imagine that any particular population is just a draw from an infinite superpopulation. In this case, we can conceive of the randomness in the design as being fundamental—every unit is a random draw from the superpopulation.\nImplicitly if you set up a simulation and draw data using some probability density function you are drawing from a superpopulation. But you get to specify the type of target of inference when you specify the inquiry, as in Declaration 7.1.\n\nDeclaration 7.1 Super-population, population, and finite sample design\n\ndeclaration_7.1 <-\n  declare_model(\n    N = 20, \n    U = rnorm(N),\n    Y = 1 + U\n  ) +\n  declare_inquiry(\n    superpopulation_mean = 1,\n    population_mean = mean(Y)\n  ) + \n  declare_sampling(\n    S = complete_rs(N, n = 10)\n  ) +\n  declare_inquiry(sample_mean = mean(Y))\n\n\nHere is one draw on the estimands:\n\ndraw_estimands(declaration_7.1)\n\n\n\n\n\nSuper-population, population, and finite sample inquiries.\n \n inquiry \n    estimand \n  \n\n\n superpopulation_mean \n    1.00 \n  \n\n population_mean \n    1.19 \n  \n\n sample_mean \n    1.50 \n  \n\n\n\n\nWhich of these to choose? Researchers sometimes prefer superpopulation inquiries because they describe general processes on substantive grounds, seeing understanding general processes as the primary goal of social scientific interest. Some are skeptical however about speculating about general, unobservable, processes, preferring to make statements about cases that actually exist in the world. They prefer to select populations of substantive importance. Others seeking to avoid engaging in generalization inferences prefer to focus on sample quantities. In some cases the statistics are more suited for finite populations: for instance randomization inference is based on the randomized induced by assignment and not from sampling populations from superpopulations or samples from populations.3 Critics worry that keeping the focus on the sample means having inquiries that are determined by the realizations of your data strategy rather than having data strategies developed to answer prespecified inquiries."
  },
  {
    "objectID": "declaration-diagnosis-redesign/defining-inquiry.html#summary",
    "href": "declaration-diagnosis-redesign/defining-inquiry.html#summary",
    "title": "\n7  Defining the inquiry\n",
    "section": "\n7.4 Summary",
    "text": "7.4 Summary\nInquiries are our targets of inference, stated in terms of a reference model of the world. We described how inquiries are defined with respect to specific outcomes expressed by specific units under specific conditions, which are summarized with some function. Inquiries can be descriptive or causal, depending on the mix conditions they depend on. They can be data-dependent or not and decomposable or not. They should be well-defined and they should be answerable. When we lose track of our inquiries, research studies can end up estimating whatever the answer strategy ends up targeting. Researchers should choose their inquiries with intention so that they can select appropriate empirical strategies for that inquiry.\n\n\n\n\nAronow, Peter M., Donald P. Green, and Donald K. K. Lee. 2014. “Sharp Bounds On The Variance In Randomized Experiments.” The Annals of Statistics 42 (3): 850–71.\n\n\nCoppock, Alexander. 2019. “Avoiding Post-Treatment Bias in Audit Experiments.” Journal of Experimental Political Science 6 (1): 1–4.\n\n\nDawid, Philip, Macartan Humphreys, and Monica Musio. 2022. “Bounding Causes of Effects with Mediators.” Sociological Methods & Research.\n\n\nDeaton, Angus S. 2010. “Instruments, Randomization, and Learning about Development.” Journal of Economic Literature 48 (2): 424–55.\n\n\nEgami, Naoki, and Erin Hartman. 2022. “Elements of External Validity: Framework, Design, and Analysis.” American Political Science Review.\n\n\nGoertz, Gary, and James Mahoney. 2012. A Tale of Two Cultures: Qualitative and Quantitative Research in the Social Sciences. Princeton: Princeton University Press.\n\n\nImbens, Guido W. 2010. “Better LATE Than Nothing: Some Comments on Deaton (2009) and Heckman and Urzua (2009).” Journal of Economic Literature 48 (2): 399–423.\n\n\nPearl, Judea. 1999. “Probabilities of Causation: Three Counterfactual Interpretations and Their Identification.” Synthese 121 (1-2): 93–149.\n\n\nShadish, William, Thomas D. Cook, and Donald Thomas Campbell. 2002. Experimental and Quasi-Experimental Designs for Generalized Causal Inference. Boston: Houghton Mifflin.\n\n\nYamamoto, Teppei. 2012. “Understanding the Past: Statistical Analysis of Causal Attribution.” American Journal of Political Science 56 (1): 237–56."
  },
  {
    "objectID": "declaration-diagnosis-redesign/crafting-research-strategy.html",
    "href": "declaration-diagnosis-redesign/crafting-research-strategy.html",
    "title": "\n8  Crafting a data strategy\n",
    "section": "",
    "text": "In order to collect information about the world, researchers must deploy a data strategy. Depending on the design, the data strategy could include decisions about any or all of the following: sampling, assignment, and measurement. Sampling is the procedure for selecting which units will be measured; assignment is the procedure for allocating treatments to sampled units; and measurement is the procedure for turning information about the sampled units into data. These three procedures parallel the three elements of an inquiry: the units, treatment conditions, and outcomes.\nWe think about data strategies in response to Principle 3.1: Design holistically – we make data strategy choices to respond to model features.\nSampling choices are used to justify generalization inferences: we want to make general claims which often implies inferences about units not sampled. For this reason, we need to pay special attention to the procedure by which units are selected into the sample. We might use a random sampling procedure in order to generate a design-based justification for generalizing from samples to populations. Nonrandom sampling procedures are also possible: convenience sampling, respondent-driven sampling, and snowball sampling are examples of data strategies that do not include an explicitly randomized component.\nAssignment choices are used to justify causal inferences: we want to make inferences about the conditions to which units were not assigned. For this reason, experimental design is focused on the assignment of treatments. Should the treatment be randomized? How many treatment conditions should there be? Should we use a simple coin flip to decide who receives treatment, or should we use a more complicated strategy like blocking?\nMeasurement choices are used to justify descriptive inferences: we want to make inferences about latent values not observed on the basis of measured values. The tools we use to measure are a critical part of the data strategy. For many social scientific studies, a main way we collect information is through surveys. A huge methodological literature on survey administration has been developed to help guide questionnaire development. Bad survey questions yield distorted or noisy responses due to large measurement error. A biased question systematically misses the true latent target it is designed to measure, in which case we say the question has low “validity.” A question is high variance if (hypothetically) we would obtain different answers each time we ask, in which case we say the question has low “reliability.” The concerns about validity and reliability do not disappear once we move out of the survey environment. For example, the information that shows up in an administrative database is itself the result of many human decisions, each of which has the possibility of increasing or decreasing the distance between the measurement and the latent measurement target.\nStrong research design can help address these three inferential challenges, but we can never be sure that our sample generalizes, or that we know what would have happened in a counterfactual state of the world, or what the true latent value of the outcome is (or if it even exists). Researchers have to choose good sampling, assignment, and measurement techniques that, when combined and applied to the world, will produce analysis-ready information.\nMore formally, the data strategy D is a set of procedures that result in a dataset \\(d^*\\). It is important to keep these two concepts straight. If you apply data strategy D to the world \\(m^*\\), it produces a dataset \\(d^*\\). We say \\(d^*\\) is “the” result of D, since when we apply the data strategy to the world, we only do so once and we obtain the data we obtain. But when we are crafting a data strategy, we have to think about the many datasets that the data strategy could have produced under all the models in M, since we don’t know which one \\(m^*\\) is. Some of the datasets might be really excellent (from the researcher’s perspective). For example, in good datasets, we achieve good covariate balance across the treatment and control groups. Or we might draw a sample whose distribution of observable characteristics looks really similar to the population. But some of the datasets might be worse: because of the vagaries of randomization, the particular realizations of the random assignment or random sampling might be more or less balanced. We do not have to settle for data strategies that might produce weak datasets – we are in control of the procedures we choose. We want to choose a data strategy D that is likely to result in a high-quality dataset \\(d^*\\).\nIn Figure 8.1, we illustrate the three elements of data strategies: sampling (\\(S\\)), treatment assignment (\\(Z\\)), and measurement (\\(Q\\)). These nodes are highlighted by blue boxes to emphasize that they are in the control of the researcher. No arrows go into the \\(S\\), \\(Z\\), or \\(Q\\) nodes; are set by the researcher. In each case, the strategy selected by the researcher affects a corresponding endogenous variable. The sampling procedure causes changes in the endogenous response (\\(R\\)), which represents whether participants provide outcome data, for example responding to survey questions. \\(R\\) is not under the full control the researchers: it is affected by \\(S\\), the sampling procedure, but also by the idiosyncratic choices of participants who have higher and lower interest and ability to respond and participate in the study (\\(U\\)). Similarly, the endogenous variable treatment \\(D\\) represents whether participants actually receive the treatment, regardless of their assignment \\(Z\\). \\(D\\) is affected by the treatment assignment procedure (\\(Z\\)) of course. But except in cases when \\(Z\\) fully determines \\(D\\) (no noncompliance), we are concerned that it will be affected by unobserved idiosyncratic features of individuals \\(U\\). The third researcher node is \\(Q\\), the measurement procedure. \\(Q\\) affects \\(Y\\), the observed outcome, measured by the researcher. \\(Y\\) is also affected by a latent variable \\(Y^*\\), which cannot be directly observed. The measurement procedure provides an imperfect measurement of that latent variable, which is (potentially) affected by treatment \\(D\\) and unobserved heterogeneity \\(U\\). In the robustness section at the end of the chapter, we explore further variations in this DAG that incorporate threats to inference from noncompliance, attrition, excludability violations, and interference."
  },
  {
    "objectID": "declaration-diagnosis-redesign/crafting-research-strategy.html#elements-of-data-strategies",
    "href": "declaration-diagnosis-redesign/crafting-research-strategy.html#elements-of-data-strategies",
    "title": "\n8  Crafting a data strategy\n",
    "section": "\n8.1 Elements of data strategies",
    "text": "8.1 Elements of data strategies\nIn chapter (defining-the-inquiry?), an inquiry \\(I\\) was characterized by a set of outcomes, a set of treatment conditions, and a set of units, as well as a function that summarizes those outcomes, assessed at those conditions, over those units. The three elements of the data strategy \\(D\\) parallel the first three elements of inquiries: we sample units, assign treatment conditions, and measure outcomes.\n\n8.1.1 Sampling\nSampling is the process by which units are selected from the population to be studied. The starting point for every sampling strategy should be to consider the units defined in the inquiry. In some cases, all the units to whom the inquiry apply are included in the study, but in others, we consider only a subset.\nWhy would we ever be content to study a sample and not the full population? For infinite populations, we have no choice. For finite populations the first and best explanation is cost: it’s expensive and time-consuming to conduct a full census of the population. Even well-funded research projects face this problem, since money and effort spent answering one question could also be spent answering a second question. A second reason to sample is the diminishing marginal returns of additional data collection. Increasing the number of sampled units from 1,000 to 2,000 will greatly increase the precision of our estimates. Moving from 100,000 to 101,000 will improve things too, but the scale of the improvement is much smaller. Finally, it may simply not be possible to sample some units. Units in the distant past or distant future, for example, are not available to be sampled, even if they are in the set of units that define the inquiry.\nSome sampling procedures involve randomization while others do not. Whether a sampling procedure is randomized or not has large implications for the answer strategy. Randomized designs support “design-based inference,” which refers to the idea that we rely on known features of the sampling process when producing population-level estimates – much more about this in the next chapter on answer strategies. When randomization breaks down (e.g., if the design encounters attrition) or if non-randomized designs are used, then we have to fall back on model-based inference to generalize from the sample to the population. Model-based inference relies on researcher beliefs about the nature of the uncontrolled sampling process in order to make inferences about the population. When possible, design-based inference has the advantage of letting us ground inferences in known rather than assumed features of the world. That said, when randomly sampled individuals fail to respond or when we seek to make inferences about new populations, we oftentimes fall back to model-based inference.\n\n8.1.1.1 Randomized sampling designs\nOwing to the natural appeal of design-based inference, we start off with randomized designs before proceeding to non-randomized designs. Randomized sampling designs typically begin with a list of all units in a population, then choose a subset to sample using a random process. These random processes can be simple (every unit has an equal probability of inclusion) or complex (first we select regions at random, then villages at random within selected regions, then households within selected villages, then individuals within selected households).\nTable ?tbl-samplingtypes collects all of these kinds of random sampling together and offers an example of functions in the randomizr package you can use to conduct these kinds of sampling. The most basic form is simple random sampling. Under simple random sampling, all units in the population have the same probability \\(p\\) of being included in the sample. It is sometimes called coin flip random sampling because it is as though for each unit, we flip a weighted coin that has probability \\(p\\) of landing heads-up. While quite straightforward, a drawback of simple random sampling is that we can’t be sure of the number of sampled units in advance. On average, we’ll sample \\(N*p\\) units, sometimes slightly more units will be sampled and sometimes fewer.\n\n(#tab:samplingtypes) Kinds of random sampling\n\n\n\n\n\nDesign\nDescription and randomizr R code\n\n\n\nSimple random sampling\n\n“Coin flip” or Bernoulli random sampling. All units have the same inclusion probability p\nsimple_rs(N = 100, p = 0.25)\n\n\n\nComplete random sampling\n\nExactly n of N units are sampled, and all units have the same inclusion probability n/N\nsimple_rs(N = 100, p = 0.25)\n\n\n\nStratified random sampling\n\nComplete random sampling within pre-defined strata. Units within the same strata have the same inclusion probability n_s / N_s\nstrata_rs(strata = regions)\n\n\n\nCluster random sampling\n\nWhole groups of units are brought into the sample together.\ncluster_ra(clusters = households)\n\n\n\nStratified cluster sampling\n\nCluster random sampling within strata\nstrata_and_cluster_rs(strata = regions,clusters = villages)\n\n\n\nMulti-stage random sampling\n\nFirst clusters, then units within clusters\ncluster_ra(clusters = villages)\nstrata_ra(strata = villages)\n\n\n\n\nComplete random sampling addresses this problem. Under complete random sampling, exactly \\(n\\) of \\(N\\) units are sampled. Each unit still has an inclusion probability of \\(p = n/N\\), but in contrast to simple random sampling, we are guaranteed that the final sample will be of size \\(n\\).1 Complete random sampling represents an improvement over simple random sampling because it rules out samples in which more or fewer than \\(N*p\\) units are sampled. One circumstance in which we might nevertheless go with simple random sampling is when the size of the population is not known in advance, sampling choices may have to be made “on the fly.”\nComplete random sampling solves the problem of fixing the total number of sampled units, but it doesn’t address the problem that the total number of units with particular characteristics will not be fixed. Imagine a population with \\(N_{y}\\) young people and \\(N_{o}\\) old people. If we sample exactly \\(n\\) from the population \\(N_{y} + N_{o}\\), the number of sampled young people (\\(n_y\\)) and sampled old people (\\(n_{o}\\)) will bounce around from sample to sample. We can solve this problem by conducting complete random sampling within each group of units. This procedure goes by the name stratified random sampling, since the sampling is conducted separately within the strata of units.2 In our example, our strata were formed by a dichotomous grouping of people into “young” and “old” categories, but in general, the sampling strata can be defined by any information we have about units before they are sampled. Stratification offers at least three major benefits. First, we defend against sampling surprisingly too few units in some stratum by “bad luck.” Second stratification tends to produce lower variance estimates of most inquiries. Finally, stratification allows researchers to “oversample” subgroups of particular interest (see Section (subgroup-designs?)).\nStratified sampling should not be confused with cluster sampling. Stratified sampling means that a fixed number of units from a particular group are drawn into the sample. Cluster sampling means that units from a particular group are brought into the sample together. For example, if we cluster sample households, we interview all individuals living in a sampled household. Clustering introduces dependence in the sampling procedure – if one member of the household is sampled, the other members are also always sampled. Relative to a complete random sample of the same size, cluster samples tend to produce higher variance estimates. Just as the individual sampling designs, cluster sampling comes in simple, complete, and stratified varieties with parallel logics and motivations.\nLastly, we turn to multi-stage random sampling, in which we conduct random sampling at multiple levels of a hierarchically-structured population. For example, we might first sample regions, then villages within regions, then households within villages, then individuals within households. Each of those sampling steps might be stratified or clustered depending on the researcher’s goals. The purpose of a multi-stage approach is typically to balance the logistical difficulties of visiting many geographic areas with the relative ease of collecting additional data once there.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8.2 gives a graphical interpretation of each of these kinds of random sampling. Here, we imagine a population of 64 units with two levels of hierarchy. For concreteness, we can imagine that the units are individuals nested within 16 households of four people each and the 16 households are nested within four villages of four people each. Starting at the top left, we have simple random sampling at the individual level. The inclusion probability was set to 0.5, so on average, we ought to sample 32 people, but in this particular draw, we actually sampled only 29. Complete random sampling (top center), fixes this problem, so exactly 32 people are sampled – but these 32 are unevenly spread across the four villages. This is addressed with stratified sampling. In the top right, we sample exactly 8 people at random from each village of 16 total people.\nMoving down to the middle row of the figure, we have three approaches to clustered random sampling. Under simple random sampling at the cluster level, each cluster has the same probability \\(p\\) of inclusion in the sample, so on average we will sample eight clusters. This time, we only sampled seven. This problem can again be fixed with complete random sampling (center facet), but again we have an uneven distribution across villages. Stratified cluster sampling ensures that exactly two households from each village are sampled.\nThe bottom row of the figure illustrates some approaches to multistage sampling. In the bottom left panel, we conduct a simple random sample of individuals in each sampled cluster. In the bottom center, we draw a complete random sample of individuals in each sampled household. And in the bottom right, we stratify on an individual level characteristic – we always draw one individual from each row of the household. “Row” could refer to the age of the household members. This doubly-stratified multistage random sampling procedure ensures that we sample two households from each village and within those households, one older member and one younger member.\n\n\nFigure 8.2: Nine kinds of random sampling. In the first row individuals are the sampling units, in the second row clusters are sampled, in the third clusters are sampled and then individuals within these clusters are sampled. In the first column units are sampled independently, in the second units are sampled to hit a target, in the third units are sampled to hit targets within strata.\n\n\n\n8.1.1.2 Nonrandomized sampling designs\nBecause non-randomized sampling procedures are defined by what they don’t do – they don’t use randomization – the term encompasses a hugely varied set of procedures. We’ll consider just a few common ones, since the idiosyncrasies of each nonrandomized approach are hard to systematize.\nConvenience sampling refers to the practice of gathering units from the population in an inexpensive way. Convenience sampling is a good choice when generalizing to an explicit population is not a main goal of the design, for example when a sample average treatment effect is a theoretically-important inquiry. For many decades, social science undergraduates were the most abundant data source available to academics and many important theoretical claims have been established on the basis of experiments conducted with such samples. In recent years, however, online convenience samples like Mechanical Turk, Prolific, or Lucid have mostly supplanted undergraduates as the convenience sample of choice. Convenience sampling may to lead to badly biased estimates of population quantities. For example, cable news shows often conduct viewer polls that should not be taken at all seriously. While such polls might promote viewer loyalty (and so might be worth doing from the cable executives’ perspective) they do not provide credible evidence about what the population at large thinks or believes.\nMany types of qualitative and quantitative research involve convenience sampling. Archival research often involves the “convenience” sample of documents on a certain topic that exist in an archive. The question of how these documents differ from those that would be in a different archive, or how the documents available in archives differ from those that do not ever make it into the archive importantly shapes what we can learn from them. With the decline of telephone survey response rates, researchers can no longer rely on random digit dialing to obtain a representative sample of people in many countries, and instead must rely on convenience samples from the internet or panels who agree to have their phone numbers in a list. Reweighting techniques in the answer strategy can, in some cases, help recover estimates for the population as a whole if sampling if a credible model of the unknown sampling process can be agreed upon.\nNext, we consider purposive sampling. Purposive is a catch-all term for rule-based sampling strategies that do not involve random draws but also are not purely based on convenience and cost. A common example is quota sampling. Sampling purely based on convenience often means we will end up with many units of one type but very few of another type. Quota sampling addresses the problem by continuing to search for subjects until target counts (quotas) of each kind of subject are found. Loosely speaking, quota sampling is to convenience sampling as stratified random sampling is to complete random sampling: it fixes the problem that not enough (or too many) subjects of particular types are sampled by employing specific quotas. Importantly, however, we have no guarantee that the sampled units within a type are representative of that type overall. Quota samples remain within-stratum convenience samples.\nA second common form of purposive sampling is respondent-driven sampling (RDS), which is used to sample from hard-to-reach populations such as HIV-positive needle users. RDS methods often begin with a convenience sample and then systematically obtain contacts for other units who share the same characteristic in order the build a large sample.\nEach of these three nonrandom sampling procedures – convenience, quota, and respondent-driven – is illustrated in Figure 8.3. Imagining that village A is easier to reach, we could obtain a convenience sample by contacting everyone we can reach in village A before moving on to village B. This process doesn’t yield good coverage across villages. For that, we can turn to quota sampling scheme in which we talk to the five people who are easiest to reach in each of the four villages. Finally, if we conduct a respondent-driven sample, we select one seed unit in each village, and that person recruits their four closest friends (who may or may not reside in the same village).\n\n\nFigure 8.3: Three forms of non-random sampling.\n\n\n\n8.1.1.3 Sampling designs for qualitative research\nAnother term for sampling is case selection. In case study research, whether qualitative or quantitative, the way we select the (typically small) set of cases is of great importance, and considerable attention has been paid to developing case selection methods.\nAdvice for selecting cases ranges widely with many seeming disagreements across scholars (see for instance the symposium in Collier et al. (2008)). We describe the major strategies used below and highlight some of the goals and assumptions motivating them. The most general advice however, is that there are likely situations and rationales that could justify any of these strategies. But whether one or other strategy is right for the problem you face mostly likely depends on the three other components of your design: what your model set is, what your inquiry is, and what your answer strategy is. Conversely, it is very difficult to assess whether one approach is more appropriate than another without knowing about these other parts of a design because is hard to tell whether a case will be useful without knowing what you plan to do with it. In short, the case selection decision is one that is usefully made, and justified, by diagnosis.\nGeddes (2003) warned that “the cases you choose affect the answers you get.” This warning emphasizes the importance of case selection. If we select cases in order to arrive at a particular answer, then the research design doesn’t provide good evidence in favor of the answer.\nNon-purposive selection. Fearon and Laitin (2008) argue that the best approach is to select randomly. The argument for this approach depends on the purpose and details of the design. If the goal is to use case studies to check the quality of data used in large \\(n\\) analysis, or to explore the sets of pathways that might link a cause to an outcome, then random selection has the virtue of generating a representative set of cases and guards against cherry picking.\nPositive selection. Goertz (2008) argues that one should select multiple cases for which a positive outcome (e.g., a revolution) is unambiguously observed and one should also seek diversity in possible causes. The goal is to have many opportunities as possible to observe possibly distinct paths leading to an outcome. We have a different perspective on the positive selection procedure, for two reasons. First, the approach presupposes an ability to figure out the causal factors within a case, i.e., it presupposes that one can assess the counterfactual values of outcomes within a case. Second, even if one can do single-case causal inference, Goertz argues that cases in which \\(X=0\\) and \\(Y=0\\) are not very useful for figuring out if \\(X=1\\) causes \\(Y=1\\). On the contrary, we might believe that the effect of \\(X\\) on \\(Y\\) runs through a positive effect of \\(X\\) on \\(M\\) and a positive effect of \\(M\\) on \\(Y\\). But if looking at an \\(X=0, Y=0\\) case we find that, awkwardly, \\(M=1\\), the evidence casts doubt on the causal importance of \\(X\\) in the \\(X=Y=1\\) cases. Ultimately, whether positive selection approach is appropriate in any given instance is a question for diagnosis insofar it depends on the model, the inquiry, and the answer strategy.\nOther purposive strategies. Lieberman (2005) proposes using the predicted values from a initial cross-case regression model in order to select cases for in-depth analysis. Exactly how to select however depends on the inquiry and answer strategy. When the inquiry is focused on uncovering the same causal relationship sought in the regression analysis, Lieberman (2005) suggests selecting cases that are relatively well-predicted and that maximize variation on the causal variable. He points to Martin (1992) and Swank (2002) as examples of designs employing this strategy. However, Lieberman (2005) advocates a different case selection strategy when the goal is to expand upon the theory initially tested in the regression analysis. In that instance, he recommends choosing cases lying far from the regression line, which are not well-predicted and may therefore lead to insights about what alternative mechanisms were left out of the initial regression.\nSeawright and Gerring (2008) use the regression line analogy to describe seven different sampling strategies tailored to suit different inquiries.3 These include “typical cases” which are representative of the cross-case relationship and can be chosen in order to explore and validate mediating mechanisms. If the researcher’s model implies union membership increases welfare spending in democracies through its effects on negotiations with the government, for example, then the researcher might look for evidence of such processes in the cases well-predicted by the theory. Diverse cases maximize variation on both \\(X\\) and \\(Y\\), while extreme cases are located at a maximal distance from other cases on just one dimension—in our example, the researcher chooses the two cases with the highest degree of union strength. While diverse and extreme cases might lie on the regression line, deviant cases are defined by their distance from it. Such cases call for new explanations to account for outcomes. Influential cases are those whose exclusion would most noticeably change the imaginary regression line (i.e., those with the highest leverage in a regression).\nMills methods. Two more approaches, correspond to “methods of difference” and “methods of similarity” (Mill 1869). The method of difference approach selects a set of cases that are similar in a set of pretreatment variables but nevertheless differ in \\(Y\\). This gives an opportunity to search for a cause other than those held constant that could explain the variation. The method of similarity approach selects a set of cases that have similar outcomes and discounts causes that vary across these cases and focuses on potential causes that do not. If one characteristic covaries with the outcome, it becomes a candidate for the cause. For example, Skocpol (1979) compares historical periods in France, Russia, the United Kingdom, and Germany that look very similar in many regards. The first two, however, had social revolutions, while the second two did not. The presence of agrarian institutions that provided a degree of political autonomy to the peasants in France and Russia and their absence in the UK and Germany then becomes a possible clue to understanding the underlying causal structure of social revolutions. By contrast, the method of agreement involves examining cases that share the same outcome but diverge on other characteristics. Any characteristics that are common to the cases then become candidates for causal attribution. These “methods” are inferential rules given characteristics of cases.\nThese methods make sense for identifying possible causes within cases. But they are a dangerous guide to case selection if you want to use covariation to assess the effect of a putative cause, and you selection on the basis of both causes and outcomes. Simply put if we select two cases because they differ on the outcome but on all but one (observable) characteristic and then apply the method of difference to conclude that the different factor made the difference, then we have effectively selected the answer. More generally, if the information used to make an inference is already available prior to data gathering, then there is nothing to be gained from the data gathering.4 Following Principle 3.1: Design holistically will point to the errors of the strategy.\nHerron and Quinn (2016) used Monte Carlo simulations to study how well these strategies perform for the specific question of providing leverage on average causal effects. The inquiry is the average treatment effect in the population, and the answer strategy involves, perhaps optimistically, perfectly observing the selected cases’ causal types. With these simplifying assumptions, they uncover a clear hierarchy and set of prescriptions: extreme and deviant case selection fare much worse than the other methods in terms of the three diagnosands considered (root mean square error, variance, and bias of the mean of the posterior distribution). By contrast, influential case selection outperforms the other strategies, followed closely by diverse and simple random sampling. As the authors acknowledge, however, this hierarchy might look very different if the inquiry aimed at a different, exploratory quantity (such as discovering the number of causal types that exist).\nOther advice focuses less on the values of \\(X\\) and \\(Y\\) and more about the scope for learning within the case. Humphreys and Jacobs (2015) provide simulations of a process tracing procedure that highlight the importance of “probative value” for case selection. The point is that there is rarely a case selection strategy that fits all problems equally well—the best strategy is the one that optimizes a particular diagnosand given stipulations about the inquiry, the model, and the answer strategy. If you can justify those stipulations and the importance of the diagnosand, then defending the choice of sampling strategy is straightforward.\nFinally, Levy (2008) clarifies the logic behind “most likely” and “least likely” case selection strategies – what are sometimes called “crucial case” designs. The idea here is that we may have beliefs over the heterogeneity of causal effects over cases but uncertainty about the level. If we learn that a causal effect is indeed in operation in a least likely case, we update on our beliefs about it operating in other cases. This is “Sinatra inference” (Levy 2008): “if I can make it [in New York], I’ll make it anywhere.” Conversely the most likely case is based on the idea that if I can’t make it in New York then I can’t make it anywhere! Both logics presuppose an answer strategy that can reliably impute counterfactual outcomes.\n\n8.1.1.4 Choosing among sampling designs\nThe choice of sampling strategy depends on features of the model and the inquiry, and different sampling strategies can be compared in terms of power and RMSE in design diagnosis. The model defines the population of units we want to make inferences about, and the sampling frame of the sampling strategy should match that as much as possible. The model also points us to important subgroups that we may wish to stratify on, depending on the variability within those subgroups. Whether we select convenience, random, or purposive sampling depends on our budget and logistical constraints as well as the efficiency (power or RMSE) of the design. If there is little bias from convenience sampling, we will often want to select it for cost reasons. If we cannot obtain a convenience sample that has the right composition, we may choose a purposive method that ensures we do. The choice between simple and stratified sampling comes down to the inquiry and to a diagnosis of the RMSE. When the inquiry involves a comparison of subgroups, we will often select stratified sampling. In either, a diagnosis of alternative designs in terms of power or RMSE will guide selection.\n\n8.1.2 Treatment assignment\nIn many studies, researchers intervene in the world to set the level of the causal variable of interest. The procedures used to assign units to treatment are tightly analogous to the procedures explored in the previous section on sampling. Like sampling, assignment procedures fall into two classes, randomized and non-randomized.\n\n8.1.2.1 Two arm trials\nThe analogy between sampling and assignment runs deep. All of the sampling designs discussed in the previous section have directly equivalent assignment designs. Simple random sampling is analogous to Bernoulli random assignment, stratified random sampling is analogous to blocked random assignment and so on. Many of the same design tradeoffs hold as well: just like cluster sampling generates higher variance estimates than individual sampling, clustered assignment generates higher variance estimates than individual assignment. While we usually think of randomized assignment designs only, non-randomized designs in which the researcher applies treatments also occur. For example, researchers sometimes treat a convenience sample, then search out a different convenience sample to serve as a control group. Within-subject designs in which subjects are measured, then treated, then measured again are a second example of a non-randomized application of treatment.\nThe analogy between sampling and assignment runs so deep because, in a sense, assignment is sampling. Instead of sampling units in or out of the study, we sample from alternative possible worlds. The treatment group represents a sample from the alternative world in which all units are treated and the control group represents a sample from the alternative world in which all units are untreated.5 We can re-encounter the fundamental problem of causal inference through this lens – if a unit is sampled from one possible world, it can’t be sampled from any other possible world. Table ?tbl-assignmenttypes collects together common forms of random assignment.\n\n(#tab:assignmenttypes) Kinds of random assignment\n\n\n\n\n\nDesign\nDescription and randomizr R code\n\n\n\nSimple random assignment\n\n“Coin flip” or Bernoulli random assignment. All units have the same probability of assignment\nsimple_ra(N = 100, prob = 0.25)\n\n\n\nComplete random assignment\n\nExactly m of N units are assigned to treatment, and all units have the same probability of assignment m/N\ncomplete_ra(N = 100, m = 40)\n\n\n\nBlock random assignment\n\nComplete random assignment within pre-defined blocks. Units within the same block have the same probability of assignment m_b / N_b\nblock_ra(blocks = regions)\n\n\n\nCluster random assignment\n\nWhole groups of units are assigned to the same treatment condition.\ncluster_ra(clusters = households)\n\n\n\nBlock-and-cluster assignment\n\nCluster random assignment within blocks of clusters\nblock_and_cluster_ra(blocks = regions, clusters = villages)\n\n\n\nSaturation random assignment\n\nFirst clusters are assigned to a saturation level, then units within clusters are assigned to treatment conditions according to the saturation level\nsaturation = cluster_ra(clusters = villages,\n                        conditions = c(0, 0.25, 0.5, 0.75))\nblock_ra(blocks = villages, prob_unit = saturation)\n\n\n\n\nFigure 8.4 visualizes nine kinds of random assignment, arranged according to whether the assignment procedure is simple, complete, or blocked and according to whether the assignment procedure is carried out at the individual, cluster, or saturation level. In the top left facet, we have simple (or Bernoulli) random assignment, in which all units have a 50% probability of treatment, but the total number of treated units can bounce around from assignment to assignment. In the top center, this problem is fixed: under complete random assignment, exactly \\(m\\) of \\(N\\) units are assigned to treatment and the \\(N - m\\) are assigned to control. While complete random assignment fixes the number of units treated at exactly \\(m\\), the number of units that are treated within any particular group of units (defined by a pre-treatment covariate) could vary. Under block random assignment, we conduct complete random assignment within each block separately, so we directly control the number treated within each block. Moving from simple to complete random assignment tends to decrease sampling variability a bit, by ruling out highly unbalanced allocations. Moving from complete to blocked can help more, so long as the blocking variable is correlated with the outcome. Blocking rules out assignments in which too many or too few units in a particular subgroup are treated.\nThe second row of Figure 8.4 shows clustered designs in which all units within a cluster receive the same treatment assignment. Clustered designs are common for household-level, school-level, or village-level designs, where it would be impractical or unfeasible to conduct individual level assignment. When units within the same cluster are more alike than units in different clusters (as in most cases), clustering increases sampling variability relative to individual level assignment. Just like in individual level designs, moving from simple to complete or from complete to blocked tends to result in lower sampling variability.\nThe final row of Figure 8.4 shows a series of designs that are analogous to the multi-stage sampling designs shown in Figure 8.2 – but their purpose is subtly different in spirit. Multi-stage sampling designs are employed to reduce costs – first clusters are sampled but not all units within a cluster are sampled. A saturation randomization design (sometimes called a “partial population design”, see section (randomized-saturation-experiments?)) uses a similar procedure to both contain and learn about spillover effects. Some clusters are chosen for treatment, but some units within those clusters are not treated. Units that are untreated in treated clusters can be compared with units that are untreated in untreated clusters in order to suss out intra-cluster spillover effects (Sinclair, McConnell, and Green 2012). The figure shows how the saturation design comes in simple, complete, and blocked varieties.\n\n\nFigure 8.4: Nine kinds of random assignment. In the first row individuals are the units of assignment, in the second row clusters are randomly assigned, in the third clusters are randomly assigned a saturation level and then individuals within these clusters are randomly assigned. In the first column units are assigned independently, in the second units are assigned to hit a target, in the third units are assigned to hit targets within strata.\n\n\n\n8.1.2.2 Multi-arm and factorial trials\nThus far we have considered assignment strategies that allocate subjects to just two conditions: either treatment or control. All generalize quite nicely to multi-arm trials. Trials that have three, four, or many more arms can of course be simple, complete, blocked, clustered, or feature variable saturation, but we show the complete random assignment versions of multi-arm and factorial assignment here.\n\n(#tab:multiarmassignmenttypes) Examples of multi-arm random assignment\n\n\n\n\n\nDesign\nDescription and randomizr R code\n\n\n\nMulti-arm random assignment (complete)\n\nFixed numbers of units are assigned to three or more conditions\ncomplete_ra(N = 100, m_each = c(40, 30, 30))\n\n\n\nFactorial random assignment (complete)\n\nUnits are assigned to receive one treatment, the second treatment, neither, or both\nZ1 = complete_ra(N = 100, m = 50)\nZ2 = block_ra(blocks = Z1, block_m = 25)\nEquivalently, we could write:\nZ = complete_ra(N = 100, m_each = c(25, 25, 25, 25))\nZ1 = Z %in% c(\"T1\", \"T2\")\nZ2 = Z %in% c(\"T1\", \"T3\")\n\n\n\n\n?fig-multiarmquilt shows blocked versions of a three-arm trial, a factorial trial, and a four-arm trial.\nIn the three-arm trial on the left, subjects can be assigned to a control condition or one of two treatments. This design enables three comparisons: a comparison of each treatment to the control condition, but also a comparison of the two treatment conditions to each other. In the four-arm trial on the right, subjects can be assigned to a control condition or one of three treatments. This design supports six comparisons: each of the treatments to control, and all three of the pairwise comparisons across treatments.\nThe two-by-two factorial design in the center panel shares similarities with both the three-arm and the four-arm trials. Like the three-arm, it considers two treatments T1 and T2, but it also includes a fourth condition in which both treatments are applied. Factorial designs can be analyzed like a four-arm trial, but the structure of the design also enables further analyses. In particular, the factorial structure allows researchers to investigate whether the effects of one treatment depend on the level of the other treatment.\n\n\nFigure 8.5: Multi-arm random assignment.\n\n\n\n8.1.2.3 Over-time designs\nTreatment conditions can also be randomized over multiple time periods, with each unit receiving different treatment conditions in different periods. By focusing on variation in outcomes within units rather than across them, these designs can be more efficient than designs that compare across units. Often there is more variation across units than within the same units over time. However, there can be a tradeoff in the form of increased bias. Within-unit comparisons must rely on strong stability assumptions such as “no carry-over effects” of the treatment condition assigned in the preceding period. If which condition the unit is assigned to affects outcomes in later periods, we cannot isolate the effect of treatment just by considering the treatment it was assigned this period, we need to know the entire treatment history.\nA stepped-wedge random assignment procedure involves assigning a subset of units to treatment in the first period, a subset of those who were not treated in the first in the second period, and so on. In the final period, all units are treated. In this design, once you are treated in a period you are treated in all subsequent periods. For example, once you receive information in a treatment about how to vote, you already have that information in later periods. In Figure 8.6, we illustrate a three-period step-wedge design, in which one third of units are assigned in the first period, a second third are treated in the second period, and the remainder in the third and final period. In such a design, we can make two comparisons: the treatment versus control contrast in each period, and the within-units over-time contrast before and after treatment. By combining these two comparisons, we have a more efficient estimate of the average treatment effect than if we had randomly assigned one half of units to treatment and the other half to control in a single period. However, we must invoke a no carry-over assumption that in the second and third period potential outcomes are only a function of the current treatment status not whether (or not) the unit was treated earlier.\n\n\nFigure 8.6: Step-wedge random assignment.\n\n\nCrossover designs are a second common over-time random assignment procedure, in which units are first assigned one condition and then, in a second period, the opposite condition. Such a design is appropriate when units, once treated, do not retain their treatment over time. Crossover designs must also rely on an assumption of no carry-over. If this assumption is valid, the design is highly efficient: instead of having half treated and half control in a single period, all units receive treatment in one period and control in the other so we can make comparisons within each period across units with different conditions and for all units over time before and after treatment. Whether the crucial no carry-over assumptions holds is fundamentally not testable: it is an excludability assumption about the unobservable potential outcomes. The assumption may be bolstered by “washout period” between measurement waves, like buffer rows between crops in agricultural experiments.\n\n8.1.2.4 Data-adaptive assignment strategies\nWe usually think of data strategies as static: a survey asks a fixed set of questions, a randomization protocol has a fixed probability of assignment, sampling designs are designed to yield a fixed number of subjects. But they can also be dynamic. For example, the GRE standardized test many graduate students take is data-adaptive: if you answer the easy questions right, they skip you to harder ones. This process uses fewer questions to figure out test-takers’ scores, saving everyone the laborious effort of taking and grading long examinations (see (data-adaptive-measurement?) for more on data-adaptive measurement).\nData-adaptive designs are also used when the space of possible treatments to choose from is large. We could conduct a static multi-arm trial to evaluate all of them, but experiments with too many conditions tend to have low precision because the sample is spread too thinly across conditions. The usual response to this cost problem is to turn to theory to consider which treatments are most likely to work and test those options only.\n“Response-adaptive” designs are an alternative that may be appropriate in these settings. The subject pool is split into sequential “batches” subjects. The first batch does the experiment, then the second, and so on. The probabilities of assignment to each condition (or arm) starts out equal, but we tweak them between batches. We assign a higher fraction of the second batch to conditions that performed well in the first batch. This process continues until the sample pool is exhausted. Many algorithms for deciding how to update between batches are available, but the most common (Thompson sampling) estimates the probability that each arm is the best arm, then randomly allocates subjects to arms using these probabilities. See Offer-Westort, Coppock, and Green (2021) for a recent introduction to this algorithm and elaborations.\n\n8.1.2.5 Non-randomized assignment\nStrong causal inferences can be drawn from treatment allocation strategies that do not involve random assignment. We outline four such strategies below, with their costs and benefits.\nA commonly considered strategy is alternating assignment, in which every other participant who arrives is assigned to treatment. The procedure would be identical to block random assignment — blocked on time of treatment — if participants arrived in a randomized order. It is appealing for this similarity, but it is often impossible to demonstrate that order was randomized. In fact, participants who work at different times of day may arrive at different times, and many other correlations between individual characteristics and order may arise. But the real problem comes when there are correlations between those characteristics and the order within each couple of participants. For example, if treatment status is correlated with who goes through the door first, there could be a very strong correlation between individual characteristics and treatment condition. A simple fix for this would be to block units into pairs or quartets as they arrive, then randomize within each block, rather than alternating.\nWhen participants can be assigned a score that represents need, desire, or eligibility for a treatment, with higher score representing higher likelihood of treatment, a common design is to set a cutoff score above which all units are treated and below which none are. With such a cutoff, units very near the cut-off may be very similar to each other, so a regression discontinuity design can be used to estimate the treatment effect by predicting the outcome under control (just below the cutoff) and the outcome under treatment (just above the cutoff). In such a design, the assignment of treatment is deterministic and has no random component.\nA range of strategies aim to improve upon random assignment by identifying assignments that are optimal in some sense. Bayesian optimal assignment strategies identify individually-optimal assignments from a set of multiple treatments, based on past data from experiments and individual characteristics that predict treatment effectiveness. Indeed, from a Bayesian perspective randomization is an usual choice of procedure for assigning treatment because it suggests that you expect the same learning will emerge from all assignments that you might select via randomization (see Kasy (2016) and Bai (2021) for an alternative motivation for randomization in this setting). Diagnosing the properties of these so-called optimal designs is crucial, because though a treatment assignment may be optimal in terms of the likelihood that each individual receives the treatment most effective for them, the design may be inefficient due to highly variable assignment propensities and even some units with zero probability of receiving one of the treatments. Such choices may be appropriate, but in a diagnosis researchers can assess sensitivity to priors and directly tradeoff design criteria like efficiency with the average expected effectiveness of the treatment assigned to units.\n\n8.1.3 Measurement\nMeasurement is the part of the data strategy in which variables are collected about the population of units to enable sampling, variables are collected about the sample before treatment assignment including those used in treatment assignment, and outcomes are collected after treatment assignment. All variables used in the answer strategy are collected in measurement, aside from the treatment assignment variable and assignment and sample inclusion probabilities.\nDescriptive inference is threatened whenever measurements differ from that which they are meant to measure. For example when we want to measure “latent variables” such as fear, support for a political candidate, or economic well-being, we use a measurement technology to imperfectly observe them. We represent that measurement technology as the function \\(Q\\) that yields the observed outcome \\(Y^{\\mathrm obs}\\): \\(Q(Y^*) = Y^{\\mathrm obs}\\). Our measurement strategy is a set of functions \\(Q\\) for each variable we measure.\nSome measurement strategies exhibit little to no measurement error. It’s easy enough to measure some plain matters of fact, like whether a country is a member of the European Union (though clerical errors could still crop up). In the social sciences, most measurement strategies are threatened by the possibility of measurement errors due to any number of biases (e.g., recall bias, observer bias, hawthorn effects, demand effects, sensitivity bias, response substitution, among many others).\nWe often describe measurement error in two ways, measurement validity, and measurement reliability. Validity is the difference between the observed and latent outcome, \\(Y^{\\mathrm obs} - Y^*\\). Reliability is the variance of the measurements we would obtain if we were to repeat the measurement many times:, \\(\\V(Y_1^{\\mathrm obs}, Y_2^{\\mathrm obs}, \\ldots, Y_k^{\\mathrm obs})\\). We would of course like to always select valid, reliable measurement strategies. When no perfect measure is available, choices among alternative measurement strategies typically reduce to tradeoffs between their validity and reliability.\nTo make these choices, we depend on methodological research whose main inquiries are the reliability and validity of particular measurement procedures. Sometimes measurement studies are presented as “validation” studies that compare a proposed measure to a “ground truth.” But even “ground truths” must be measured, usually with an expensive or otherwise unfeasible approach (otherwise they would be no need for the alternative measurement). Further, neither measurement is known to be exactly \\(Y_i^*\\), so ultimately validation studies are comparisons of multiple techniques each with their own advantages and disadvantages. This fact does not make these studies useless, but rather underlines that they rely on our faith in ground truths.\nResearchers select several characteristics of a measurement strategy: who collects the measures, the mode of measurement, how often and when measures are taken, how many different observed measures of the latent outcome \\(Y^*\\) are collected, how they are summarized into a single measure. These design characteristics may affect validity, reliability, cost, or all three.\nData may be collected by researchers themselves, by participants, or by third parties. In some forms of qualitative research such as participant-observation and interview-based research, the researcher may be the primary data collector. In survey research, interviewers are typically a hired agents of the researcher, each of whom may ask questions differently. Participants are sometimes asked to collect data on themselves, either through self-administered surveys, journaling, or taking measurements of themselves using thermometers or scales. A primary concern with self-reports is validity: do respondents report their measurements truthfully? A parallel concern is raised when participants do not collect their own data but are made aware of the fact that they are being measured by others. Finally, data may be collected by agents of government or other organizations, yielding so-called “administrative” data.\nMost of the variety in measurement strategies is how those data collectors obtain their data. Data collectors can use observation and ask respondents for self-reports. Increasingly, photos, videos, sound recordings, and even water and soil measurements are used for outcome measurement. The translation of raw data, like videos, into coded data, like counts of the number of police stops, that can be used for analysis is part of \\(Q\\) in the measurement strategy.\n\n8.1.3.1 Multiple measures\nWe measure the latent outcome \\(Y_i^*\\) imperfectly with any single measure. In many cases, we have access to multiple imperfect measures of the same \\(Y_i^*\\). When possible, collecting all of these different measures and averaging them to construct a single index measure will yield efficiency improvements. The average measure can borrow the different strengths of the different measures. When the tools produce answers that are highly correlated, taking multiple measures is unlikely to be worth the cost because the same information is simply duplicated, but when the correlation is low, it will be worth taking multiple measurements and averaging to improve efficiency. Pilot studies may be usefully tasked with measuring the correlation between items. Index measures are distinct from \\(Y_i^*\\) outcomes that have multiple dimensions and must be measured with multiple items, one per dimension. In these cases, we have a single measure of \\(Y_i^*\\) just constructed in a more complex way.\n\n8.1.3.2 Over-time measurement\nData need not be collected at a single time period. The model encodes beliefs about the autocorrelation (correlation over time) of outcomes, and this can help guide whether to collect multiple measurements or just one. If data are expected to be highly variable (low autocorrelation), then taking multiple measurements and averaging them may provide efficiency gains.\nWhen outcomes exhibit high autocorrelation, there will be large precision gains from collecting a baseline measure before a treatment in an experiment. When outcomes exhibit lower autocorrelation, baseline measurements may not be worth the cost.\n\n8.1.3.3 Data-adaptive measurement\nJust as we can use data-adaptive methods to hone in on the most effective treatments (Section (data-adaptive-assignment-strategies?)), we can use adaptive measurement techniques to hone in on the most useful measures. Adaptive inventory techniques enable deploying long batteries of survey items, for example, but enumerating the shortest set of items to any given respondent that results in a definitive measurement of \\(Y_i^*\\). In the same way as many modern standardized tests condition the choice of survey items on students past answers in order to hone in quickly on the correct test score, adaptive inventories ask questions that will be maximally informative. The logic is the same as that of using multiple different measures for the same construct: the lower the correlation, or in other words the more new information, between two items the more informative they are. Adaptive inventories select a set of items to enumerate that provide the most uncorrelated information. See Montgomery and Rossiter (2020) for an up-to-date treatment of the adaptive measurement possibilities for constructs measured by long survey batteries."
  },
  {
    "objectID": "declaration-diagnosis-redesign/crafting-research-strategy.html#how-to-craft-data-strategies",
    "href": "declaration-diagnosis-redesign/crafting-research-strategy.html#how-to-craft-data-strategies",
    "title": "\n8  Crafting a data strategy\n",
    "section": "\n8.2 How to craft data strategies",
    "text": "8.2 How to craft data strategies\n\n\n\n\nPrinciple 3.2: Design agnostically encourages us to consider plausible variations of the set of variables, their probability distributions, and the relationships between them. The payoff of doing so comes in selecting the data and answer strategies, in particular choosing D and A such that they are good designs under a wide array of plausible models.\nIn this section, we discuss how to select empirical strategies that are robust to multiple models by focusing on the data strategy. We identify four core threats to data strategies: noncompliance (failure to treat), attrition (failure to be included in the sample or provide measures), and excludability violations (causal effects of random sampling, random assignment, or measurement on the latent outcome). If serious, these threats may necessitate a changes to the inquiry, the answer strategy, or the data strategy itself.\nBelow, we adapt Figure 8.1 presented in the chapter’s introduction to introduce each of these threats and discuss each threat in turn.\n\n\nFigure 8.7: Directed acyclic graph of random sampling, random assignment, and measurement with exclusion restriction violations as dotted lines.\n\n\n\n8.2.1 Noncompliance\nNoncompliance occurs when the assignment variable \\(Z\\) imperfectly manipulates the treatment variable \\(D\\). When noncompliance is not a problem, \\(D_i = Z_i\\), but in design that encounter noncompliance \\(D_i \\neq Z_i\\). One-sided noncompliance occurs when some treated units fail to be treated (and receive the control condition instead). Two-sided noncompliance occurs when some units assigned to treatment do not take treatment and some units assigned to control do take treatment. Noncompliance hampers experimental studies but also affects observational designs for causal inference in which nature or a non-random administrative process affects treatment such as a threshold cut-off, but only imperfectly.\nIn the presence of noncompliance, a change in inquiry is sometimes unavoidable. The average difference between those assigned to treatment and those assigned to control no longer targets the average treatment effect, but instead only the effect of assignment to treatment. We instead call this inquiry the intent-to-treat effect, and we can estimate it well by comparing the groups as assigned. An alternative inquiry is the complier average treatment effect, which is “local” to the subset of units that comply with treatment (take it when offered). See Section (encouragement-designs?) for a discussion of noncompliance in randomized experiments and Section (instrumental-variables?) for related discussion of “noncompliance” in observational studies.\n\n8.2.2 Attrition\nAttrition occurs when we do not obtain outcome measures for all sampled units. Two types of missing data may result: when a single measure is missing (known as item nonresponse) and when all measures are missing for a participant (known as survey nonresponse). Though these terms were coined by survey researchers, analogous problems can affect non-survey measurement strategies, like missing administrative data, for example.\nWhether attrition is a problem depends on whether response (\\(R\\)) is causally affected by variables other than random sampling. If it is not, we say the missingness completely at random, just as if we had simply added one more random sampling step to the design. Outside of explicit sampling designs, missingness completely at random is rare, though possible, perhaps due to idiosyncratic administrative procedures or computer error. If attrition is completely at random, precision suffers due to a loss of sample size but bias is unaffected.\nIf missingness is affected by other variables – some units are more likely to response because of unobserved background characteristics such as being at home when the survey taker calls – then inferences may be biased. Attrition is doubly difficult in experiments, because if treatment affects not just how a unit responds, but whether it responds, then treatment-control comparisons on the basis of observed data may be biased.\nA bounding approach like the one described in Section (interval-estimation?) is a design-based answer strategy for drawing inferences despite missingness. Section (what-can-go-wrong?) describes a design-based data strategy for avoiding the problem in the first place. Model-based approaches involve reweighting the data by stratum, supposing random missingness within stratum but not across.\n\n8.2.3 Excludability\nExcludability means that when we define potential outcomes, we can exclude extraneous, nontreatment variables from the potential outcomes functions. When we define the treated potential outcome for the latent outcome as \\(Y_i^*(D_i = 0)\\), we invoke (at least) three important excludability assumptions: no effect of sampling \\(S\\), no effect of treatment assignment \\(Z\\) (except through treatment \\(D\\)!), and no effect of measurement \\(Q\\) on the latent outcome \\(Y_i^*\\). If we did not invoke these assumptions, we would have to define the potential outcome function as \\(Y_i^*(D_i, S_i, Z_i, Q_i)\\). When we do invoke the assumptions, we can write plain \\(Y_i^*(D_i)\\). The three assumptions are represented as gray dotted lines in Figure 8.7.\nThe figure asserts no effect of sampling \\(S\\) on latent outcome \\(Y_i^*\\). This assumption could be violated if the fact of being included in the sample changes your attitudes. For exampling, if the very act of being asked to be in a focus group causes subjects to reflect on their political beliefs and thereby change them, the sampling excludability assumption would be violated.\nNext, we assert no causal effect of assignment \\(Z\\) on outcome \\(Y^*\\) – except through the treatment \\(D\\). This assumption is constantly under threat! In observational studies “instrumental variables” design, excludability is the assumption of no alternative channels through which the instrument affects outcomes except the treatment variable. In the entertainingly-titled “Rain, Rain, Go Away: 176 potential exclusion-restriction violations for studies using weather as an instrumental variable,” Mellon (2021) discusses how random variation in rainfall has been misused to study the effects of other treatments.\nWe further assume that \\(Q\\) does not affect \\(Y^*\\). Hawthorne effects, in which the fact of being measured changes outcomes, are an example a violation of this kind excludability assumption. If outcomes depend on whether subjects know they are being measured or do not, then we cannot exclude the effect of measurement from our effect estimates.\nThe DAG also encodes the assumption that \\(Z\\) has no effect on \\(Q\\). How and whether we measure outcomes should not depend on whether a unit is assigned to treatment. This excludability assumption is commonly referred to as the requirement that measurement be parallel across treatment conditions. If we measure outcomes using a face-to-face survey in the treatment group and an mail-back survey in control, then we cannot separate (exclude!) the effect of measurement from the effect of treatment.\n\n8.2.4 Interference\nWe have four endogenous outcomes in the DAG of a research design above: \\(R\\), whether a participant responds to data collection; \\(D\\), whether a respondent receives treatment; \\(Y^*\\), the latent outcome; and \\(Y\\), the observed outcome. Setting aside attrition and noncompliance for the moment, \\(R\\) is a function only of sampling; \\(D\\) of treatment assignment; \\(Y^*\\) of \\(D\\); and \\(Y\\) of measurement strategy \\(Q\\).\nInterference occurs when these endogenous variables depend not only on whether and how individual units are sampled, assigned to treatment, and measured, but whether and how other units are sampled, assigned to treatment, and measured. We usually assume, for example, that \\(Y_i(Z_i) = Y_i(Z_i, \\mathbf{Z}_{-i})\\). In other words, \\(Y_i\\) the outcome for unit \\(i\\), is a function of its own treatment assignment status \\(Z_i\\) not those of other units (\\(\\mathbf{Z}_{-i}\\)).\nWe often think of interference when considering how treatments spill from treated to untreated units. But interference can also be induced by sampling: potential outcomes might depend on whether other units are included in the sample. Or by measurement: Measurement interference occurs when \\(Y_i^*\\) depends on whether and how other units (or outcomes) are measured. For example, asking about one attitude might affect how subjects respond to a second question.\nWe discuss the some complications of interference in experiments in Sections (randomized-saturation-experiments?) and (experiments-over-networks?)."
  },
  {
    "objectID": "declaration-diagnosis-redesign/crafting-research-strategy.html#summary",
    "href": "declaration-diagnosis-redesign/crafting-research-strategy.html#summary",
    "title": "\n8  Crafting a data strategy\n",
    "section": "\n8.3 Summary",
    "text": "8.3 Summary\nData strategies are made of up three kinds of empirical strategies: sampling strategies, assignment strategies, and measurement strategies. All research designs have a data strategy – even just downloading a dataset curated by others constitutes a data strategy (it’s theirs). The blizzard of choices enumerated in this chapter underlines the central importance of the data strategy in developing strong research designs. The data strategy is where we exert researcher control over how, under what conditions, and from whom we collect the empirical information we will use when generating answers to our research questions.\n\n\n\n\nBai, Yuehao. 2021. “Why Randomize? Minimax Optimality Under Permutation Invariance.” Journal of Econometrics.\n\n\nCollier, David, David A Freedman, James D Fearon, David D Laitin, John Gerring, and Gary Goertz. 2008. “Symposium: Case Selection, Case Studies, and Causal Inference.” Qualitative & Multimethod Research 6 (2): 2–16.\n\n\nFearon, James, and David Laitin. 2008. “Integrating Qualitative and Quantitative Methods.” In Oxford Handbook of Political Methodology, edited by Janet M. Box-Steffenmeier, David Collier, and Henry E Brady, 756–76. London, England: Oxford University Press.\n\n\nGeddes, Barbara. 2003. Paradigms and Sand Castles: Theory Building and Research Design in Comparative Politics. Ann Arbor, Michigan: University of Michigan Press.\n\n\nGerring, John, and Lee Cojocaru. 2016. “Selecting Cases for Intensive Analysis: A Diversity of Goals and Methods.” Sociological Methods & Research 45 (3): 392–423.\n\n\nGoertz, Gary. 2008. “Choosing Cases for Case Studies: A Qualitative Logic.” Newsletter of the APSA Section on Qualitative & Multi-Method Research 6 (2): 11–14.\n\n\nHerron, Michael C., and Kevin M. Quinn. 2016. “A Careful Look at Modern Case Selection Methods.” Sociological Methods & Research 45 (3): 458–92.\n\n\nHumphreys, Macartan, and Alan M. Jacobs. 2015. “Mixing Methods: A Bayesian Approach.” American Political Science Review 109 (4): 653–73.\n\n\nKasy, Maximilian. 2016. “Why Experimenters Might Not Always Want to Randomize, and What They Could Do Instead.” Political Analysis 24 (3): 324–38.\n\n\nLevy, Jack S. 2008. “Case Studies: Types, Designs, and Logics of Inference.” Conflict Management and Peace Science 25 (1): 1–18.\n\n\nLieberman, Evan S. 2005. “Nested Analysis as a Mixed-Method Strategy for Comparative Research.” American Political Science Review 99 (3): 435–52.\n\n\nMartin, Lisa. 1992. Coercive Cooperation: Explaining Multilateral Economic Sanctions. Princeton: Princeton University Press.\n\n\nMellon, Jonathan. 2021. “Rain, Rain, Go Away: 176 Potential Exclusion-Restriction Violations for Studies Using Weather as an Instrumental Variable.” Unpublished Manuscript.\n\n\nMill, John Stuart. 1869. A System of Logic, Ratiocinative and Inductive: Being a Connected View of the Principles of Evidence and the Methods of Scientific Investigation. New York: Harper & Brothers.\n\n\nMontgomery, Jacob M., and Erin L. Rossiter. 2020. “So Many Questions, so Little Time: Integrating Adaptive Inventories into Public Opinion Research.” Journal of Survey Statistics and Methodology 8 (4): 667–90.\n\n\nOffer-Westort, Molly, Alexander Coppock, and Donald P. Green. 2021. “Adaptive Experimental Design: Prospects and Applications in Political Science.” American Journal of Political Science 65: 826–44.\n\n\nPlümper, Thomas, Vera E. Troeger, and Eric Neumayer. 2019. “Case Selection and Causal Inference in Qualitative Research.” PloS One 14 (7): 1–18.\n\n\nSeawright, Jason, and John Gerring. 2008. “Case Selection Techniques in Case Study Research: A Menu of Qualitative and Quantitative Options.” Political Research Quarterly 61 (2): 294–308.\n\n\nSinclair, Betsy, Margaret McConnell, and Donald P. Green. 2012. “Detecting Spillover Effects: Design and Analysis of Multilevel Experiments.” American Journal of Political Science 56 (4): 1055–69.\n\n\nSkocpol, Theda. 1979. States and Social Revolutions: A Comparative Analysis of France, Russia and China. Cambridge, UK: Cambridge University Press.\n\n\nSwank, Duane. 2002. Global Capital, Political Institutions, and Policy Change in Developed Welfare States. New York: Cambridge University Press."
  },
  {
    "objectID": "declaration-diagnosis-redesign/choosing-answer-strategy.html",
    "href": "declaration-diagnosis-redesign/choosing-answer-strategy.html",
    "title": "9  Choosing an answer strategy",
    "section": "",
    "text": "The answer strategy is a plan for what to do with the information gathered from the world in order to generate an answer to the inquiry. Qualitative and quantitative methods courses provide guidance about the properties of different strategies and the conditions under which they work well or poorly. Under what conditions should we use ordinary least squares, when should we use logit? When is a machine learning algorithm the appropriate choice and when would a comparative case study be more informative? When is no answer strategy worth pursuing because of the fundamental limitations of the data strategy?\nFollowing Principle 3.3: Design for purpose, the evaluation of an answer strategy depends on our ultimate goals: what is the answer to be used for? A perfect answer is generally elusive in empirical research so in practice we often need to select among strategies that come with different strengths and weaknesses. For instance some might suffer less from bias while others might be more precise. In other words, which answer strategy is best for you depends on what diagnosands you care about.\nThis chapter first describes the elements of the answer strategy, the most important of which are the type of answer and the approach to assessing the level of uncertainty in the answer. We then describe four distinct approaches to answering a question: point estimation, hypothesis tests, Bayesian posteriors, and interval estimation. Last we identify some general principles for selecting an answer strategy highlighting especially how the choice of A depends on the other three elements of research design. Principle 3.1 is a reminder to diagnose holistically: we can’t choose answer strategies in isolation from the the other design elements."
  },
  {
    "objectID": "declaration-diagnosis-redesign/choosing-answer-strategy.html#elements-of-answer-strategies",
    "href": "declaration-diagnosis-redesign/choosing-answer-strategy.html#elements-of-answer-strategies",
    "title": "9  Choosing an answer strategy",
    "section": "\n9.1 Elements of answer strategies",
    "text": "9.1 Elements of answer strategies\nThe three core elements of an answer strategy are the identification of a type of answer, the strategy for conceptualizing and reporting uncertainty about the answer, and a procedure for obtaining both.\n\n9.1.1 Answer characterization\nAt its most basic an answer strategy delivers a guess at the value of an estimand. The answer itself, like the inquiry, generally requires a specification of units, outcomes, and conditions. Like the inquiry, it requires a domain.\nDomain: We often think of the answer as a number: 55% or an effect of 0.25. But the domain of the answer can be much broader: it could be a logical statement, TRUE or FALSE; a vector of predictions; a statement “This theory is helpful”; even a model. The domain is likely matched with the domain of the estimand, but it might not be. For instance the estimand might be 5 and the answer an interval \\([3, 6]\\). The rubber hits the road when a diagnosand has to establish the usefulness of an answer; the primary question is whether the usefulness of an answer can be assessed or not.\nUnits: The units that serve as input to the answer strategy are, likely, either the same as those in the inquiry or good stand-ins. How good is determined by choices in the data strategy: were study units drawn in a random sample from the population? Are some subgroups excluded from the sampling procedure, because they are hard-to-reach? In some cases, the sampling procedure will be complex and some units will stand in for more than one unit in the population. In this case, the answer strategy should take account of this fact. In some cases the data is measured using units that are not defined at the same “level” as the units that define the inquiry. For instance you might be interested in women’s voting but have data on polling station level outcomes only. This generates what is called a challenge of ecological inference and your answer strategy should address this.\nOutcomes: Answer strategies summarize outcomes that represent measured characteristics of each unit. The outcomes must be measured in the data strategy, and should usually match closely the outcomes defined in the inquiry. However, we always have imperfect measures of outcomes; how good is determined by the data strategy. Answer strategies then often involve multiple measured outcomes to best represent an unobserved outcome such as an attitude. The measures might be analyzed separately and interpreted together or formally combined using an indexing method.\nConditions: Inquiries define the treatment conditions over which outcomes are compared. Sometimes outcomes from than one treatment condition are compared, in the case of causal inquiries, whereas only one is used in the case of descriptive inquiries. The data strategy then determines which treatment conditions are assigned to which units, and thus linking units’ outcomes to the potential outcomes used in the inquiries. This linking occurs in the answer strategy. Just as sampled units will be analyzed to stand in for units in the population, units assigned to a control group often stand in for the control potential outcome for all units (and the same for treated units). Just as with sampling weights, assignment weights may be used to allow some units to stand in for more than one (or fewer than one) unit in the inquiry when units are assigned to treatments with different probabilities. For descriptive inquiries, all units may be used to stand in for the naturally-assigned potential outcome in the inquiry.\n\n9.1.2 Uncertainty\nMuch empirical work involves inference: making guesses about quantities that we cannot directly observe. Sometimes the challenge is descriptive inference, sometimes causal inference, sometimes generalization, and oftentimes all three at once.\nIn general when we are doing inference our answers are uncertain and we need to find ways to communicate that uncertainty. Two prominent and clearly distinct perspectives on estimating uncertainty are the Bayesian approach and the frequentist approach.\nBayesian uncertainty. The simplest way of thinking about uncertainty about inferences that arise from data is nicely described by Bayes’ rule.\nThe probability of a quantity of interest \\(\\theta\\) is given by:\n\\[\\Pr(\\theta = \\theta'|d = d') = \\frac{\\Pr(d = d'|\\theta = \\theta')\\Pr(\\theta = \\theta')}{\\sum_{\\theta''}\\Pr(d = d'|\\theta = \\theta'')\\Pr(\\theta = \\theta'')}\\]\nwhere \\(\\theta'\\) and \\(\\theta''\\) represent particular values of \\(\\theta\\), \\(d\\) represents data, and \\(d'\\) a particular realization of the data.\nUsing MIDA notation, we might think that a quantity of interest \\(a_{m^*}\\) could take on a range of possible values \\(a \\in A\\). Given M and D, we can assess the probability of a particular data realization, d, given any particular value for \\(a\\). For instance if \\(a_{m^*}\\) is the share of women in a very large population and you sample \\(m\\) individuals of which \\(d\\) are women, then \\(\\Pr(d = d'|a = a') = {m \\choose d} a^d(1-a)^{m-d}\\). We can then use Bayes rule to calculate \\(\\Pr(a_{m^*} = a'|d = d')\\).\nApplying the rule over different values of \\(\\theta\\) we build up a full probability distribution over possible answers. The probability distribution simultaneously represents our answer and our certainty in the answer. For instance we might report the mean of the distribution (“posterior mean”) as our best guess and the variance as our uncertainty (“posterior variance”). Or we might just report the whole posterior distribution as an answer.\nWhile this approach is intuitive, many are uncomfortable with it. One reason is that the method requires a specification of prior uncertainty \\(\\Pr(\\theta = \\theta')\\). A second reason is philosophical. If we think that the estimand has some particular value then what does it mean to say something like \\(\\Pr(a_{m^*} = a) = 0.5\\)? Surely either \\(a_{m^*} = a\\) or \\(a_{m^*} \\neq a\\). The Bayesian response is that the probability does not refer to a physical probability but to “degrees of belief”: essentially a measure of how confident you are in the claim.\nFrequentist uncertainty. Say you wanted to make a statement about your uncertainty about an answer but did not want to specify prior beliefs about what the answer is. Instead, you want any statements about probability to come from physical processes—actual randomization for instance. Can you do it?\nThe short answer is no. You can’t escape Bayes’ rule if you want to make a claim about the probability that some answer is correct given the data. However, you can do something related.\nLeaving \\(\\Pr(\\theta = \\theta')\\) aside you can pick out one element of Bayes’ rule from above and report the simpler quantity:\n\\[\\Pr(d = d'|\\theta = \\theta')\\]\nIn other words: how likely is it that we would see data like this if indeed \\(\\theta\\) were \\(\\theta'\\). You can answer this without thinking of probability as representing strengths of beliefs but working instead from the idea that \\(\\theta\\) generates an actual probability distribution over possible data, \\(d\\). And of course you can do this for many different possible values of \\(\\theta\\).\nWhen you go this route you can get a number that you can defend (as Fisher put it: “a reasoned basis for inference”). The basic idea gives rise to a set of useful tools:\n\nThe \\(p\\) value for a null hypothesis \\(\\theta_0\\) corresponds exactly to \\(\\Pr(d = d_{m*}|\\theta = \\theta_0)\\).\nThe maximum likelihood approach to estimation corresponds to finding the value \\(\\theta'\\) for which \\(\\Pr(d = d'|\\theta = \\theta')\\) is greatest.\nThe 95% confidence interval is interpretable as the set of values for which \\(\\Pr(d = d'|\\theta = \\theta') \\leq 0.05\\).\n\nIn short \\(\\Pr(d = d'|\\theta = \\theta')\\) is a powerful quantity and the frequentist approach that uses it is currently the dominant approach in social sciences, and the most commonly used approach in this book also. But it is worth being very clear on what this quantity does and does not do. We seek estimates of uncertainty, but this quantity does not provide a statement about your confidence in your answer. Rather it provides a statement about the consistency between possible answers and the data you have. It lets you say that you are certain in your answer to the extent that the world is not as we would expect it to be if other answers were correct. For this reason we often think of it as an approach to ruling out possible answers: an answer is ruled out if the patterns we see are out of line with what the answer would predict.\n\n9.1.3 Procedure\nHow the outcomes of study units are analyzed and, if relevant, compared across conditions is the method of the answer strategy. This element is the choice of estimator (e.g., OLS or difference-in-means) but also the regression specification and if-then procedures for model selection.\nThe method should be thought of like a procedure or a function: data goes in, answers come out. The output responds to the inputs. If the events generated by the world had been different, the data produced by the data strategy would be different too. If the data produced by the data strategy had been different, the answers rendered by the answer strategy would be different too. We want to understand the functions as M, I, and D vary.\nCritically, when declaring answer strategies as functions, we have to think about more than just the single estimation function that ends up in the final paper. To see this, consider an estimator that is selected through on an exploratory procedure in which multiple estimators are compared on the basis of fit statistics. The answer strategy is not this final estimator—it is this entire multi-step if-then procedure. The reason to declare the procedure rather than the final estimator is that the diagnosis of the design may differ. The procedure may be more powerful, if for example we assessed multiple sets of covariate controls and selected the specification with the lowest standard error of the estimate. But that procedure would also exhibit poor coverage, since the confidence interval produced by the final estimator does not account for these multiple bites at the apple.\nAnswer strategies can become multi-stage procedures in unexpected ways. For example, sometimes a planned-on maximum likelihood estimator won’t converge when executed on the realized data. In these cases, analysts switch estimators (or sometimes inquiries!). The full set of steps — a decision tree, depending on what is estimable — is the answer strategy we want to declare and compare to alternative decision trees.\nThis principle extends to settings in which analysts run diagnostic tests, like falsification or placebo tests. If we learn from a sensitivity test that a mediation estimate is very sensitive to unobserved confounding, we might choose not to present it at all. By this logic, the answer strategy includes the sensitivity test, the decision made on the basis of the test. When we inspect the resulting distribution of mediation estimates, some are undefined.\nWriting down the full set of if-then choices we might make in the answer strategy depending on revealed data is hard to do. We often imagine answer strategies if things go right but spend less imagination on elaborating what might happen if things go wrong. When things do go wrong — missing data, noncompliance, suspension of the data collection — answer strategies will change. One way to guard against over-correcting to the revealed data is to adopt a standard operating procedures document that systematizes these procedures in advance (Green and Lin 2016)."
  },
  {
    "objectID": "declaration-diagnosis-redesign/choosing-answer-strategy.html#types-of-answer-strategies",
    "href": "declaration-diagnosis-redesign/choosing-answer-strategy.html#types-of-answer-strategies",
    "title": "9  Choosing an answer strategy",
    "section": "\n9.2 Types of answer strategies",
    "text": "9.2 Types of answer strategies\nWe identify four distinct types of answers that might be provided by an answer strategy. In each case we describe how information regarding uncertainty is communicated.\n\n9.2.1 Point estimation\n\n9.2.1.1 Answer\nThe most familiar class of answer strategies are point-estimators that produce estimates of scalar parameters. The sample mean of an outcome, the difference-in-means estimate, the coefficient on a variable in a logistic regression, and the estimated number of topics in a text corpus are all examples of point estimators.\nTo illustrate point estimation in general, we’ll try to estimate the average age of the citizens of a small village in Italy. Our model is straightforward – the citizens of the small village all have ages – and the inquiry is the average of them. In our data strategy, we randomly sample three citizens whose ages are then measured via survey to be 5, 15, and 25. Our answer strategy is the sample mean estimator, so our estimate of the population average age is a point estimate of 15.\n\n9.2.1.2 Uncertainty\nA standard way to report uncertainty of a point estimate is to provide a “standard error.” In this case we know that our answer is probably not a good answer. It is almost certainly wrong in the sense that the population average age in the small village is probably not fifteen (Italy’s population is aging!), but we don’t know how wrong because, of course, we don’t actually know the value of the inquiry under study. We have instead to evaluate the properties of the procedure. Under a random sampling design – even an egregiously stingy random sampling design that only selects three citizens! – we can justify the approach on the basis of the “bias” diagnosand.1\nBut that doesn’t tell us much about how confident we should be in this answer. The design in Declaration 9.1 can be used to generate a view of what answers we might get when we choose just three subjects for our sample.2\n\nDeclaration 9.1 Italian village design\n\ndeclaration_9.1 <-\n  declare_model(N = 100, age = sample(0:80, size = N, replace = TRUE)) +\n  declare_inquiry(mean_age = mean(age)) +\n  declare_sampling(S = complete_rs(N = N, n = 3)) +\n  declare_estimator(age ~ 1, .method = lm_robust) \n\n\\(~\\)\n\nWe now diagnose the design by simulating this design repeatedly, plotting the sampling distribution along with the true population mean age, and calculating the bias.\n\n\nDiagnosis 9.1 (Italian village design diagnosis) \ndiagnosis_9.1 <- diagnose_design(declaration_9.1)\n\n\n\n\nFigure 9.1 shows that we are right on average but usually quite wrong. The average estimate lies right on top of the true value of the estimand (40), but the estimates range enormously widely, from close to zero to close to 80 in some draws. The answer strategy – the sample mean estimator – is just fine, the problem here lies in the data strategy that generates tiny samples. Substantively this does not mean that we now believe we are wrong. But it does tell us that the data is just as consistent with lots of other possibilities as it is with the estimate that we got from a single run of the design.\n\n\nFigure 9.1: Sampling distribution of the estimates from the point estimator answer strategy\n\n\n\n\n9.2.2 Hypothesis Tests\n\n9.2.2.1 Answer\nTests are an elemental kind of answer strategy. Tests yield binary yes/no answers to a binary yes/no inquiry. In some qualitative traditions, hoop tests, straw-in-the-wind tests, smoking-gun tests, and doubly-decisive tests are common. These tests are procedures for making analysis decisions in a structured way. Similarly, many forms of quantitative tests have been developed. Sign tests assess whether a test statistic is positive, negative, or zero. Null hypothesis significance tests assess whether a parameter is different than a null value, such as zero. Equivalence tests assess whether a parameter falls within a range, rather than comparing to a fixed value. Many procedures for conducting tests are also available, with different assumptions about the null hypothesis, the distributions of variables, and the data strategy.\nA typical null hypothesis test proceeds by imagining a null model \\(M_{0}\\) and imagining the sampling distribution of the empirical answer \\(a_{d_0}\\) under a hypothetical design M\\(_{0}\\)IDA. That sampling distribution enumerates all the ways the design could have come out if the null model \\(M_{0}\\) were the correct one. For a null hypothesis test, we entertain the null model and consider its implications. We ask, under \\(M_{0}\\) how frequently would we obtain an answer as large or larger than the empirical answer \\(a_d\\) (or other test statistic)? That frequency is known as a \\(p\\)-value.3 The last step of the test is to turn the \\(p\\)-value into a binary decision about statistical significance. The typical threshold in the social sciences is 0.05: hypothesis test with \\(p\\)-values less than 0.05 indicate statistical significance. This threshold is arbitrary, reflecting the inertia of the scientific community much more than some a priori scientific standard. The appropriate threshold value for statistical significance is a matter of furious debate, with some authors calling for the threshold to be lowered to \\(0.005\\) to guard against false positives (Benjamin et al. 2018).\nWe’ll illustrate the idea of a hypothesis test in general with the Italian village example. Here, we test against the hypothesis that the average age is 20. If we have strong evidence against this hypothesis, we will reject it. If we have weak evidence against the hypothesis, we will fail to reject it. For instance, we might reject 10 and 70 but fail to reject 35 and 45.\n\nDeclaration 9.2 Italian village design, continued\n\ndeclaration_9.2 <- \n  declaration_9.1 +\n  declare_test(age ~ 1, \n               linear_hypothesis = \"(Intercept) = 20\", \n               .method = lh_robust, label = \"test\")\n\n\\(~\\)\n\nHere’s one run of that design. The output can be confusing. By default, most statistical software tests against the null hypothesis that the true parameter value is zero – so the p.value in the first row refers to that null hypothesis test. The second row is the test against the hypothesis that the mean is equal to twenty. The “estimate” in the second row is the difference of the observed estimate from 20. The p.value in the second row is the one we care about when testing against the null hypothesis that the average age is 20.\n\nrun_design(declaration_9.2)\n\n\n#> Loading required namespace: car\n\n\n\nEstimates from the test of the mean age equalling 20 from Italian villages example\n \n estimator \n    estimate \n    p.value \n  \n\n\n estimator \n    32 \n    0.02 \n  \n\n test \n    12 \n    0.10 \n  \n\n\n\n\nNext, we diagnose the modified design, by running the design many times. Figure 9.2 shows how frequently we reject the null that the average age is 20. When estimate is close to 20, we rarely reject the null, but when the estimate is far from 20, we are more like to reject. Again, this diagnosis comes from a design with a weak data strategy of sampling only three citizens at time. We need to see estimates breaking 60 before the testing answer strategy reliably rejects this (false) null hypothesis.\n\n\nDiagnosis 9.2 (Italian village diagnosis, continued) \ndiagnosis_9.2 <- diagnose_design(declaration_9.2)\n\n\n\n\n\n\nFigure 9.2: Sampling distribution of the test answer strategy\n\n\n\n\n9.2.2.2 Uncertainty\nClassic hypothesis tests do not provide estimates of uncertainty regarding any associated estimate. They do however let you impose stricter or weaker conditions before rejecting an null hypothesis. If you wanted to be very conservative you could stipulate that you will only reject a hypothesis if the associated \\(p\\) values is less than \\(0.0001\\). In this case you are not likely to reject many null hypotheses close to your estimate but those that you do reject you may feel you reject with greater confidence. Not to belabor the point, it is not that the procedure tells you how sure you should be that the null is wrong, it is just that you figure out that the data is very unusual if the null were right.\n\n9.2.3 Bayesian formalizations\n\n9.2.3.1 Answer\nBayesian answer strategies sometimes target the same inquiries as classical approaches, but rather than seeking a point estimate, they try to generate rational beliefs over possible values of the estimand. Rather than trying to provide a single best guess for the average age in a village, a Bayesian answer would try to figure out how likely different answers are given the data. To do so they need to know how likely different age distributions are before seeing the data—the priors—and the likelihood of different types of data for each possible age distribution. A Bayesian would likely not be very impressed by the 15 answer given by the point estimator in Section (point-estimation?) because, prior to see any samples, they would likely expect that the answer had to be bigger than this. Bayesians would chalk the answer “15” down to an unusual draw.\nThe Bayesian answer strategy specifies a prior distribution (here distributed normal centered on 50 to reflect a prior that Italian villages skew older) as well as a log normal distribution for ages. Here we retain the (median) posterior estimates for average age alongside a standard error based on the posterior variance. In the .summary argument we ask the tidier to exponentiate the coefficient estimate and standard error before returning them.\n\nDeclaration 9.3 Italian village design a la Bayes\n\n\nbase_declaration <-\n  declare_model(N = 100, \n                age = round(rnorm(N, mean = true_mean, sd = 23))) +\n  declare_inquiry(mean_age = mean(age)) +\n  declare_sampling(S = complete_rs(N = N, n = 3)) +\n  declare_estimator(age ~ 1, .method = lm_robust) \n\ndeclaration_9.4 <- redesign(base_declaration, true_mean = seq(0, 100, length.out = 10))\n\n\n\n\nDiagnosis 9.3 (We can then simulate this design in the same way and examine the distribution of estimates we might get.) \ndiagnosis_9.4 <- diagnose_design(declaration_9.4)\n\nWhat we see in Figure 9.3 is that using the same (poor) data strategy as before, a Bayesian answer strategy gets us a somewhat tighter distribution on our answer, but exhibits greater bias: the average estimate is higher than the estimand. We might accept higher bias for lower variance if overall, the root-mean-squared error is lower for the Bayesian approach. See Section (exploring-design-tradeoffs?) for a further discussion of RMSE. The main difference between the Bayesian and classical approaches is the handling of prior beliefs, which carry a lot of weight in the Bayesian estimation but no weight in the classical approach.\n\n\nFigure 9.3: Sampling distribution of the Bayesian answer strategy\n\n\n\nBayesian approaches are also used by qualitative researchers drawing case level inferences from causal process observations. Recent developments in qualitative methods have sought to take Bayes’ rule “from metaphor to analytic tool” (Bennett 2015). This approach characterizes qualitative inference as one in which prior beliefs about the world can be specified numerically and then are updated on the basis of evidence observed. At a minimum, writing down such an answer strategy on a computer requires specifying beliefs, expressed as probabilities, about the likelihood of seeing certain kinds of evidence under different hypotheses. We provide an example of such a strategy the design library Section (process-tracing?). Herron and Quinn (2016) provide one approach to formalizing a qualitative answer strategy that focuses on understanding an average treatment effect. Humphreys and Jacobs (2015) provide an approach that can be used to formalize answer strategies targeting both causal effect and causal attribution inquiries, while Fairfield and Charman (2017) formalize a Bayesian approach that approaches causal attribution as a problem of attaching a posterior probability to competing alternative hypotheses. Abell and Engel (2021) suggest the use of “supra-Bayesian” methods to aggregate multiple participant-provided narratives in ethnographic studies targeting causal attribution estimands.\n\n9.2.3.2 Uncertainty\nIn the Bayes approach parameter estimates and uncertainty estimates are generated simultaneously. One could imagine introducing uncertainty arising also from uncertainty about the prior or uncertainty about the model but in practice this is rarely done and in principle can be done by respecifying the prior and the model.\n\n9.2.4 Interval estimation\n\n9.2.4.1 Answer\nFor point and interval estimators, uncertainty is often expressed as a standard error estimate or confidence interval. Many approaches to standard error estimation are available. Indeed, just like point estimators for inquiries, we have point estimators for standard errors. You might choose classical standard errors or cluster-robust standard errors; you might bootstrap your standard errors or use the jackknife. Similarly, many approaches to confidence interval construction are available. Most often, confidence intervals are built from variance estimates under an appeal to sampling theory. Alternatively, a confidence interval can be formed by “inverting the test,” i.e. finding the range of null hypotheses we fail to reject. Whether any particular approach to uncertainty estimation is appropriate in a context will depend on the full set of design parameters and we encourage you to diagnose your uncertainty estimates as well.\nHere is the output of the answer strategy from Declaration 9.1, applied to the realized data set. We see the sample mean estimate of 15, the standard error estimate of 6, and the confidence interval from -10 to 40. These numbers communicate that our answer is 15 – but also that we know that number is shaky. We’re uncertain because the tool we used to answer the inquiry is high variance: it could bounce around a lot depending on which three people we happened to sample.\n\nthree_italian_citizens <- fabricate(N = 3, age = c(5, 15, 25))\nanswer_strategy <- declare_estimator(age ~ 1)\nanswer_strategy(three_italian_citizens)\n\n\n\n\n\nOne draw of the answer strategy\n \n estimate \n    std.error \n    statistic \n    p.value \n    conf.low \n    conf.high \n  \n\n 15 \n    5.77 \n    2.6 \n    0.12 \n    -9.84 \n    39.84 \n  \n\n\n\nFor tests, uncertainty is expressed by describing the properties of a procedure in terms of error rates. A test is an answer strategy that returns a binary answer to a binary estimand. The result of a test is an error if the empirical answer \\(a_d\\) does not equal the truth \\(a_{m^*}\\). Conventionally, a Type 1 error occurs when \\(a_d = 1\\) but \\(a_{m^*} = 0\\) and a Type 2 error occurs when \\(a_d = 0\\) but \\(a_{m^*} = 1\\). A perfect test (i.e., a test about about which we are fully certain) has Type 1 error rate of 0% and a Type 2 error rate of 0% as well. A test about which we are less certain might return \\(a_d = 1\\) 40% of the time when \\(a_{m^*} = 0\\) (a Type I error rate of 40%) and might return \\(a_d = 1\\) 90% of the time when \\(a_{m^*} = 1\\) (a Type II error rate of 10%).\nThe test reported by the answer_strategy function is a null hypothesis significance test against the null hypothesis that the average age in this Italian village is equal to exactly zero. The test returns “yes” if we reject the null and “no” if we fail to reject it. If we use the standard significance threshold of \\(\\alpha = 0.05\\) we fail to reject the null model because the p.value reported in the table is 0.12. It’s a silly test, but silly tests like these are reported by default in many statistical software languages and in many scientific papers to boot. It’s a silly test because we always knew the average age was not zero!\nOur uncertainty about the decision we made in the hypothesis test to fail to reject is not represented by the information in the table. Importantly, the \\(p\\)-value does not represent the probability that the null model is correct. The p.value is the probability that with our data and answer strategy, draws from the null model would lead to estimates of 15 or larger. According to our calculations, draws from the null model will do so 12 percent of the time. We use this probability along the way to making a decision about whether to reject the null model, but amazingly, a p.value does not describe our certainty about the significance test!\nWhat does characterize our uncertainty about a significance test? The Type I and type II error rates of the test. The Type I error rate is controlled by the significance threshold. A Type I error occurs if we reject the null model when it is true. If we use \\(\\alpha = 0.05\\) and the test is correctly accounts for all design elements, then a Type I error should only happen 5% of the time. Type II error rates are harder to learn about. In our case, we failed to reject the null model. To characterize our uncertainty about the test, we also want to calculate the probability that a design like this one would generate Type II errors. To do so, we have to imagine what it means for the null model to be false, since they can be false in many ways. One approach is to imagine how the test would perform on under a series of non-null models.\nFigure 9.4 describes the Type II error rate over a range of non-null models. If the true population mean is around 25 or lower, we fail to reject the null 75% of the time or more. With this comically small sample size, even if the true mean were 75, we would still fail to reject 20% of the time. We are rightly uncertain about this test – it may have a low enough Type I error rate (set as \\(\\alpha = 0.05\\)), but the Type II errors are way too big.\n\nDeclaration 9.4 Italian village declaration, varying the true mean age parameter\n\ndeclaration_9.5 <-\n  declare_model(data = resample_data(clingingsmith_etal)) +\n  declare_estimator(views ~ success, .method = difference_in_means)\n\n\n\n\nDiagnosis 9.4 (Diagnosing the Italian village design over many values of the true mean age parameter) \ndiagnosis_9.5 <- diagnose_designs(declaration_9.5)\n\n\n\nFigure 9.4: Type II error rates of the Italian village design\n\n\n\nA second type of interval estimation is bounding. In many circumstances, the details of the data strategy alone are insufficient to “point-identify” the inquiry, which means we can’t generate a point estimate without adding further assumptions. A standard approach is to simply make those further assumptions and move on to reporting point estimates. Under an agnostic approach – we don’t know if those assumptions are right because they aren’t grounded in the data strategy – we can turn to interval estimation instead.\nOne way to handle settings in which parameters are not point-identified is to generate “extreme value bounds.” These bounds report the the best and worst possibilities according to the logical extrema of the outcome variable.\nWe illustrate interval estimation back in our Italian village where we have learned the ages of three of the 100 citizens. Suppose we did not know whether the data strategy used random sampling, so we can’t rely on the guarantee that, under random sampling, the sample mean is unbiased for the population mean. Now we have reason about best and worst case scenarios. Let’s agree that the youngest a person can be is zero and the oldest is 110. Starting with an estimate of 15 among three citizens, we can generate lower and upper bound estimates for the average age of the entire 100-person village like this:\n\nlower_bound <- (3 * 15 + 97 * 0)/100\nupper_bound <- (3 * 15 + 97 * 110)/100\n c(lower_bound, upper_bound)\n\n\n\n\n\nExtreme value bounds estimate\n \n Lower bound \n    Upper bound \n  \n\n 0.45 \n    107.15 \n  \n\n\n\nThis procedure generates enormously wide bounds – we already knew before we started that the average age had to be somewhere between 0.45 and 107.15 years. But consider if we had data on 90 of the 100 citizens and among those 90, the average is 44. Now when we generate the bounds, they are still wide but not ridiculously so – the bounds put the average age somewhere between 40 and 50.\n\nlower_bound <- (90 * 44 + 10 * 0)/100\nupper_bound <- (90 * 44 + 10 * 110)/100\nc(lower_bound, upper_bound)\n\n\n\n\n\nThe extreme value bounds are tighter with more data\n \n Lower bound \n    Upper bound \n  \n\n 39.6 \n    50.6 \n  \n\n\n\nExtreme value bounds and variations on the idea can be applied when experiments encounter missingness or when we want to estimate effects among subgroups that only reveal themselves in some but not all treatment conditions Coppock (2019). The extreme value bound approach can also be used in qualitative settings in which we can impute some but not all of the missing potential outcomes using qualitative information; the bounds reflect our uncertainty about those missing values (Coppock and Kaur 2022).\n\n9.2.4.2 Uncertainty\nBounding approaches are built around about researcher uncertainty over models, however it’s important to remember that the bounds are themselves estimates that could have come out differently depending on the realization of the data. By this logic, we can attach standard errors and confidence intervals to the bounds (see Coppock et al. 2017 for an example)."
  },
  {
    "objectID": "declaration-diagnosis-redesign/choosing-answer-strategy.html#sec-ch9s3",
    "href": "declaration-diagnosis-redesign/choosing-answer-strategy.html#sec-ch9s3",
    "title": "9  Choosing an answer strategy",
    "section": "\n9.3 How to choose an answer strategy",
    "text": "9.3 How to choose an answer strategy\nNow that we have discussed all four research design elements in detail, we describe how to choose an answer strategy.\nThe model and the inquiry form the empirical half of the design, and the data and answer strategies make up the empirical half. Research designs that have parallel theoretical and empirical halves tend to be strong (though not all strong designs need be parallel in this way). This principle is motivated by the intersection of two ideas from statistics: the “plug-in principle” and “analyze as you randomize” (“AAYR”).\n\n9.3.1 Plug-in principle\nThe plug-in principle refers to the idea that sometimes, the answer strategy function and the inquiry function are very similar in form. The estimand, \\(I(m) = a_m\\), can often be estimated by choosing an A that is very similar to I and then “plugging-in” the realized data \\(d\\) that result from the data strategy for the unobserved data \\(m\\), i.e. \\(A(d) = a_d\\).\nMore formally, Aronow and Miller (2019) describe a plug-in estimator as:\n\nFor i.i.d. random variables \\(X_1, X_2, \\ldots, X_n\\) with common CDF \\(F\\), the plug-in estimator of \\(\\theta = T(F)\\) is: \\(\\widehat\\theta = T(\\widehat F)\\).\n\nwhere \\(\\widehat F\\) is an estimate of \\(F\\).\n\n9.3.2 Illustration of estimates using the plug-in principle\nTo illustrate the plug-in principle, suppose that our inquiry is the average treatment effect among the \\(N\\) units in the population.\n\\(I(m) = \\frac{1}{N}\\sum_1^N[Y_i(1) - Y_i(0)] = \\frac{1}{N}\\sum_1^NY_i(1) - \\frac{1}{N}\\sum_1^NY_i(0) = \\mathrm{ATE}\\)\nHere \\(T()\\) is the difference-in-means function.\nWe can develop a plug-in ATE estimator by replacing the population means — \\(\\frac{1}{N}\\sum_1^N Y_i(1)\\) and \\(\\frac{1}{N}\\sum_1^N Y_i(0)\\) — with sample analogues:\n\\(A(d) = \\frac{1}{m}\\sum_1^m{Y_i} - \\frac{1}{N - m}\\sum_{m+1}^N{Y_i}\\),\nwhere units 1 though \\(m\\) reveal their treated potential outcomes and the remainder reveal their untreated potential outcomes.\nWe could do the same thing for other functions, such as quantiles of the distribution or the variance of the distribution. In general plug-in estimators are not guaranteed to be unbiased but they can have nice asymptotic properties, converging to targets as the data increases (for conditions see Van der Vaart 2000).\n\n9.3.3 Illustration of estimates of uncertainty using the plug-in principle\nThe plug-in principle can be used also for generating estimates of uncertainty. For instance if we are interested in understanding the variance of the sampling distribution of estimates that we get from our procedure, we can use the bootstrap. With the bootstrap, we “plug-in” the sample for the population data, then repeatedly resample from our existing data. We can then approximate the true sampling distribution by calculating estimates on each resampled dataset, from which we can estimate the variance. (We use this nonparametric bootstrapping approach when generating estimates of uncertainty of diagnosands, see Section (simulation-error?)).\nAs an illustration of the logic of the approach using DeclareDesign, we compare usual the standard error estimate that accompanies a difference-in-means estimate with a bootstrapped standard error.\nFirst we set up a design that resamples from the Clingingsmith, Khwaja, and Kremer (2009)’s study of the effect of being randomly assigned to go on Hajj on tolerance of foreigners.\n\nBootstrapped standard errors\n\ndeclaration_9.5 <-\n  declare_model(data = resample_data(clingingsmith_etal)) +\n  declare_estimator(views ~ success, .method = difference_in_means)\n\n\n\nDiagnosis 9.5 (Bootstrap diagnosis) The bootstrapped estimates are gotten by summarizing over multiple runs of the design:\n\ndiagnosis_9.5 <- \n  declaration_9.5 |> \n  simulate_design(sims = sims) |> \n  summarize(se = sd(estimate)) \n\n\n\n\n\nBootstrapped standard error estimate\n \n se \n  \n\n 0.161 \n  \n\n\n\n\nWe can compare the standard error estimates using the standard deviation of the bootstrapped estimates to the standard error provided by the difference in means estimator implemented on the original data to see that they are quite close:\n\nget_estimates(design = declaration_9.5, data = clingingsmith_etal) \n\n\n\n\n\nAnalytic standard error estimate\n \n estimate \n    std.error \n  \n\n 0.475 \n    0.163 \n  \n\n\n\n\n9.3.4 Analyze as you randomize\nFollowing the plug-in principle only yields good answer strategies under some circumstances. Those circumstances are determined by the data strategy. We need data strategies that sample units, assign treatment conditions, and measure outcomes such that the revealed data can indeed by “plugged in” to the inquiry function. Whether this plug-in ATE estimator is a good answer strategy depends on features of the data strategy. It’s a good estimator when units are assigned to treatment with equal probabilities, but it’s a bad estimator if the probabilities differ from unit to unit.\nWhen the data strategy introduces distortions like differential probabilities of assignment, the answer strategy function should not equal the inquiry function: we can no longer just plug in the observed data. We have to compensate for those distortions, reversing them to reestablish parallelism.\nThis idea can be summarized as “analyze as you randomize,” a dictum attributed to R.A. Fisher. We use known features of the data strategy to adjust the answer strategy. We can undo the distortion introduced by differential probabilities of assignment by weighting units by the inverse of the probability of being in the condition that they are in. If we use an inverse-probability weighted (IPW) estimator, we restore parallelism because even though A no longer equals I, the relationship of D to A once again parallels the relationship of M to I.\n\n9.3.5 Illustration of estimates using the AAYR principle\nDeclaration 9.5 illustrates this idea. We declare the theoretical half of the design as MI then consider the intersection of two data strategies with two answer strategies. D1 has constant probabilities of assignment and D2 has differential probabilities of assignment. A1 is the plug-in estimator and A2 is the IPW estimator with the inverse probability weights generated by the D2 randomization protocol.\n\nDeclaration 9.5 Restoring parallelism design\n\nMI <-\n  declare_model(\n    N = 100,\n    X = rbinom(N, size = 1, 0.5),\n    U = rnorm(N),\n    potential_outcomes(Y ~ 0.5 * Z+-0.5 * X + 0.5 * X * Z + U)\n  ) +\n  declare_inquiry(ATE = mean(Y_Z_1 - Y_Z_0))\n\nD1 <- \n  declare_assignment(Z = complete_ra(N = N)) +\n  declare_measurement(Y = reveal_outcomes(Y ~ Z))\nD2 <- \n  declare_assignment(Z = block_ra(blocks = X, block_prob = c(0.1, 0.8))) +\n  declare_measurement(Y = reveal_outcomes(Y ~ Z))\n\n\nA1 <- declare_estimator(Y ~ Z, label = \"Unweighted\")\nA2 <-\n  declare_step(\n    handler = fabricate,\n    ipw = 1 / obtain_condition_probabilities(\n      assignment = Z,\n      blocks = X,\n      block_prob = c(0.1, 0.8)\n    )\n  ) +\n  declare_estimator(Y ~ Z, weights = ipw, label = \"Weighted\")\n\ndeclaration_9.6 <- list(MI + D1 + A1,\n                        MI + D1 + A2,\n                        MI + D2 + A1,\n                        MI + D2 + A2)\n\n\n\n\nDiagnosis 9.6 (Restoring parallelism diagnosis) \ndiagnosis_9.6 <- diagnose_design(declaration_9.6)\n\nWe diagnose the bias of all four design. Figure 9.5 shows that when the answer strategy and the data strategy match (D1 + A1 and D2 + A2), we have no bias. When they do not match (D1 + A2 and D2 + A1), we do. In this case, seeking parallelism in the choice of answer strategy improves the design. Of course, an alternative answer strategy we might call A3 that implements the weights corresponding to whatever the data strategy says they should be unbiased under both D1 and D2.\n\n\nFigure 9.5: When data and answer strategies are mismatched, we obtain bias.\n\n\n\nThis principle applies most clearly to the bias diagnosand, but it applies to others as well. For example, Abadie et al. (2017) recommend that answer strategies should include clustered standard errors at the level of sampling or assignment, whichever is higher. The data strategies that include clustering introduce a dependence among units that was not present in the model; clustered standard errors account for this dependence. If we did not do so, our estimated standard error would be a bad estimate of the “standard deviation” diagnosand.\nMore generally, the principle to “design agnostically” implies that we should choose “agnostic” answer strategies, by which we mean answer strategies that produce good answers under a wide range of models. Selecting answer strategies that are robust to multiple models ensures that we not only get good answers when our model is spot on — which is rare! — but under many possible circumstances.\nUnderstanding whether the choices over answer strategies — logit or probit or OLS — depend on the model being a particular way is crucial to making a choice. For example, many people have been taught that whenever the outcome variable is binary, OLS is inappropriate and they must use a binary choice model like logit instead. When the inquiry is the probabilities of success for each unit and we use covariates to model them, how much better logit performs at estimating probabilities depends on the model. When probabilities are all close to 0.5, the two answer strategies both perform well. When the probabilities spread out from 0.5, OLS is less robust and logit beats it. In the same breath, however, we can consider these same two estimators in the context of a randomized experiment with a binary outcome. Here, OLS is just as strong as logit, no matter the distribution of the potential outcomes. In this setting, when design agnostically, we find that both estimators are robust (see Section (redesigning-over-estimators-logit-probit-or-ols?)).\nDesigning agnostically has something in common with robustness checks: both share the motivation that we have fundamental uncertainty about the true model. A robustness check is an alternative answer strategy that changes some model assumption that the main answer strategy depends on. Presenting three estimates of the same parameter under different answer strategies (logit, probit, and OLS) and making a joint decision based on the set of estimates about whether the main analysis is “robust” is a procedure for assessing “model dependence.” But robustness checks are just answer strategies themselves, and we should declare them and diagnose them to understand whether they are good answer strategies. We want to understand the properties of the robustness check, e.g., under what models and how frequently does it correctly describe the main answer strategy as “robust.”\n\n9.3.6 Illustration of estimates of uncertainty using the AAYR principle\nRandomization inference describes a large class of procedures for generating \\(p\\)-values that merit special attention. Randomization inference leverages known features of the randomization procedure to simulate trials under a null model (see Gerber and Green (2012), chapter 3, for an introduction to randomization inference). In a common case, a randomization inference test proceeds by stipulating a null model under which the counterfactual outcomes of each unit are exactly equal to the observed outcomes, the so-called “sharp null hypothesis of no effect.” Under this null hypothesis, the treated and untreated potential outcomes are exactly equal for each unit, reflecting a model in which the treatment has exactly zero effect for each unit.\nAs described above, a \\(p\\)-value is an answer to the question: what is the probability the null model would generate estimates as large or larger in absolute value than the observed estimate? We can answer this question by diagnosing the design under the sharp null model. Importantly, the randomization inference procedure follows the “analyze as you randomize” principle by conducting repeated random assignments according to the original randomization protocol.\nWe illustrate randomization inference with a voter mobilization experiment reported by Foos et al. (2021). The design was blocked and clustered: voters were clustered by their street, and the assignment was blocked by ward. Wards vary in their size and in the probability of assignment, so we have to recompute inverse probability weights in each draw from the null model.\nHere is the observed estimate, indicating that our best guess is that average effect of the mobilization treatment on voter turnout is 2.8 percentage points.\n\nobserved_estimate <-\n  lm_robust(\n    marked_register_2014 ~ treat + ward,\n    weights = weights,\n    clusters = street,\n    data = foos_etal\n  ) \n\n\n\n\n\nResults from @Foos2021\n \n term \n    estimate \n  \n\n treat \n    0.028 \n  \n\n\n\nHere we declare the null model (indicated by potential_outcomes(Y ~ 0 * Z + marked_register_2014)) and add it to the data and answer strategies:\n\nRandomization inference under the sharp null\n\nlibrary(rdddr) # for helper functions\nlibrary(rstanarm)\n#> Loading required package: Rcpp\n#> This is rstanarm version 2.21.3\n#> - See https://mc-stan.org/rstanarm/articles/priors for changes to default priors!\n#> - Default priors may change, so it's safest to specify priors, even if equivalent to the defaults.\n#> - For execution on a local, multicore CPU with excess RAM we recommend calling\n#>   options(mc.cores = parallel::detectCores())\n\ndeclaration_9.3 <-\n  declare_model(N = 100, age = sample(0:80, size = N, replace = TRUE)) +\n  declare_inquiry(mean_age = mean(age)) +\n  declare_sampling(S = complete_rs(N = N, n = 3)) +\n  declare_estimator(\n    age ~ 1,\n    .method = stan_glm,\n    family = gaussian(link = \"log\"),\n    prior_intercept = normal(50, 5),\n    .summary = ~tidy_stan(., exponentiate = TRUE),\n    inquiry = \"mean_age\"\n  )\n\n\n\nDiagnosis 9.7 (Randomization inference “diagnosis”) \nWe diagnose the null design with respect to the p.value diagnosand: what fraction of simulations under the null model exceed the observed estimate? We find that the p.value is 0.13, so the estimate is not deemed statistically significant.\n\n\np.value <-\n  declare_diagnosands(\n    p.value = mean(abs(estimate) >= abs(observed_estimate$coefficients))\n  )\n\ndiagnosis_9.7 <-\n  diagnose_design(\n    declaration_9.7,\n    diagnosands = p.value\n  )\n\ntidy(diagnosis_9.7)\n\n\n\n\n\n\n\n\nRandomization inference 'diagnosis' to obtain a p-value\n \n design \n    diagnosand \n    estimate \n  \n\n declaration_9.7 \n    p.value \n    0.13 \n  \n\n\n\nSome naive procedures for generating \\(p\\)-values might ignore or otherwise fail to incorporate the important design information (in this example, the blocking and clustering procedures). Randomization inference naturally incorporates this design information by holding the data and answer strategies fixed, while swapping in a null model. Simulating the resulting design yields a sampling distribution under the null, which can then be compared to the observed estimate."
  },
  {
    "objectID": "declaration-diagnosis-redesign/choosing-answer-strategy.html#summary",
    "href": "declaration-diagnosis-redesign/choosing-answer-strategy.html#summary",
    "title": "9  Choosing an answer strategy",
    "section": "\n9.4 Summary",
    "text": "9.4 Summary\nThe answer strategy describes what we do with the data once we’ve got it. We use the data to generate estimates that, if we’ve calibrated our design correctly, will with high frequency come close to the estimand, the value of the inquiry. Answer strategies are defined with respect to units, their conditions, their outcomes, and a summary method for generating estimates. We like to include measures of uncertainty without estimates; these measures are like estimates of design properties. Answer strategies come in a many varieties but the main four are point estimation, interval estimation, tests, and summaries of Bayesian posterior distributions. Choosing a good answer strategy means staying responsive to the relevant model, inquiry, and data strategy elements.\n\n\n\n\nAbadie, Alberto, Susan Athey, Guido W. Imbens, and Jeffrey Wooldridge. 2017. “When Should You Adjust Standard Errors for Clustering?” NBER Working Paper 24003.\n\n\nAbell, Peter, and Ofer Engel. 2021. “Subjective Causality and Counterfactuals in the Social Sciences: Toward an Ethnographic Causality?” Sociological Methods & Research 50 (4): 1842–62.\n\n\nAronow, Peter M., Jonathon Baron, and Lauren Pinson. 2019. “A Note on Dropping Experimental Subjects Who Fail a Manipulation Check.” Political Analysis 27 (4): 572–89.\n\n\nAronow, Peter M., and Benjamin T. Miller. 2019. Foundations of Agnostic Statistics. Cambridge, UK: Cambridge University Press.\n\n\nBenjamin, Daniel J., James O. Berger, Magnus Johannesson, Brian A. Nosek, E. -J. Wagenmakers, Richard Berk, Kenneth A. Bollen, et al. 2018. “Redefine Statistical Significance.” Nature Human Behaviour 2 (1): 6–10.\n\n\nBennett, Andrew. 2015. “Appendix.” In Process Tracing: From Metaphor to Analytic Tool, edited by Andrew Bennett and Jeffrey Checkel. New York: Cambridge University Press.\n\n\nClingingsmith, David, Asim Ijaz Khwaja, and Michael Kremer. 2009. “Estimating the Impact of the Hajj: Religion and Tolerance in Islam’s Global Gathering.” The Quarterly Journal of Economics 124 (3): 1133–70.\n\n\nCoppock, Alexander. 2019. “Avoiding Post-Treatment Bias in Audit Experiments.” Journal of Experimental Political Science 6 (1): 1–4.\n\n\nCoppock, Alexander, Alan S. Gerber, Donald P. Green, and Holger L. Kern. 2017. “Combining Double Sampling and Bounds to Address Nonignorable Missing Outcomes in Randomized Experiments.” Political Analysis 25 (2): 188–206.\n\n\nCoppock, Alexander, and Dipin Kaur. 2022. “Qualitative Imputation of Missing Potential Outcomes.” American Journal of Political Science.\n\n\nFairfield, Tasha, and Andrew E Charman. 2017. “Explicit Bayesian Analysis for Process Tracing: Guidelines, Opportunities, and Caveats.” Political Analysis 25 (3): 363–80.\n\n\nFoos, Florian, Peter John, Christian Müller, and Kevin Cunningham. 2021. “Social Mobilization in Partisan Spaces.” The Journal of Politics 83 (3): 1190–97.\n\n\nGerber, Alan S., and Donald P. Green. 2012. Field Experiments: Design, Analysis, and Interpretation. New York: W.W. Norton.\n\n\nGreen, Donald P., and Winston Lin. 2016. “Standard Operating Procedures: A Safety Net for Pre-Analysis Plans.” PS: Political Science & Politics 49 (3): 495–99.\n\n\nHerron, Michael C., and Kevin M. Quinn. 2016. “A Careful Look at Modern Case Selection Methods.” Sociological Methods & Research 45 (3): 458–92.\n\n\nHumphreys, Macartan, and Alan M. Jacobs. 2015. “Mixing Methods: A Bayesian Approach.” American Political Science Review 109 (4): 653–73.\n\n\nVan der Vaart, Aad W. 2000. Asymptotic Statistics. Vol. 3. Cambridge university press."
  },
  {
    "objectID": "declaration-diagnosis-redesign/redesigning.html",
    "href": "declaration-diagnosis-redesign/redesigning.html",
    "title": "\n10  Redesigning\n",
    "section": "",
    "text": "Redesign is the process of choosing the single empirical design to be implemented from a large family of possible designs. To make this choice, we systematically vary aspects of the data and answer strategies to understand their impact on the most important diagnosands. Redesign entails diagnosing many possible empirical designs over the range of plausible theoretical models, and comparing them.\nA sample size calculation is the prototypical example of a redesign. Holding the model, inquiry, and answer strategy constant, we vary the “sample size” feature of the data strategy in order to understand how a diagnosand like the width of the confidence interval changes as we change \\(N\\).\nNot surprisingly, most designs get stronger as we allocate more resources to them. The expected width of a confidence interval could always be tighter, if only we had more subjects. Standard errors could always be smaller, if only we took more pre-treatment measurements. At some point, though, the gains are not worth the increased costs, so we settle for an affordable design that meets our scientific goals well enough. (Of course, if the largest affordable design has poor properties, no version of the study is worth implementing). The knowledge-expense tradeoff is a problem that every empirical study faces. The purpose of redesign is to explore this and other tradeoffs in a systematic way."
  },
  {
    "objectID": "declaration-diagnosis-redesign/redesigning.html#redesigning-over-data-strategies",
    "href": "declaration-diagnosis-redesign/redesigning.html#redesigning-over-data-strategies",
    "title": "\n10  Redesigning\n",
    "section": "\n10.1 Redesigning over data strategies",
    "text": "10.1 Redesigning over data strategies\nA redesign over a data strategy choice can be summarized with a “power curve.” We want to learn the power of a test at many sample sizes, either so we can learn the price of precision, or so we can learn what sample size is required for a minimum level of statistical power.\nWe start with a minimal design declaration: we draw samples of size \\(N\\) and measure a single binary outcome \\(Y\\), then conduct a test against the null hypothesis that the true proportion of successes is equal to 0.5.\n\nDeclaration 10.1 A baseline declaration intended to be redesigned over \\(N\\).\n\nN <- 100\n\ndeclaration_11.1 <-\n  declare_model(N = N) +\n  declare_measurement(Y = rbinom(n = N, size = 1, prob = 0.55)) +\n  declare_test(handler =\n                 label_estimator(function(data) {\n                   test <- prop.test(x = table(data$Y), p = 0.5)\n                   tidy(test)\n                 }))\n\n\n\nDiagnosis 10.1 (Diagnosing over a redesign) To construct a power curve, we redesign our baseline declaration over values of \\(N\\) that vary from 100 to 1000.\n\ndiagnosis_11.1 <- \n  declaration_11.1 |>\n  redesign(N = seq(100, 1000, 100)) |>\n  diagnose_designs()\n\n\n\n\nRedesigns are often easiest to understand graphically, as in Figure 10.1. At each sample size, we learn the associated level of statistical power. We might then choose the least expensive design (sample size 800) that meets a minimum power standard (0.8).\n\n\nFigure 10.1: Redesigning with respect to sample size"
  },
  {
    "objectID": "declaration-diagnosis-redesign/redesigning.html#redesigning-under-model-uncertainty",
    "href": "declaration-diagnosis-redesign/redesigning.html#redesigning-under-model-uncertainty",
    "title": "\n10  Redesigning\n",
    "section": "\n10.2 Redesigning under model uncertainty",
    "text": "10.2 Redesigning under model uncertainty\nWhen we diagnose studies, we do so over the many theoretical possibilities we entertain in the model. Through diagnosis, we learn how the values of the diagnosands change depending on model parameters. When we redesign, we explore a range of empirical strategies over the set of model possibilities. Redesign might indicate that one design is optimal under one set of assumptions, but that a different design would be preferred if a different set holds.\nWe illustrate this idea with an analysis of the minimum detectable effect (MDE) and how it changes at different sample sizes. The MDE diagnosand is complex. Whereas most diagnosands can be calculated with respect to a single possible model in \\(M\\), the MDE is defined over a range of possible models. It is obtained by calculating the statistical power of the design over a range of possible effect sizes (holding the empirical design constant), then reporting the effect size that is associated with (typically) 80% statistical power.\nMDEs can be a useful heuristic for thinking about the multiplicity of possibilities in the model. If the minimum detectable effect of a study is enormous – a one standard deviation effect, say – then we don’t have to think much harder about our beliefs about the true effect size. Whatever our priors over the true effect size are, they are probably smaller than 1.0 SDs, so we can immediately conclude that the design is too small.\nDeclaration 10.2 contains uncertainty over the true effect size. This uncertainty is encoded in the runif(n = 1, min = 0, max = 0.5) command, which corresponds to our uncertainty over the ATE. It could be as small as 0.0 SDs or as large as 0.5 SDs, and we are equally uncertain about all the values in between. We redesign over three values of \\(N\\): 100, 500, and 1000, then simulate each design. Each run of simulation features a different true ATE somewhere between 0.0 and 0.5.\n\nDeclaration 10.2 Uncertainty over effect size design\n\nN <- 100\ndeclaration_11.2 <-\n  declare_model(N = N, U = rnorm(N),\n                # this runif(n = 1, min = 0, max = 0.5) generates 1 random ATE between 0 and 0.5\n                potential_outcomes(Y ~ runif(n = 1, min = 0, max = 0.5) * Z + U)) +\n  declare_inquiry(ATE = mean(Y_Z_1 - Y_Z_0)) +\n  declare_assignment(Z = complete_ra(N, prob = 0.5)) +\n  declare_measurement(Y = reveal_outcomes(Y ~ Z)) +\n  declare_estimator(Y ~ Z, inquiry = \"ATE\")\n\n\n\n\nDiagnosis 10.2 (Redesigning under uncertainty) \ndiagnosis_11.2 <-\n  declaration_11.2 |>\n  redesign(N = c(100, 500, 1000)) |>\n  diagnose_designs()\n\nFigure 10.2 summarizes the simulations by smoothing over effect sizes: the loess curves describes the fraction of simulations that are significant at each effect size.1 The MDEs for each sample size can be read off the plot by examining the intersection of each curve with the dotted line at 80% statistical power. At N = 1000, the MDE is approximately 0.175 SDs. At N = 500, the MDE is larger, at approximately 0.225 SDs. If the design only includes 100 units, the MDE is some value higher than 0.5 SDs. We could of course expand the range of effect sizes considered in the diagnosis, but if effect sizes above 0.5 SDs are theoretically unlikely, we don’t even need to – we’ll need a design larger that 100 units in any case.\nThis diagnosis and redesign shows how our decisions about the data strategy depend on beliefs in the model. If we think the true effect size is likely to be 0.225 SDs, then a design with 500 subjects is a reasonable choice, but if it is smaller than that, we’ll want a larger study. Small differences in effect size have large consequences for design. Researchers who arrive at a plot like Figure 10.2 through redesign should be inspired to sharpen up their prior beliefs about the true effect size, either through literature review, meta-analysis of past studies, or through piloting (see Section (piloting?)).\n\n\nFigure 10.2: Redesigning an experiment over model uncertainty about the true effect size\n\n\n\n\n10.2.1 Redesigning over two data strategy parameters\nSometimes, we have a fixed budget (in terms of financial resources, creative effort, or time), so the redesign question isn’t about how much to spend, but how to spend it across competing demands. For example, we might want to find the sample size N and the fraction of units to be treated prob that minimize a design’s error subject to a fixed budget. Data collection costs $2 per unit and treatment costs $20 per treated unit. We need to choose how many subjects to sample and how many to treat. We might rather add an extra 11 units to the control units (additional cost $2 * 11 = $22) than add one extra unit to the treatment group (additional cost $2 + $20 = $22).\nWe solve the optimization problem:\n\\[\\begin{align*}\n& \\underset{N, N_t}{\\text{argmin}}\n& & E_M(L(a^{d} - a^{m}|D_{N, m})) \\\\\n& \\text{s.t.}\n& & 5 N + 20  m \\leq 5000\n\\end{align*}\\]\nwhere \\(L\\) is a loss function, increasing in the difference between \\(a^{d}\\) and \\(a^{m}\\).\nWe can explore this optimization with bare-bones declaration of a two-arm trial that depends on two separate data strategy parameters, N and prob:\n\nDeclaration 10.3 Bare-bones two-arm trial\n\nN <- 100\n\ndeclaration_11.3 <-\n  declare_model(N = N, U = rnorm(N),\n                potential_outcomes(Y ~ 0.2 * Z + U)) +\n  declare_inquiry(ATE = mean(Y_Z_1 - Y_Z_0)) +\n  declare_assignment(Z = complete_ra(N = N, prob = prob)) +\n  declare_measurement(Y = reveal_outcomes(Y ~ Z)) +\n  declare_estimator(Y ~ Z, inquiry = \"ATE\")\n\n\n\nDiagnosis 10.3 (Redesigning over two parameters) We redesign, varying those two parameters over reasonable ranges: 100 to 1000 subjects, with probabilities of assignment from 0.1 to 0.5. The redesign function smartly generates designs with all combinations of the two parameters. We want to consider the consequences of these data strategy choices for two diagnosands: cost and a very common loss function: mean squared error.\n\ndiagnosands <-\n  declare_diagnosands(cost = unique(N * 2 + prob * N * 20),\n                      rmse = sqrt(mean((estimate - estimand) ^ 2)))\n\ndiagnosis_11.3 <-\n  declaration_11.3 |>\n  redesign(N = seq(100, 1000, 25),\n           prob = seq(0.1, 0.5, 0.2)) |>\n  diagnose_designs(diagnosands = diagnosands)\n\n\n\n\nThe diagnosis is represented in Figure 10.3. The top panel shows the cost of empirical designs, at three probabilities of assignment over many sample sizes. The bottom panel shows the RMSE of each design. According to this diagnosis, the best combination that can be achieved for less than $5,000 is N = 600 with prob = 0.3. This conclusion is in mild tension with common the design advice that under many circumstances, balanced designs are preferable (see Section (analytic-design-diagnosis?) in the design library for an in-depth discussion of this point). Here, untreated subjects are so much less expensive than treated subjects, we want to tilt the design towards having a larger control group. How far to tilt depends on model beliefs as well as the cost structure of the study.\n\n\nFigure 10.3: Redesigning an experiment with respect to RMSE and subject to a budget constraint"
  },
  {
    "objectID": "declaration-diagnosis-redesign/redesigning.html#redesigning-over-answer-strategies",
    "href": "declaration-diagnosis-redesign/redesigning.html#redesigning-over-answer-strategies",
    "title": "\n10  Redesigning\n",
    "section": "\n10.3 Redesigning over answer strategies",
    "text": "10.3 Redesigning over answer strategies\nRedesign can also take place over possible answer strategies. An inquiry like the average treatment effect could be estimated using many different estimators: difference-in-means, logistic regression, covariate-adjusted ordinary least squares, the stratified estimator, doubly robust regression, targeted maximum likelihood regression, regression trees – the list of possibilities is long. Redesign is an opportunity to explore how many alternative analysis approaches work.\nA key tradeoff in the choice of answer strategy is the bias-variance tradeoff. Some answer strategies exhibit higher bias but lower variance while others have lower bias but higher variance. Choosing which side of the bias-variance tradeoff to take is complicated and the process for choosing among alternatives must be motivated by the scientific goals at hand.\nA common heuristic for trading off bias and variance is the mean squared error (MSE) diagnosand. Mean squared error is equal to the square of bias plus variance, which is to say MSE weighs bias and variance equally. Typically, researchers choose among alternative answer strategies by minimizing MSE. If in your scientific context, bias is more important than variance, you might choose a an answer strategy that accepts slightly more variance in exchange for a decrease in bias.\nTo illustrate the bias-variance tradeoff, Declaration 10.4 describes a setting in which the goal is to estimate the conditional expectation of some outcome variable Y with respect to a covariate X. The true conditional expectation function (produced by the custom dip function) is not smooth, but we estimate it with smooth polynomial functions of increasing order.\n\nDeclaration 10.4 Conditional expectation function design\n\nlibrary(purrr)\n\ndip <- function(x) (x <= 1) * x + (x > 1) * (x - 2) ^ 2 + 0.2\nx_range <- seq(from = 0, to = 3, length.out = 50)\npolynomial_degrees <- 1:6\n\ndeclaration_11.4 <-\n  declare_model(\n    N = 100,\n    X = runif(N, 0, 3)) +\n  declare_inquiry(\n    X = x_range, inquiry = str_c(\"X_\", X), estimand = dip(X),\n    data = NULL, handler = tibble\n  ) +\n  declare_measurement(Y = dip(X) + rnorm(N, 0, .5)) +\n  declare_estimator(handler = function(data) {\n    map(polynomial_degrees, ~lm(Y ~ poly(X, .), data = data)) |> \n      set_names(nm = str_c(\"A\", polynomial_degrees)) |> \n      map_dfc(~predict(., newdata = tibble(X = x_range))) |> \n      bind_cols(tibble(X = x_range)) |> \n      mutate(inquiry = str_c(\"X_\", X)) |> \n      pivot_longer(cols = starts_with(\"A\"),\n                   names_to = \"estimator\",\n                   values_to = \"estimate\")\n  })\n\n\nFigure 10.4 shows one draw of this design – the predictions of the CEF made by nine regressions of increasing flexibility. A polynomial of order 1 is just a straight line, a polynomial of order 2 is a quadratic, order 3 is a cubic etc. Aronow and Miller (2019) show (Theorem 4.3.3) that even nonlinear CEFs can be approximated to up to an arbitrary level of precision by increasing the order of the polynomial regression used to estimate it, given enough data. The figure provides some intuition for why. As the order of the polynomial increases, the line becomes more flexible and can accommodate unexpected twists and turns in the CEF.\n\n\nFigure 10.4: Estimating a CEF with polynomials of increasing order\n\n\n\nDiagnosis 10.4 (Conditional expectation function diagnosis) Increasing the order of the polynomial decreases bias, but this decrease comes at the cost of variance. Figure 10.4 shows how, when the order increases, bias goes down while variance goes up. Mean squared error is one way to trade these two diagnosands off one another. Here, MSE is minimized with a polynomial of order 3. If we were to care much more about bias than variance, perhaps we would choose a polynomial of even higher order.\n\ndiagnosis_11.4 <- diagnose_design(diagnosis_11.4)\n\n\n\n\n\n\nFigure 10.5: The bias-variance tradeoff when choosing the flexibility of polynomial approximations to the CEF\n\n\n\n\n10.3.1 Redesigning over estimators: Logit, Probit, or OLS?\nA perennial debate among social scientists is whether to use a binary choice model like logit or probit when the outcome is binary, or if the workhorse OLS estimator is preferable. Unsurprisingly, who is right in this debate depends on other features of the research design. For example, in an observational descriptive study in which the inquiry is a prediction for the probability of success among a particular group of units, explicitly accounting for the binary nature the outcome variable is important; OLS can generate predictions that lie outside the theoretically permissible zero to one range. However, in an experimental causal study in which the inquiry is the average treatment effect, it is not possible for a comparison of treatment and control group means (as estimated by OLS) to generate a nonsense treatment effect estimate in the sense of being outside the theoretically permissible -100 to +100 percentage point range. By contrast, binary choice models like logit or probit can theoretically impossible average treatment effect estimates, once average marginal effect estimates are calculated. This problem is demonstrated in the following declaration, diagnosis, and redesign:\n\nDeclaration 10.5 Choosing logit, probit, or OLS\n\nlibrary(margins)\n\ntidy_margins <- function(x) {\n  tidy(margins(x, data = x$data), conf.int = TRUE)\n}\n\nN <- 10\n\ndeclaration_11.5 <-\n  declare_model(N = N,\n                U = rnorm(N),\n                potential_outcomes(Y ~ rbinom(N, 1, prob = 0.2 * Z + 0.6))) +\n  declare_inquiry(ATE = mean(Y_Z_1 - Y_Z_0)) +\n  declare_assignment(Z = complete_ra(N, prob = 0.5)) +\n  declare_measurement(Y = reveal_outcomes(Y ~ Z)) +\n  declare_estimator(Y ~ Z,\n                    inquiry = \"ATE\",\n                    term = \"Z\",\n                    label = \"OLS\") +\n  declare_estimator(\n    Y ~ Z,\n    .method = glm,\n    family = binomial(\"logit\"),\n    .summary = tidy_margins,\n    inquiry = \"ATE\",\n    term = \"Z\",\n    label = \"logit\"\n  ) +\n  declare_estimator(\n    Y ~ Z,\n    .method = glm,\n    family = binomial(\"probit\"),\n    .summary = tidy_margins,\n    inquiry = \"ATE\",\n    term = \"Z\",\n    label = \"probit\"\n  ) \n\n\n\n\nDiagnosis 10.5 (Redesigning alternative estimators over sample sizes) \ndiagnosis_11.5 <-\n  declaration_11.5 |>\n  redesign(N = seq(10, 100, by = 10)) |>\n  diagnose_designs()\n\nFigure 10.6 displays the distribution of average treatment effect estimates from this design, over three answer strategies (OLS, logit, and probit) and 10 sample sizes. As sample size increases, the differences across these three approaches vanishes. Eventually, all three sampling distributions center on the ATE. However, at small sample sizes, the three approaches do differ in an important respect. The estimates from OLS are always constrained to the theoretical minimum and maximum average treatment effects of -100 percentage points to 100 percentage points, whereas logit and probit sometimes generate estimates outside this theoretical range.\nThis example underlines how the choice among estimators depends so deeply on the inquiry. When the inquiry is a predicted probability, OLS has a bad property of sometimes generating predictions outside the zero-one range. When the inquiry is an average treatment effect, the linear model works just fine, but the nonlinear binary choice models can get tripped up when calculating average marginal effects.\n\n\nFigure 10.6: Sampling distribution of three estimators, varying sample size"
  },
  {
    "objectID": "declaration-diagnosis-redesign/redesigning.html#summary",
    "href": "declaration-diagnosis-redesign/redesigning.html#summary",
    "title": "\n10  Redesigning\n",
    "section": "\n10.4 Summary",
    "text": "10.4 Summary\nWe use the redesign process to learn about design tradeoffs. Most often, the tradeoff is some measure of design quality like power against cost. We want to trade quality off against cost until we find a good enough study for the budget. Sometimes the tradeoff is across design parameters – holding budget fixed, should we sample more clusters or should we sample more people within clusters? Sometimes the tradeoff is across diagnosands – more flexible answer strategies may exhibit lower bias but higher variance. Minimizing RMSE is weighs bias and variance equally, but other weightings are possible. Tradeoffs across diagnosands are implicit in many design decisions, but design diagnosis and redesign can help make those tradeoffs explicit.\n\n\n\n\nAronow, Peter M., and Benjamin T. Miller. 2019. Foundations of Agnostic Statistics. Cambridge, UK: Cambridge University Press."
  },
  {
    "objectID": "declaration-diagnosis-redesign/diagnosing-designs.html",
    "href": "declaration-diagnosis-redesign/diagnosing-designs.html",
    "title": "\n11  Diagnosing designs\n",
    "section": "",
    "text": "Research design diagnosis is the process of evaluating the properties of a research design. We invent the term “diagnosand” to refer to those properties of a research design we would like to diagnose. Many diagnosands are familiar. Power is the probability of obtaining a statistically significant result. Bias is the average deviation of estimates from the estimand. Other diagnosands are more exotic, like the Type-S error rate, which is the probability the estimate has the incorrect sign, conditional on being statistically significant (Gelman and Carlin 2014). This chapter focuses mainly on the use of Monte Carlo computer simulations to estimate diagnosands, though we touch on analytic design diagnosis as well.\nResearch designs are strong when the empirical answer \\(a_{d^*}\\) generated by the design is close to the true answer \\(a_{m^*}\\). In general we cannot know \\(a_{m^*}\\) with certainty—some combination of the fundamental problems of generalization, causal inference, and descriptive inference conspire to hide the truth from us. This problem means we have to assess the performance of our designs, not with respect to the real world, but instead with respect to our unverified and unverifiable beliefs about the world. In other words, we assess the properties of research designs by comparing the simulated answer \\(a_d\\) to the answers under the model \\(a_m\\), over many possible realizations of the design.\nFigure 11.1 below is similar to Figure 6.1, which we described when defining the components of a research design. To recapitulate the main points of that discussion: The theoretical half of a research design defines an inquiry I in terms of a model M. The true answer to that question is the inquiry applied to the real world \\(I(m^*) = a_{m^*}\\). The empirical half of a research design applies the data strategy to the real world to generate a dataset (\\(D(m^*) = d^*\\)), then applies the answer strategy to the realized dataset to generate an answer: (\\(A(d^*) = a_{d^*}\\)). All these stars reflect the fact that reality plays an important role in empirical research designs. We commit to the notion that there are real answers to the questions we have posed. Our empirical answers are tethered to reality because the data strategy generates information that depends on the real world.\nWhen we simulate and diagnose designs, however, this tether to reality is snipped, and we find ourselves in the bottom half of Figure 11.1. When we simulate, the theoretical half of the design entertains many possibilities in the model M, which we label \\(m_1\\), \\(m_2\\), …, \\(m_k\\). The answers under each of \\(m_1\\), \\(m_2\\), …, \\(m_k\\) models are labeled \\(a_{m_k}\\). Each of the \\(k\\) possibilities is generates a different answer to the inquiry: \\(a_{m_1}\\), \\(a_{m_2}\\), …, \\(a_{m_k}\\). Importantly, the simulated research design does not have access to the true answer \\(a_{m^*}\\), only answers under the \\(k\\) considered models.\nWe pause to address one confusing aspect of our notation. The set of models \\(m_1\\), \\(m_2\\), …, \\(m_k\\) could refer to a set of \\(k\\) separate theoretical perspectives. Or it could refer to \\(m_1\\), \\(m_2\\), …, \\(m_k\\) draws from the same basic model, but the values of the exogenous variables (following the specific meaning described in Section (signature?)) are slightly different. In other words, our notation doesn’t draw a deep distinction between large differences between theoretical models and small ones.\nWhen we simulate, the empirical half of the design is similarly dissociated from reality. We apply the data strategy \\(D\\) to each of the model draws \\(m_k\\) to produce simulated datasets index as \\(d_k\\). These fabricated datasets may be similar to or different from the true dataset \\(d^*\\) that would result if the design were realized (the more similar the better). We apply the answer strategy A to each simulated dataset \\(d_k\\) in order to produce simulated answers \\(a_{d_k}\\).\nA comparison of the top row to the bottom three rows of Figure 11.1 shows how actual research designs differ from simulated research designs. Actual research designs are influenced by reality—we learn about the real world by conducting empirical research. We don’t learn about the real world from simulated research designs. Instead, we learn about research designs themselves. We learn how the research design would behave if the real world \\(m^*\\) were like the model draw \\(m_1\\)—or like \\(m_{100}\\). We can only evaluate designs under the possibilities we consider in \\(M\\). If the world \\(m^*\\) is not in that set of possibilities, we won’t have evaluated the design under the most important setting, i.e., what will happen when we actually apply the design in reality. We want to follow Principle 3.2 to “design agnostically” or in order words, to consider such a wide range of possibilities that some \\(a_{m_k}\\) will be close enough to \\(a_{m^*}\\).\nWith this definition of a diagnosis in hand, we can now sharpen our thinking on when we have a complete design declaration. A design declaration is “diagnosand-complete” when the declaration contains enough information to calculate a specific diagnosand, or statistic about the design. The “bias” diagnosand requires that all four of M, I, D, and A are specified in sufficient detail because we need to be able to compare the answer under the model (\\(a_m\\)) to the simulated empirical answer (\\(a_d\\)). The “statistical power” diagnosand often does not require the inquiry \\(I\\) to be specified, since we can mechanistically conduct null hypothesis significance tests without reference to inquiries. Neither the “bias” nor “statistical power” diagnosands require any details of confidence interval construction, but without those details, the declaration is not diagnosand-complete for “coverage.”\nEvery design we have ever declared is diagnosand-complete for some diagnosands but not for others. Diagnosand-completeness for every common diagnosand is not a goal for design declaration. We want to be diagnosand-complete for the set of diagnosands that matter most in a particular research setting."
  },
  {
    "objectID": "declaration-diagnosis-redesign/diagnosing-designs.html#elements-of-diagnoses",
    "href": "declaration-diagnosis-redesign/diagnosing-designs.html#elements-of-diagnoses",
    "title": "\n11  Diagnosing designs\n",
    "section": "\n11.1 Elements of diagnoses",
    "text": "11.1 Elements of diagnoses\n\n11.1.1 Diagnosand statistics\nDiagnosands are summaries of the distributions of diagnostic statistics. We’ll start by defining diagnostic statistics, then move on to describing how to generate the distribution of diagnostic statistics, then how to summarize those distributions in order to estimate diagnosands.\nA diagnostic statistic \\(\\phi_k\\) is itself a function of \\(a_{m_k}\\) and \\(a_{d_k}\\): \\(\\phi_k = g(a_{m_k}, a_{d_k})\\). We elide here two important notational annoyances. For the purpose of the following discussion of diagnostic statistics and diagnosands, \\(a_{d_k}\\) can refer to a estimate like an difference-in-means estimate or an associated uncertainty statistic like a standard error or a \\(p\\)-value. Importantly, diagnostic statistics can be functions that depend on \\(a_{m_k}\\) only, \\(a_{d_k}\\) only, or both together.\nEach diagnostic statistic is the result of a different function \\(g\\). For example, the “error” diagnostic statistic is produced by this function: \\(g(a_{m_k}, a_{d_k}) = a_{d_k} - a_{m_k}\\). The “significant” diagnostic statistic is \\(g(a_{d_k}) = \\mathbf{I}[p(a_{d_k}) \\leq \\alpha]\\), where \\(p()\\) is a function of the empirical answer that returns a \\(p\\)-value, \\(\\alpha\\) is a significance cutoff, and \\(\\mathbf{I}\\) is an indicator function that returns 1 when the \\(p\\)-value is below the cutoff and \\(0\\) when it is above. A diagnostic statistic is something that can be calculated on the basis of a single run of the design.\nTypically, diagnostic statistics will be different from simulation to simulation In other words, \\(\\phi_1\\) will be different from \\(\\phi_2\\), \\(\\phi_2\\) will be different from \\(\\phi_3\\), and so on. These differences arise partially from the variation in M: \\(m_1\\) is different from \\(m_2\\), \\(m_2\\) is different from \\(m_3\\), and so on. Differences can also arise from the explicitly random procedures in \\(D\\): sampling, assignment, and measurement can all include stochastic elements that will ramify through to the diagnostic statistics. As a result of these sources of variation, a diagnostic statistic is a random variable \\(\\Phi\\).\n\n11.1.2 Diagnosands\nA diagnosand is a summary of the distribution of \\(\\Phi\\), written \\(f(\\Phi)\\). For example, the expectation function \\(\\E[\\Phi]\\) reports the expectation (the mean) of \\(\\Phi\\).\nLet’s back up a moment to work through two concrete examples of common diagnosands: bias and power (see Section (types-of-diagnosands?) for a more exhaustive list).\nConsider the diagnosand “bias” in the context of a two-arm randomized trial where the inquiry is the average treatment effect, the data strategy entails complete random assignment, and the answer strategy is the difference-in-means. Bias is the average difference between the estimand and the estimate. Under a single realization \\(m_k\\) of the model M, the value of the ATE will be a particular number, which we call \\(a_{m_k}\\). We simulate a random assignment and measurement of observed outcomes, then apply the difference-in-means estimator. The diagnostic statistic is the error \\(a_{d_k} - a_{m_k}\\); this error is a random variable because each \\(m_k\\) differs slightly. The bias diagnosand is the expectation of this random variable is \\(\\E[a_{d_k} - a_{m_k}]\\), where the expectation is taken over the randomization distribution implied by M and D (or distinct regions of the randomization distribution).\nNow consider the diagnosand “power.” Like bias, statistical power is an expectation, this time of the “significant” diagnostic statistic \\(\\mathbf{I}(p(a_{d_k}) \\leq 0.05)\\). Power describes how frequently the answer strategy will return a statistically significant result. Some textbooks define statistical power as one minus the Type II error rate, where a Type II error is the failure to reject the null hypothesis, given that the null hypothesis is false. This definition is accurate, but hard to understand. The phrase “given that the null hypothesis is false” refers to model possibilities (\\(a_{m_k}\\)’s) in which the null hypothesis does not hold. Our definition of power is instead, “the probability of getting a statistically significant result, conditional on a set of beliefs about the model.”\nUncertainty statistics like standard errors can be thought of as empirical estimates of diagnosands describing the quality of your design under some set of models.\nSuppose we report that the estimate is equal to 15 and carries with it a standard error of 6. Here we are communicating that we now think our design has the property that, if we were to run it over and over, the standard deviation of our estimates would be 6. That is, if we were to declare a design over a large class of models that all have the same outcome variance and combined it with I, D, and A, and diagnosed, the standard deviation of the estimates under all those models would be 6.\nLikewise, estimated \\(p\\)-values can also be thought of as estimates of a peculiar diagnosand: the probability of obtaining a test statistic of a particular value, under a maintained null model. That is, if we were to write down a null model \\(m_{0}\\) under which the estimand were in fact zero, then combined it with I, D, and A, we could diagnose that design to learn the fraction of estimates that are as large or larger than 15—a \\(p\\)-value.\nThinking about \\(p\\)-values as diagnosands can be seen most directly in the randomization inference approach to hypothesis testing. Randomization inference often considers “sharp” null hypothesis in which we impute the missing potential outcomes with exactly their observed values, then simulate all the ways the design could come out holding I, D, and A constant. The \\(p\\)-value is the fraction of simulations in which the null model generates estimates that are more extreme than the observed estimate. See Section (illustration-of-estimates-of-uncertainty-using-the-aayr-principle?) for a worked example of this approach.\nEven frequentist confidence intervals, with their notoriously confusing interpretation (a 95% confidence interval is an interval that has the goal of covering the estimand 95% of the time) can be viewed as estimates of the 2.5th and 97.5th quantiles of the sampling distribution under the model that the estimand equals the estimate.\nThe payoff from thinking about uncertainty statistics as estimates of diagnosands is that uncertainty statistics can be poor estimators of diagnosands. For example, social scientists criticize one another’s choice of standard error—classical or robust, clustered or not, bootstrapped, jackknifed, and on and on. The reason for this debate is that when the procedures we follow to form uncertainty statistics are inappropriate for the design setting, we can be falsely confident in our answers.\nFigure 11.2 illustrates what’s at stake in the age old contest between classical standard errors and robust standard errors. Here we are in the context of a two arm trial under three settings: the control potential outcomes are higher variance, the groups have the same variances, or the treated outcomes have higher variance. The calculation for classical standard errors pools the data from both groups when estimating a variance, thereby assuming “homoskedasticity,” a Greek work for having the “same spread.” This estimation choice leads to poor performance. Depending on the fraction of the sample that receive treatment, the estimate of the sampling distribution can be upwardly biased (conservative) or downwardly biased (anti-conservative or falsely confident). We’re usually worried about standard error estimates being too small because anti-conservatism is probably worse for scientific communication than conservatism.\nRobust standard errors are “heteroskedasticity-robust,” which means that that the estimation does not assume that the two groups have the same error variance. Samii and Aronow (2012) show that a particular variant of robust standard errors (HC2) is exactly equal to the Neyman variance estimator described in Section (analytic-design-diagnosis?), which is why HC2 robust standard errors are default in the lm_robust function. The bottom row of Figure 11.2 shows that the robust standard error estimator hews closely to the true value of the diagnosand in all of the design settings considered. We prefer robust SEs because they do impose extra model assumptions like homoskedasticity that are not grounded in any particular design feature.\n\n\nFigure 11.2: Why robust standard errors are preferred to classical standard errors"
  },
  {
    "objectID": "declaration-diagnosis-redesign/diagnosing-designs.html#types-of-diagnosands",
    "href": "declaration-diagnosis-redesign/diagnosing-designs.html#types-of-diagnosands",
    "title": "\n11  Diagnosing designs\n",
    "section": "\n11.2 Types of diagnosands",
    "text": "11.2 Types of diagnosands\nAs described above, a diagnostic statistic is any summary function of \\(a_m\\) and \\(a_d\\), and a diagnosand is any summary of the distribution of diagnostic statistics. As a result, there are a great many diagnosands researchers may consider. In Table ?tbl-diagnosticstatistics, we introduce a nonexhaustive set of diagnostic statistics, and in Table ?tbl-diagnosands a nonexhaustive set of diagnosands.\n\n(#tab:diagnosticstatistics) Examples of diagnostic statistics.\n\n\n\n\n\nDiagnostic statistic\nDefinition\n\n\n\nEstimate\n\\(a_{d_k}\\)\n\n\nEstimand under the model\n\\(a_{m_k}\\)\n\n\n\n\\(p\\)-value\n\\(p(a_{d_k})\\)\n\n\n\n\\(p\\)-value is no greater than \\(\\alpha\\)\n\n\\(\\mathbf{I}(p \\leq \\alpha)\\)\n\n\nConfidence interval\n\\(\\mathrm{CI}_{1-\\alpha}\\)\n\n\nConfidence interval covers the estimand under the model\n\\(\\mathrm{covers}_{a_m} \\equiv \\mathbb{I}\\{ a_m \\in \\mathrm{CI}_{1-\\alpha} \\}\\)\n\n\nEstimated standard error\n\\(\\widehat\\sigma(A)\\)\n\n\nCost\n\\(\\mathrm{cost}\\)\n\n\nProportion of subjects harmed\n\\(\\Pr(\\mathrm{harm}) \\equiv \\frac{1}{n} \\sum_i \\mathrm{harm_i}\\)\n\n\n\n\n(#tab:diagnosands) Examples of iagnosands.\n\n\n\n\n\n\nDiagnosand\nDescription\nDefinition\n\n\n\nAverage estimate\n\n\\(\\E(a_d)\\)\n\n\nAverage estimand\n\n\\(\\E(a_m)\\)\n\n\nPower\nProbability of rejecting null hypothesis of no effect\n\\(\\E(I(p \\leq \\alpha))\\)\n\n\nNull risk\nProbability of failing to reject null hypothesis of no effect\n\\(\\E(I(p > \\alpha))\\)\n\n\nBias\nExpected difference between estimate and estimand\n\\(\\E(a_d - a_m)\\)\n\n\nVariance of the estimates\n\n\\(\\V(a_d)\\)\n\n\nTrue standard error\n\n\\(\\sqrt{\\V(a_d)}\\)\n\n\nAverage estimated standard error\n\n\\(\\widehat\\sigma(A)\\)\n\n\nRoot mean-squared-error (RMSE)\n\n\\(\\sqrt{\\E(a_d - a_m)}\\)\n\n\nCoverage\nProbability confidence interval overlaps estimand\n\\(\\Pr(\\mathrm{covers}_{a_m})\\)\n\n\nBias‐eliminated coverage\nProbability confidence interval overlaps average estimate (Morris, White, and Crowther (2021))\n\\(\\Pr(\\mathrm{covers}_{a_d})\\)\n\n\nType-S error rate\nProbability estimate has an incorrect sign, if statistically significant (Gelman and Carlin 2014)\n\n\\(\\Pr(\\mathrm{sgn}(a_d) \\neq \\mathrm{sgn}(a_m) \\mid p \\leq \\alpha)\\)\n\n\nExaggeration ratio\nExpected ratio of absolute value of estimate to estimand, if statistically significant (Gelman and Carlin 2014)\n\n\\(\\E( a_d / a_m \\mid p \\leq \\alpha)\\)\n\n\nType I error\nRejecting the null hypothesis when it is true\n\\(\\Pr(p \\leq \\alpha \\mid a_m = a^0)\\)\n\n\nType II error\nFailure to reject the null hypothesis when it is false\n\\(\\Pr(p \\geq \\alpha \\mid a_m \\neq a^0)\\)\n\n\nSampling bias\nExpected difference between population average treatment effect and sample average treatment effect (Imai, King, and Stuart 2008)\n\n\\(\\E(a_{m_{\\mathrm{sample}}} - a_{m_{\\mathrm{population}}})\\)\n\n\nExpected maximum cost\nMaximum cost across possible realizations of the study\n\\(\\max{\\mathrm{cost}}\\)\n\n\nBayesian learning\nDifference between prior and posterior guess of the value of the estimand\n\\(a_{m_{\\mathrm{post}}} - a_{m_{\\mathrm{pre}}}\\)\n\n\nValue for money\nProbability that a decision based on estimated effect yields net benefits\n\n\n\nSuccess\nQualitative assessment of the success of a study\n\n\n\nMinimum detectable effect (MDE)\nSmallest effect size for which the power of the design is nominal (e.g., powered at 80%)\n\\(\\mathrm{argmin}_{a_{m_*}} \\Pr(p \\leq \\alpha) = 0.8\\)\n\n\nRobustness\nJoint probability of rejecting the null hypothesis across multiple tests\n\n\n\nMaximum proportion of subjects harmed\n\n\\(\\max{\\Pr(\\mathrm{harm})}\\)"
  },
  {
    "objectID": "declaration-diagnosis-redesign/diagnosing-designs.html#estimation-of-diagnosands",
    "href": "declaration-diagnosis-redesign/diagnosing-designs.html#estimation-of-diagnosands",
    "title": "\n11  Diagnosing designs\n",
    "section": "\n11.3 Estimation of diagnosands",
    "text": "11.3 Estimation of diagnosands\n\n11.3.1 Analytic design diagnosis\nDiagnosis can be done with analytic, pencil-and-paper methods. In an analytic design diagnosis, we typically derive a formula that returns the value of a diagnosand under a stipulated set of beliefs about the model, inquiry, data strategy, and answer strategy. For example, research design textbooks often contain analytic design diagnoses for statistical power. Gerber and Green (2012) write:\n\n“To illustrate a power analysis, consider a completely randomized experiment where \\(N>2\\) of \\(N\\) units are selected into a binary treatment. The researcher must now make assumptions about the distributions of outcomes for treatment and for control units. In this example, the researcher assumes that the control group has a normally distributed outcome with mean \\(\\mu_c\\), the treatment group has a normally distributed outcome with mean \\(\\mu_t\\), and both group’s outcomes have a standard deviation \\(\\sigma\\). The researcher must also choose \\(\\alpha\\), the desired level of statistical significance (typically 0.05). Under this scenario, there exists a simple asymptotic approximation for the power of the experiment (assuming that the significance test is two-tailed): \\[\n\\beta = \\Phi \\bigg(\\frac{|\\mu_t - \\mu_c| \\sqrt{N}}{2\\sigma} - \\Phi^{-1} \\left(1 - \\frac{\\alpha}{2}\\right) \\bigg)\n\\] where \\(\\beta\\) is the statistical power of the experiment, \\(\\Phi(\\cdot)\\) is the normal cumulative distribution function (CDF), and \\(\\Phi^{-1}(\\cdot)\\) is the inverse of the normal CDF.”\n\nThis power formula makes detailed assumptions about M, I, D, and A. Under M, it assumes that both potential outcomes are normally distributed with group specific means and a common variance. Under I, it assumes the average treatment effect. Under D, it assumes a particular randomization strategy (simple random assignment). Under A, it assumes a particular hypothesis testing approach (equal variance \\(t\\)-test with \\(N - 2\\) degrees of freedom). Whether this set of assumptions is “close enough” will depend on the research setting.\nAnalytic design diagnosis can be hugely useful, since they cover a large families of designs that meet the scope criteria. For example, the “standard error” diagnosand \\(\\sqrt{\\E[(a_{d_k} - \\E[a_{d_k}])^2]}\\) of a standard two-arm trial has been worked out by statisticians to be \\(\\sqrt{\\frac{1}{n-1}\\left\\{\\frac{m\\V(Y_i(0))}{n-m} + \\frac{(N-m)\\V(Y_i(1))}{m} + 2Cov(Y_i(0), Y_i(1))\\right\\}}\\) (see Section (analytic-design-diagnosis?) for details). This standard error is accurate for any completely randomized design with stable potential outcomes and a difference-in-means estimator. Many, if not most, advances in our understanding of the properties of research design come from analytic design diagnosis. For example, Middleton (2008) and Imai, King, and Nall (2009) show that cluster randomized trials with heterogeneous cluster sizes are not unbiased for the ATE, which leads to the design recommendation that clustered trials should block on cluster size. These lessons apply broadly, more broadly perhaps than the lessons learned about a specific design in a specific Monte Carlo simulation.\nThat said, scholars conducting analytic design diagnosis have only worked out a few diagnosands for a limited set of designs. Since designs are so heterogeneous and can vary on so many dimensions, computer simulation is often the only feasible way to diagnose. We learn a lot from analytic design diagnosis—what are the important parameters to consider, what are the important inferential problems—but they often cannot provide direct answers to practical questions like, how many subjects do I need for my conjoint experiment? For that reason, we turn to design diagnosis via Monte Carlo simulation.\n\n11.3.2 Design diagnosis by simulation\nResearch design diagnosis by simulation occurs in two steps. First we simulate research designs repeatedly, collecting diagnostic statistics from each run of the simulation. Second, we summarize the distribution of the diagnostic statistics in order to estimate the diagnosands.\nMonte Carlo simulations of research designs can be written in any programming language. To illustrate the most common way of simulating a design—a for loop—we’ll write a concise simulation in base R code. Diagnosis (lem:diagnosis-10-1?) conducts the simulation 500 times, each time, collecting the \\(p\\)-value associated with a regression estimate of the average effect of an outcome on a treatment.\nLoops like this one can be implemented in any language and they remain a good tool for design diagnosis. We think, however, writing simulations in this way obscures what parts of the design refer to the model, the inquiry, the data strategy, or the answer strategy. We might imagine Z and Y are generated by a data strategy that randomly assigns Z, then measures Y (though without a language for linking code to design steps, it’s a bit unclear). The answer strategy involves running a regression of Y on Z, then conducting a hypothesis test against the null hypothesis that the coefficient on Z is 0. The model and inquiry are left entirely implicit. The inquiry might be the ATE, but it might also be a question of whether the ATE is equal to zero. The model might include only two potential outcomes for each subject, or it might have more, we don’t know.\n\n\nDiagnosis 11.1 (By-hand diagnosis example) \nsims <- 500\np.values <- rep(NA, sims)\n\nfor(i in 1:sims){\n  Z <- rbinom(100, 1, 0.5)\n  U <- rnorm(100)\n  Y <- 0.2 * Z + U\n  p.values[i] <- summary(lm(Y ~ Z))$coefficients[2, 4]\n}\n\npower <- mean(p.values <= 0.05)\npower\n\n\n\n\n\nBy-hand design diagnosis with a for-loop\n \n power \n  \n\n 0.16 \n  \n\n\n\n\nFor this reason, we advocate for explicit description of all four research design components. Explicit design declaration can also occur in any programming language, but DeclareDesign is purpose-built for this task. We begin with the explicit declaration of the same two-arm trial as above. We have 100 subjects with a constant response to treatment of 0.2 units. Our inquiry is the average difference between the treated and untreated potential outcomes—the ATE. We assign treatment using complete random assignment and estimate treatment effects using difference-in-means.\n\nDeclaration 11.1 Example of declaration using DeclareDesign\n\ndeclaration_10.1 <-\n  declare_model(\n    N = 100,\n    U = rnorm(N),\n    potential_outcomes(Y ~  0.2 * Z + U)\n  ) +\n  declare_inquiry(ATE = mean(Y_Z_1 - Y_Z_0)) +\n  declare_assignment(Z = complete_ra(N)) +\n  declare_measurement(Y = reveal_outcomes(Y ~ Z)) +\n  declare_estimator(Y ~ Z, inquiry = \"ATE\")\n\n\n\nDiagnosis 11.2 (Diagnosis in DeclareDesign) We can now diagnose the declared design. For comparison with the loop, we calculate just one diagnosand: statistical power. Both approaches return approximately the same answer. Any difference between the two can be attributed to simulation error, which is why DeclareDesign by default returns a bootstrapped standard error for each diagnosand estimate. If we were to increase the number of simulations for both approaches dramatically (100,000 simulations would be plenty), the differences would vanish.\n\ndiagnosands <- declare_diagnosands(power = mean(p.value <= 0.05))\n\ndiagnosis <- \n  diagnose_design(declaration_10.1, \n                  diagnosands = diagnosands)\n\ndiagnosis\n\n\n\n\n\n\n\n\nDesign diagnosis using DeclareDesign\n \n power \n    se(power) \n    n_sims \n  \n\n 0.1615 \n    0.0083482 \n    2000 \n  \n\n\n\n\nBacking up, we now construct that same diagnosis step-by-step. We build up from a single simulation run of the design to the distribution of simulations, to summaries of that distribution.\n\nset.seed(343)\n\nsimulation_10.1 <- \n  simulate_design(declaration_10.1, sims = 10) |>\n  mutate(significant = p.value <= 0.05)\n\n\n\n\nWe can run this simulation a single time with run_design:\n\nrun_design(declaration_10.1)\n\n\n\n\n\nOne simulation draw\n \n estimand \n    estimate \n    std.error \n    df \n    p.value \n    conf.low \n    conf.high \n  \n\n 0.2 \n    0.278 \n    0.209 \n    98 \n    0.186 \n    -0.136 \n    0.692 \n  \n\n\n\nFigure 11.3 shows the information we might obtain from a single run of the simulation. The filled point is the estimate \\(a_{d_k}\\). The open triangle is the estimand \\(a_{m_k}\\). The bell-shaped curve is our normal-approximation based estimate of the sampling distribution. The standard deviation of this estimated distribution is our estimated standard error, which expresses our uncertainty. The confidence interval around the estimate is another expression of our uncertainty: We’re not sure where \\(a_{d_k}\\) is, but if things are going according to plan, confidence intervals constructed this way will bracket \\(a_{m_k}\\) 95% of the time.\n\n\nFigure 11.3: Visualization of one draw from a design diagnosis.\n\n\nFrom this single draw, we can’t yet estimate diagnosands, but we can calculate diagnostic statistics. The estimate was higher than the estimand in this draw, so the error is 0.45 - 0.20 = 0.25. Likewise, the squared error is (0.45 - 0.20)\\(^2\\) = 0.0625. The \\(p\\)-value is 0.01, which is well below the threshold of 0.05, so the “statistical significance” diagnostic statistic is equal to TRUE. The confidence interval stretches from 0.11 to 0.79, and since the value of the estimand (0.20) is between those bounds, the “covers” diagnostic statistic is equal to TRUE as well.\nTo calculate the distributions of the diagnostic statistics, we have to simulate designs not just once, but many times over. The bias diagnosand is the average error over many runs of the simulation. The statistical power diagnosand is the fraction of runs in which the estimate is significant. The coverage diagnosand is the frequency with which the confidence interval covers the estimand.\nFigure 11.4 visualizes 10 runs of the simulation. We can see that some of the draws produce statistically significant estimates (the shaded areas are small and the confidence intervals don’t overlap zero), but not all. We get a sense of the true standard error by seeing how the point estimates bounce around. We get a feel for the difference between the estimates of the standard error and true standard error. Design diagnosis is the process of learning about many ways the study might come out, not just the one way that it will.\n\n\nFigure 11.4: Visualization of ten draws from a design diagnosis.\n\n\nThis line of code does it all in one. We simulate the design (500 times by default) to calculate the diagnostic statistics, then we summarize them in terms of bias, the true standard error, RMSE, power, and coverage.\n\n\nDiagnosis 11.3 (Design diagnosis in one step) \ndiagnosis_10.3 <-\n  diagnose_design(\n    declaration_10.1,\n    diagnosands = declare_diagnosands(\n      bias = mean(estimate - estimand),\n      true_se = sd(estimate),\n      power = mean(p.value <= 0.05),\n      coverage = mean(estimand <= conf.high &\n                        estimand >= conf.low)\n    )\n  )\n\n\n\n\n\n\n\n\nDiagnosand estimates with bootstrapped standard errors in parentheses\n \n bias \n    true se \n    power \n    coverage \n  \n\n -0.00 (0.00) \n    0.20 (0.00) \n    0.17 (0.01) \n    0.96 (0.00) \n  \n\n\n\n\n\n11.3.3 Simulation error\nWe use Monte Carlo simulation to estimate diagnosands. We only get estimates—not the exact value—of diagnosands under a particular model because of simulation error. Simulation error declines as we conduct more simulations. When we conduct many thousands of simulations, we’re relatively certain of the value of the diagnosand under the model. If we conduct just tens of simulations, we are much more uncertain.\nWe can characterize our uncertainty attending to the diagnosand estimate by calculating a standard error. If the Monte Carlo standard error is large relative to the estimated diagnosand, then we need to increase the number of simulations. Unlike empirical settings where additional data collection can be very expensive, in order to get more observations of diagnostic statistics, we can just increase the number of simulations. Computation time isn’t free—but it’s cheap.\nThe standard error of diagnosand estimates can be calculated using standard formulas. If the diagnosand can be written as a population mean, and the simulations are fully independent, then we can estimate the standard error as \\(\\frac{\\mathrm{sd}(\\phi)}{\\sqrt{k}}\\), where \\(k\\) is the number of simulations. The power diagnosand summarizes a binary diagnostic statistic (is the estimate significant or not). Binary variables exhibit maximum variability when the probability of a 1 is 0.5. So with 1,000 independent simulations, the standard error for the mean of a binary diagnostic statistic is \\(\\frac{\\sqrt{0.5 \\times 0.5} }{\\sqrt{1000}} = 0.016\\). This level of simulation uncertainty is often acceptable (the 95% confidence interval is approximately 4 * 0.016 = 6.4 percentage points wide), but if it isn’t, you can always increase the number of simulations.\nSince some diagnosands are more complex than a population mean (i.e., we can’t characterize the estimation error with simple formulas), so diagnose_design() function uses the nonparametric bootstrap."
  },
  {
    "objectID": "declaration-diagnosis-redesign/diagnosing-designs.html#how-to-diagnose-designs",
    "href": "declaration-diagnosis-redesign/diagnosing-designs.html#how-to-diagnose-designs",
    "title": "\n11  Diagnosing designs\n",
    "section": "\n11.4 How to diagnose designs",
    "text": "11.4 How to diagnose designs\n\n11.4.1 Exploring design tradeoffs\nChoosing among designs means choosing which diagnosands are important—and not all diagnosands are equally important for every study. For example, in a descriptive study whose goal is to estimate the fraction of people in France who are left-handed, statistical power is irrelevant. A hypothesis test against the null hypothesis that zero percent of the people in France are left-handed is preposterous. We know for sure that the fraction is not zero, we just don’t know its precise value. A much more important diagnosand for this study would be RMSE (root-mean-squared-error), which is a measure of how well-estimated the estimand is that incorporates both bias and variance.\nOften, we need to look at several diagnosands in order to understand what might be going wrong. If our design exhibits “undercoverage” (e.g., a procedure for constructing “95%” confidence interval only covers the estimand 50% of the time), that might be because the standard errors are too small or because the point estimator is biased, or some combination of the two. In really perverse instances, we might have a biased point estimator which, thanks to overly-wide confidence intervals, just happens to cover 95% of the time.\nMany research design decisions involve trading off bias and variance. In trade-off settings, we may need to accept higher variance in order to decrease bias. Likewise, we may need to accept a bit of bias in order to achieve lower variance. The tradeoff is captured by mean-squared error, which is the average squared distance between \\(a_d\\) and \\(a_m\\). Of course, we would ideally like to have as low a mean-squared error as possible, that is, we would like to achieve low variance and low bias simultaneously.\nTo illustrate, consider the following three designs as represented by three targets. The inquiry is the bullseye of the target. The data and answer strategies combine to generate a process by which arrows are shot towards the target. On the left, we have a very bad archer: even though the estimates are unbiased in the sense that they hit the bullseye “on average”, very few of the arrows are on target. In the middle, we have an excellent shot: they are both on target and low variance. On the right, we have an archer who is very consistent (low variance) but biased. The mean squared error is highest on the left and lowest in the middle.\n\n\nFigure 11.5: Visualization of the bias and variance of three ‘estimators’ of the bullseye\n\n\nThe archery metaphor is common in research design textbooks because it effectively conveys the difference between variance and bias, but it does elide an important point. It really matters which target the archer is shooting at. Figure 11.6 shows a bizarre double-target representing two inquiries. The empirical strategy is unbiased and precise for the left inquiry, but it is clearly biased for the right inquiry. When we are describing the properties of an answer strategy, we have to be clear about which inquiry it is associated with.\n\n\nFigure 11.6: The bias, variance, and RMSE of an answer strategy depend on the inquiry.\n\n\nMSE is an exactly equal weighting of variance and bias (squared). Yet many other weightings of these two diagnosands are possible, and different researchers will vary in their weightings.\nIn evaluating a research design diagnosis, what understand the weighting of all relevant diagnosands. We can think of this as our research design utility function. Our utility function describes how important it is to study big questions, to shift beliefs in a research field, to overturn established findings, to obtain unbiased answers, and to get the sign of the inquiry right. When we compare across empirical designs, we compare them on this utility function that aggregates all the diagnosands according to their importance.\nFor example, we too often consider the diagnosand power on its own. This diagnosand is the probability of getting a statistically significant result, which of course depends on many things about your design including, crucially, the unknown magnitude of the parameter to be estimated. But considering power alone is also misleading: no researcher wants to design a study that is 80% powered but returns highly biased estimates. Another way of saying this is that researchers always care about both power and bias. How much they care about each feature determines the weight of power and bias in their utility function.\nDiagnosands need not be about hypothesis testing or even statistical analysis of the data at all. We often trade off how much we learn from a research design with its cost in terms of money and our time. We have financial and time budgets that provide hard constraints to our designs, but we also at the margin many researchers wish to select cheaper (or shorter) designs in order to carry out more studies or finish their degree sooner. Time and cost are also diagnostic statistics! We may wish to explore the maximum cost of a study or the average amount of time it would take.\nEthical considerations also often enter the process of assessing research designs, if implicitly. We can explicitly incorporate them into our utility function by caring about harm to subjects and the degree of informed consent. When collecting data, researchers often encounter a tradeoff between informing subjects about the purpose of the study (an ethical consideration, or a requirement of the IRB) on the one hand and the bias that comes from Hawthorne or demand effects on the other. We can incorporate these considerations in a research design diagnosis by specifying diagnostic statistics related to the amount of disclosure about the purposes of research or the number of subjects harmed in the research. See Section (illustration-estimating-expected-costs-and-expected-learning?) for an example.\n\n11.4.2 Diagnosis under model uncertainty\nWe are always uncertain about the model in M. If we were certain of M (or there was no real dispute about it), there would be no need to conduct new empirical research. Research design diagnosis can incorporate this uncertainty by evaluating the performance of the design under alternative models. For example, if we are unsure of the exact value of the intra-class correlation (ICC), we can simulate the design under a range of plausible ICC values. If we are unsure of the true average treatment effect, we can diagnose the power of the study over a range of plausible effect sizes. Uncertainty over model inputs like the means, variances, and covariances in data that will eventually be collected is a major reason to simulate under a range of plausible values.\nWe illustrate diagnosis under model uncertainty with the declaration below. Here we have a 200 unit two-arm trial in which we explicitly describe our uncertainty over the value of the true average treatment effect. In the potential_outcomes call, we have Y ~ runif(1, 0, 0.5) * Z + U which indicates that the treatment effect in each run of the simulation is one draw from uniform distribution between 0.0 and 0.5.\n\nDeclaration including model uncertainty\n\ndeclaration_10.2 <-\n  declare_model(\n    N = 200, U = rnorm(N),\n    # this runif(n = 1, min = 0, max = 0.5) \n    # generates 1 random ATE between 0 and 0.5\n    potential_outcomes(Y ~ runif(n = 1, min = 0, max = 0.5) * Z + U)) +\n  declare_inquiry(ATE = mean(Y_Z_1 - Y_Z_0)) +\n  declare_assignment(Z = complete_ra(N, prob = 0.5)) +\n  declare_measurement(Y = reveal_outcomes(Y ~ Z)) +\n  declare_estimator(Y ~ Z, inquiry = \"ATE\")\n\n\n\n\nDiagnosis 11.4 (Diagnosis under model uncertainty) \n\nFigure 11.7 shows how the statistical power of this design varies over the set of model possibilities. We plot the possible effect sizes we entertain in the model on the horizontal axis and the statistical power on the vertical axis. We also plot a loess curve flexibly smooths over the full distribution of effect sizes. We see that at low levels of the effect size, statistical power is quite poor. With only 200 subjects, we couldn’t achieve 80% statistical power unless the true effect size were approximately 0.45 standard deviations. The effect size at which a design achieves 80% power is often referred to as the minimum detectable effect size (MDE). This exercise shows how the MDE diagnosand is a summary of a design that admits uncertainty over the effect size. We return to this example in Section (redesigning-under-model-uncertainty?), when we redesign this study to learn how the MDE changes at different sample sizes.\nWhat is “the” power of this design? Under one view, the true power of the design is the whichever value for power is associated with the effect size. But under an agnostic view, the ex ante “power” of the design is a weighted average of all these power values, weighted by the researcher’s prior beliefs over the distribution of possible effect sizes.\n\n\nFigure 11.7: Diagnosing an experiment over uncertainty about the true effect size\n\n\n\n\n11.4.3 Adjudicating between competing models\nThe principle to design agnostically extends to competing models. Imagine that you believe \\(M1\\) is true but that your scholarly rival believes \\(M2\\). Suppose that under \\(M1\\), the treatment affects \\(Y1\\) but not \\(Y2\\). Your rival posits the reverse: Under \\(M2\\): the treatment affects \\(Y2\\) but not \\(Y1\\). In the spirit of scientific progress, the both of you engage in an “adversarial collaboration.” You design a study together. The design you choose should, first, demonstrate \\(M1\\) is true if it is true and, second, demonstrate \\(M2\\) is true if it is true. In order to come to an agreement about the properties of the design, you will need to simulate the design under both models.\n\nDeclaration of competing models\n\nM1 <-\n  declare_model(\n    N = 200,\n    U = rnorm(N),\n    potential_outcomes(Y1 ~ 0.2 * Z + U),\n    potential_outcomes(Y2 ~ 0.0 * Z + U)\n  )\n\nM2 <-\n  declare_model(\n    N = 200,\n    U = rnorm(N),\n    potential_outcomes(Y1 ~ 0.0 * Z + U),\n    potential_outcomes(Y2 ~ 0.2 * Z + U)\n  )\n\nIDA <- \n  declare_inquiry(ATE1 = mean(Y1_Z_1 - Y1_Z_0),\n                  ATE2 = mean(Y2_Z_1 - Y2_Z_0)) +\n  declare_assignment(Z = complete_ra(N)) +\n  declare_measurement(Y1 = reveal_outcomes(Y1 ~ Z),\n                      Y2 = reveal_outcomes(Y2 ~ Z)) +\n  declare_estimator(Y1 ~ Z, inquiry = \"ATE1\", label = \"DIM1\") +\n  declare_estimator(Y2 ~ Z, inquiry = \"ATE2\", label = \"DIM2\")\n\ndeclaration_10.3a <- M1 + IDA\ndeclaration_10.3b <- M2 + IDA\n\n\n\nDiagnosis 11.5 (Diagnosis of competing models) To diagnose these alternative models, we count up how frequently each perspective receives support. If we define support by statistical significance (other metrics are of course possible), then your model is supported if the effect of treatment on \\(Y1\\) is significant but the effect on \\(Y2\\) is not significant. If the reverse pattern is obtained, your rival can claim victory. Two kinds of split decisions are possible: neither estimate is significant, or both are. By simulation, we can estimate the rates of these four possibilities, both under your beliefs and those of your theoretical adversary.\n\ndiagnosis_10.5 <-\n  diagnose_design(\n    list(declaration_10.3a, declaration_10.3b)\n  )\n\n\n\n\n\n\n\n\n\nDesign diagnosis under two alternative theories\n\n\n\ndesign\n\n\nOnly theory 1 supported\n\n\nOnly theory 2 supported\n\n\nBoth supported\n\n\nNeither supported\n\n\n\n\n\ndesign_1\n\n\n0.2610\n\n\n0.026\n\n\n0.0260\n\n\n0.687\n\n\n\n\ndesign_2\n\n\n0.0255\n\n\n0.260\n\n\n0.0255\n\n\n0.689\n\n\n\n\nDistribution of support for theories under two designs\n\n\nThe diagnosis shows that the study is responsive to the truth. When theory 1 is correct, the design is more likely to yield empirical evidence in favor of it; the reverse holds when theory 2 is correct. That said, the major concern facing this adversarial collaboration is that the study is too small to resolve the dispute. About two-thirds of the time—regardless of who is right!—neither theory receives support. This problem can be ameliorated either by elaborating more tests of each theoretical perspective or by increasing the size of the study."
  },
  {
    "objectID": "declaration-diagnosis-redesign/diagnosing-designs.html#summary",
    "href": "declaration-diagnosis-redesign/diagnosing-designs.html#summary",
    "title": "\n11  Diagnosing designs\n",
    "section": "\n11.5 Summary",
    "text": "11.5 Summary\nDiagnosis is the process of estimating the properties of research designs under uncertain beliefs about the world. We simulate under many alternative designs because we want to choose designs that are strong under a large range of model possibilities. Each run of a simulate generates a diagnostic statistic. We learn about the distribution of the diagnostic statistic by running the simulation repeatedly. Diagnosands are summaries of the distribution of diagnostic statistics. Which diagnosands are most important will vary from study to study, and depend on the relative weight you place on one diagnosand versus another. Analytic design diagnosis is possible and can be quite powerful—we nevertheless recommend full simulation of research designs in order to learn about a range of diagnosands.\n\n\n\n\n\n\n\n\n\n\nGelman, Andrew, and John Carlin. 2014. “Beyond Power Calculations Assessing Type s (Sign) and Type m (Magnitude) Errors.” Perspectives on Psychological Science 9 (6): 641–51.\n\n\nGerber, Alan S., and Donald P. Green. 2012. Field Experiments: Design, Analysis, and Interpretation. New York: W.W. Norton.\n\n\nImai, Kosuke, Gary King, and Clayton Nall. 2009. “The Essential Role of Pair Matching in Cluster-Randomized Experiments, with Application to the Mexican Universal Health Insurance Evaluation.” Statistical Science 24 (1): 29–53.\n\n\nImai, Kosuke, Gary King, and Elizabeth A. Stuart. 2008. “Misunderstandings Between Experimentalists and Observationalists about Causal Inference.” Journal of the Royal Statistical Society: Series A (Statistics in Society) 171 (2): 481–502.\n\n\nMiddleton, Joel A. 2008. “Bias of the Regression Estimator for Experiments Using Clustered Random Assignment.” Statistics & Probability Letters 78 (16): 2654–59.\n\n\nMorris, Tim P., Ian R. White, and Michael J. Crowther. 2021. “Using Simulation Studies to Evaluate Statistical Methods.” Statistics in Medicine 38 (11): 2074–2102.\n\n\nSamii, Cyrus, and Peter M. Aronow. 2012. “On Equivalencies Between Design-Based and Regression-Based Variance Estimators for Randomized Experiments.” Statistics & Probability Letters 82 (2): 365–70."
  },
  {
    "objectID": "declaration-diagnosis-redesign/design-example.html",
    "href": "declaration-diagnosis-redesign/design-example.html",
    "title": "\n12  Design example\n",
    "section": "",
    "text": "We illustrate the declare-diagnose-redesign framework with a study of political motivations among office-seekers in Pakistan. Gulzar and Khan (2021) conducted an experiment that estimated the effects of two alternative incentives for becoming a politician: helping the community or generating personal benefits. The researchers randomly assigned eligible citizens to receive different encouragements to stand for office and measured the rates of running for office, the types of people who chose to run, and the congruence of elected politicians’ policy positions with those of the general population.\nBy contrast with the stylized designs we’ve described to this point in the book, this design is moderately complex. We have multiple inquiries and a layered data strategy that has important implications for the answer strategy. The reason we selected it is that we want to show how one would actually apply the declare-diagnose-redesign framework in a complex, real world setting."
  },
  {
    "objectID": "declaration-diagnosis-redesign/design-example.html#declaration-in-words",
    "href": "declaration-diagnosis-redesign/design-example.html#declaration-in-words",
    "title": "\n12  Design example\n",
    "section": "\n12.1 Declaration in words",
    "text": "12.1 Declaration in words\nThe model describes the units under study: citizens who are eligible to run for office in villages in the study region. The model also includes citizens’ individual characteristics and their potential outcomes depending on which encouragement they receive. The model set includes four theories of political motivation: politicians respond only to encouragements that focus on themselves, only to encouragements that focus on others, to neither, or to both. Among theories that include room for both motivations, some claim that personal motivations are more powerful than community-minded motivations, while others claim the reverse. The potential outcomes are defined in terms of subjects’ underlying (“latent”) probability of running for office, which is tightly related to the binary choice to run or not to run.\nThe two inquiries for this study are the average treatment effects of each encouragement, defined as the average difference in potential outcomes between receiving and not receiving each encouragement to run. The authors consider a third inquiry — the difference between these two average treatment effects — but we’ll leave that complication to the side for the moment. It’s worth highlighting what the inquiry is not. The inquiry is not “why” do actual politicians run for office or what are the features of the job that attract candidates. The answers to the average treatment effect inquiries may shed light onto those questions, but not directly.\nThe data strategy for this study includes sampling, treatment assignment, and measurement. The sampling step takes place in two stages. First, the researchers sample 192 villages and then they sample 48 citizens who are eligible to stand for election from each village. In the assignment step, the authors allocate participants to a personal benefits encouragement, a prosocial encouragement, or no encouragement (control). All eligible citizen in a village are assigned to the same treatment condition, which is to say that this experiment used cluster random assignment. Lastly, in the measurement step, the authors record the decision to run for office by checking whether a participant’s name appears on the official candidate lists released by the Election Commission of Pakistan. In contrast to the latent probability outcome in the model, the outcome variable as measured by the data strategy is binary.\nThe answer strategy is an ordinary least squares regression of outcome variable on the treatment variable, with standard errors clustered at the village level. The clustering of the errors reflects the clustered assignment of treatments. This mirroring is an example of how choices in the answer strategy should reflect choices in the data strategy."
  },
  {
    "objectID": "declaration-diagnosis-redesign/design-example.html#declaration-in-code",
    "href": "declaration-diagnosis-redesign/design-example.html#declaration-in-code",
    "title": "\n12  Design example\n",
    "section": "\n12.2 Declaration in code",
    "text": "12.2 Declaration in code\n\nDeclaration 12.1 Gulzar and Khan (2021) design\nWith declare_model, we describe a hierarchical structure with 660 villages, each of which is home to many citizens who are eligible to run for elected office. Each citizen harbors three potential outcomes. Y_Z_neutral is the citizen’s latent probability of standing for election if treated with a neutral appeal, Y_Z_personal is the probability if treated with an appeal that emphasizes the personal returns to office, and Y_Z_social is the probability if treated with an appeal that underlines the benefits to the community. Our simplified model assumes a constant treatment effect of about 3 percentage points for the personal appeal and 4 percentage points for the social appeal.1\n\nmodel_12.1 <- \n  declare_model(\n    villages = add_level(N = 660, U_village = rnorm(N, sd = 0.1)),\n    citizens = add_level(\n      N = 100,\n      U_citizen = rnorm(N),\n      potential_outcomes(\n        Y ~ pnorm(\n          U_citizen + U_village +\n            0.10 * (Z == \"personal\") +\n            0.15 * (Z == \"social\")),\n        conditions = list(Z = c(\"neutral\", \"personal\", \"social\"))\n      )\n    )\n  )\n\nWe have two inquiries, representing the average treatment effects in the population for the personal and social appeals compared to the neutral appeal, defined as the average differences in potential outcomes:\n\ninquiry_12.1 <-\n  declare_inquiry(\n    ATE_personal = mean(Y_Z_personal - Y_Z_neutral),\n    ATE_social = mean(Y_Z_social - Y_Z_neutral)\n  )\n\nThe data strategy consists of four steps: sampling of villages, sampling of citizens, treatment assignment, and outcome measurement. In sampling, we sample 192 villages and 48 of the eligible citizens from each village. In assignment, we cluster assign 25% of the villages to the neutral condition, 37.5% to the personal appeal, and 37.5% to the social appeal. The measurement step maps the “revealed,” but still latent, probability of running to the observed binary choice to run or not.\n\nn_villages <- 192\ncitizens_per_village <- 48\n\ndata_strategy_12.1 <-\n  declare_sampling(\n    S_village = cluster_rs(clusters = villages, n = n_villages),\n    filter = S_village == 1) +\n  declare_sampling(\n    S_citizen = strata_rs(strata = villages, n = citizens_per_village),\n    filter = S_citizen == 1) +\n  declare_assignment(\n    Z = cluster_ra(\n      clusters = villages, \n      conditions = c(\"neutral\", \"personal\", \"social\"),\n      prob_each = c(0.250, 0.375, 0.375))) + \n  declare_measurement(\n    Y_latent = reveal_outcomes(Y ~ Z),\n    Y_observed = rbinom(N, 1, prob = Y_latent)\n  )\n\nThe answer strategy consists of an ordinary least squares regression (as implemented by lm_robust) of the outcome on the treatments. The standard errors are clustered at the village level in order to account for the clustering in the assignment procedure. The regression will return three coefficients: an intercept and two treatment effect estimates. We ensure that the estimators are mapped to the relevant inquiries by explicitly linking them.\n\nanswer_strategy_12.1 <- \n  declare_estimator(Y_observed ~ Z, term = c(\"Zpersonal\", \"Zsocial\"), \n                    clusters = villages, \n                    .method = lm_robust,\n                    se_type = \"stata\",\n                    inquiry = c(\"ATE_personal\", \"ATE_social\"))\n\nWhen we concatenate all four elements with the + operator, we get a design:\n\ndeclaration_12.1 <- model_12.1 + inquiry_12.1 + data_strategy_12.1 + answer_strategy_12.1\n\n\\(~\\)"
  },
  {
    "objectID": "declaration-diagnosis-redesign/design-example.html#diagnosis",
    "href": "declaration-diagnosis-redesign/design-example.html#diagnosis",
    "title": "\n12  Design example\n",
    "section": "\n12.3 Diagnosis",
    "text": "12.3 Diagnosis\nTo diagnose the design, we first define a set of diagnosands: bias, statistical power, the root mean-squared error, and total cost. The total cost calculation is in an arbitrary unit and reflects an assumption that sampling an additional village incurs a cost that is ten times larger than the cost of sampling an additional subject within a village.\n\ndiagnosands <-\n  declare_diagnosands(\n    bias = mean(estimate - estimand),\n    rmse = sqrt(mean((estimate - estimand) ^ 2)),\n    power = mean(p.value <= 0.05),\n    cost = mean(10 * n_villages + 1 * n_villages * citizens_per_village)\n  )\n\nWe then diagnose the design by simulating the design over and over, then calculating the diagnosands based on simulations data.\n\n\nDiagnosis 12.1 (Diagnosis of Gulzar and Khan (2022)) \ndiagnosis_12.1 <- diagnose_design(declaration_12.1, diagnosands = diagnosands)\n\n\n\n\n\n\n\n\nDiagnosis of the simplified Gulzar and Khan design.\n \n inquiry \n    term \n    bias \n    rmse \n    power \n  \n\n\n ATE_personal \n    Zpersonal \n    0 \n    0.014 \n    0.482 \n  \n\n ATE_social \n    Zsocial \n    0 \n    0.014 \n    0.839 \n  \n\n\n\n\nThe diagnosis reveals that the design is unbiased for both inquiries. The power of the design for the the social treatment is above the standard 80% threshold but it is not for the personal treatment. The table gives us a sense of what effect sizes the design is powered for, since the only difference in the design between these two inquiries is the assumed effect size in the model."
  },
  {
    "objectID": "declaration-diagnosis-redesign/design-example.html#redesign",
    "href": "declaration-diagnosis-redesign/design-example.html#redesign",
    "title": "\n12  Design example\n",
    "section": "\n12.4 Redesign",
    "text": "12.4 Redesign\nTwo of the most important design decisions in this study are the number of sampled villages and the number of sampled citizens per village. Due to the large fixed costs of traveling to each village, an additional sampled village is more expensive than an additional sampled citizen. In order to best allocate constrained study resources, we need understand the gains from changes to the data strategy along each margin. Here, we redesign the study across possible combinations of numbers of villages and citizens per village.\n\n\nDiagnosis 12.2 (Diagnosis of redesigned Gulzar and Khan (2022)) \ndiagnosis_12.2 <-\n  declaration_12.1 |>\n  redesign(n_villages = c(192, 500),\n           citizens_per_village = c(25, 50, 75, 100)) |>\n  diagnose_designs(diagnosands = diagnosands)\n\nIn Figure 12.1, we illustrate the results of our redesign exercise across all four diagnosands. The number of citizens per village is plotted on the horizontal axis and the value of the diagnosand is shown on the vertical axis. The plot is faceted by diagnosand and each line represents a different possible number of villages. We focus here on the social treatment only.\n\n\nFigure 12.1: Redesign of Gulzar and Khan (2020)\n\n\nWhat we see is that bias is invariant to these choices. The study is unbiased regardless of the number of villages and the number of citizens interviewed per village. However, our other three diagnosands do change. Power is increasing in the number of citizens per village, and is always higher with more villages. We might reject designs with 192 villages with only 25 citizens per village, because they fall below the 80% power threshold (in fact, the number chosen by the researchers, 48, is just over the threshold, suggesting they chose the most cost-effective design in terms of power). Root mean-squared error, a measure capturing both bias and efficiency of the design, is improving (decreasing) in the number of citizens per village and the number of villages. Cost is, of course, increasing in both sample size parameters. We can use the cost parameters to make decisions about what sample sizes to choose accounting both for scientific diagnosands of the design (i.e., power) and cost at the same time.\n\n\n\n\n\nGulzar, Saad, and Muhammad Yasir Khan. 2021. “‘Good Politicians:’ Experimental Evidence on Motivations for Political Candidacy and Government Performance.”"
  },
  {
    "objectID": "declaration-diagnosis-redesign/declaration-in-code.html",
    "href": "declaration-diagnosis-redesign/declaration-in-code.html",
    "title": "13  Designing in code",
    "section": "",
    "text": "The software package DeclareDesign provides tools for declaration, diagnosis, and redesign in code using the R statistical package. We introduce each step of the process sequentially in this chapter. The first goal is to get you started using the software for many common types of designs and to illustrate how the software works for a few less common ones. The second aim is to deepen understanding of the four elements of research design and our algorithm for selecting one. By working through each part in code, you get a second shot at grappling with how to build and assess research designs."
  },
  {
    "objectID": "declaration-diagnosis-redesign/declaration-in-code.html#model-code",
    "href": "declaration-diagnosis-redesign/declaration-in-code.html#model-code",
    "title": "13  Designing in code",
    "section": "\n13.1 Model",
    "text": "13.1 Model\nIn this section, we describe how to declare models in practice in the DeclareDesign code language. We start with declarations of units and the hierarchical structures that contain them, then move on to declarations of the characteristics of units. An important feature of models is the set of potential outcomes associated with each unit, so we spend some time describing a few approaches for thinking about them. This section is meant as a reference guide so it covers common settings and a few uncommon ones as well.\n\n13.1.1 Units\nThe model is first defined by the units under study. If there are 1,000 people who live in a city that you wish to learn about, but you don’t know anything else about them, you can declare:\n\nM <- declare_model(N = 1000)\n\nUnits often sit within multiple, sometimes overlapping geographic and social hierarchies. Households live on blocks that make up neighborhoods. Workers have jobs at firms and also often are represented by unions that sometimes represent workers in multiple firms (a non-nested hierarchy). These hierarchies are important to declare in the model as they often form the basis for why units are similar and different. Within declare_model, we define hierarchy using add_level, which adds a new level of hierarchy. This model declaration creates 100 households of varying size, then creates the appropriate number of individuals within households.\n\nM <- \n  declare_model(\n    households = add_level(\n      N = 100, \n      N_members = sample(c(1, 2, 3, 4), N, \n                         prob = c(0.2, 0.3, 0.25, 0.25), replace = TRUE)\n    ),\n    individuals = add_level(\n      N = N_members, \n      age = sample(18:90, N, replace = TRUE)\n    )\n  )\n\nPanel data have a different structure. For example, in a country-year panel dataset, we observe every country in every year. To create data with this structure, we first declare a country-level dataset, then a years-level dataset, then we join them. The join is accomplished in cross_levels call, which defines the variables we join by with by = join_using(countries, years). In cross_levels, we also create the observation-level outcome variable, which is a function in this case of a country shock, a year shock, an observation shock, and a time trend.\n\nM <- \n  declare_model(\n    countries = add_level(\n      N = 196, \n      country_shock = rnorm(N)\n    ),\n    years = add_level(\n      N = 100, \n      time_trend = 1:N,\n      year_shock = runif(N, 1, 10), \n      nest = FALSE\n    ),\n    observation = cross_levels(\n      by = join_using(countries, years),\n      observation_shock = rnorm(N),\n      Y = 0.01 * time_trend + country_shock + year_shock + observation_shock \n    )\n  )\n\n\n13.1.2 Unit characteristics\nWe can describe the characteristics of units with either by supplying existing data or generating simulated data.\nHere is an example of the simulation approach. We imagine 100 units with a characteristic X that is uniformly distributed between 0 and 100.\n\nM <- \n  declare_model(\n    N = 100, \n    X = runif(N, min = 0, max = 100)\n  )\n\nYou can use any of the enormous number of data simulation functions available in R for this purpose. Here we gather six functions we tend to use in our own declarations, but they are by no means exhaustive. Each function has arguments that govern exactly how the data are created; we chose arbitrary values here to show how they work. Figure 13.1 shows what these six look like for a 1,000 unit model.\n\nM <-\n  declare_model(\n    N = 1000,\n    X1 = rnorm(N, mean = 5, sd = 2),\n    X2 = runif(N, min = 0, max = 5),\n    X3 = rbinom(N, size = 1, prob = 0.5),\n    X4 = rbinom(N, size = 5, prob = 0.5),\n    X5 = rlnorm(N, meanlog = 0, sdlog = 1),\n    X6 = sample(c(1, 2, 3, 4, 5), N, replace = TRUE)\n  ) \n\n\n\nFigure 13.1: Six kinds of characteristics of units\n\n\nBinary variables are very important in the socials science, but they can be particularly tricky to make, so we’ll spend a little more time on them. In a common way of thinking, binary variables are translations from a latent, continuous variable into an observed binary outcomes.\nWe can draw binary outcomes in one of three ways. First, we can simulate a binary outcomes using the rbinom function as we have already seen. The function rbinom(N, size = 1, prob = 0.5) flips 1 coin for each of N subjects with a constant latent probability of success across units.\n\nM1 <- \n  declare_model(\n    N = 1000, \n    Y = rbinom(N, 1, prob = 0.5)\n  )\n\nIf you believe the latent probability varies across units, you might want to set up latent variable first before the call to rbinom. A major reason to do it this way is to build in correlation between the binary outcome Y and some other variable, like X in this model declaration.\n\nM2 <- \n  declare_model(\n    N = 1000, \n    latent = runif(N, min = 0, max = 1),\n    Y = rbinom(N, 1, prob = latent),\n    X = latent + rnorm(N)\n  )\n\nA third way to create binary variable skips the call to rbinom altogether and creates a binary variable by assessing whether the latent variable exceeds some threshold, here 0.75. A major reason to do this is when we want to control the sources of randomness. The latent variable is random because of the call to the runif function. If we pass the resulting latent probabilities to rbinom as in M2, then we add a second layer of randomness, the coin flips. If one layer is enough, then M3 might be a more appropriate model declaration.\n\nM3 <- \n  declare_model(\n    N = 1000, \n    latent = runif(N, min = 0, max = 1), \n    Y = if_else(latent > 0.75, 1, 0)\n  )\n\n\n13.1.2.1 Building in correlations between simulated variables\nMost social science variables are interrelated. The correlations between variables may affect the quality of a research design in many ways. If we control for a variable in a regression to improve power, how much power we gain depends on the correlation between that variable and the outcome.\nHere we walk through two main ways to create correlated variables. The first way is simply to make one variable a function of another:\n\nM1 <- \n  declare_model(\n    N = 1000,\n    X1 = rnorm(N),\n    X2 = X1 + rnorm(N)\n  )\n\nThe second way draws on an explicit draw from a multivariate distribution. The mvrnorm function in the MASS package generates draws from the multivariate normal distribution. We have to give it two means (mu) and a variance-covariance matrix (Sigma). The draw_multivariate function is a wrapper that makes these functions (that return more than one column of data!) play nicely with declare_model.\n\nM2 <-\n  declare_model(\n    draw_multivariate(c(X1, X2) ~ MASS::mvrnorm(\n      N = 1000,\n      mu = c(0, 0),\n      Sigma = matrix(c(1, 0.3, 0.3, 1), nrow = 2)\n    )))\n\n\n13.1.2.2 Building in correlations within clusters\nA second important form of correlation is correlation within clusters, described by the intra-cluster correlation coefficient (ICC). When ICC is low, the within-cluster differences are similar across clusters. When it is high, clusters are more homogeneous within themselves and more heterogeneous across themselves. For more on clustered designs for surveys and for experiments, see Section (cluster-random-sampling?) and Section (cluster-randomized-experiments?).\nWe can introduce ICC using the draw_normal_icc function:\n\nM <-\n  declare_model(households = add_level(N = 1000),\n                individuals = add_level(\n                  N = 4,\n                  X = draw_normal_icc(\n                    mean = 0,\n                    clusters = households,\n                    ICC = 0.65\n                  )\n                ))\n\n\n13.1.2.3 Drawing on baseline data\nIn some cases, you will be in possession of baseline data about the units you plan to study. In the model, you can replace simulations with these data, as long as your goal is to make inferences just about those units you have data for. If you have a census of the dwelling characteristics of all houses in Philadelphia and you plan to conduct a survey of homeowner attitudes toward a new tax based on a sample stratified by dwelling type, you don’t need to simulate data for the dwelling characteristics. They are known. You will still need to simulate the data on attitudes, which you have not yet collected.\n\n\n\n\nM <- \n  declare_model(\n    data = baseline_data,\n    attitudes = sample(1:5, N, replace = TRUE)\n  )\n\n\n13.1.2.4 Drawing on data about similar units\nMore commonly, you may have data on similar units from past studies or from a pilot study of your own. In these cases, you will not want to take their data as fixed, but resample from the data to simulate the population you are now drawing from. For example, if your baseline data is a random sample of households in Philadelphia, you may want to resample from that data to construct a simulated population of Philadelphia from which you can sample.\n\nM <-\n  declare_model(\n    data = baseline_data, \n    N = 619505, \n    handler = resample_data\n  )\n\nDrawing on past data holds several advantages. The natural correlations with clusters and across background characteristics are already built-in to your data. Using past data makes declaring your model easier, and in some cases can provide more realistic simulation of the data you end up collecting.\n\n13.1.2.5 Unknown heterogeneity\nMost declarations include unobserved variables, or unknown heterogeneity. These Us represent the lurking variables that confound our inferences and the variation in outcomes not correlated with observed values. In virtually every declaration in the book, we include a U term to represent these unobserved values.\nAt the point of declaration we face problems immediately: how much heterogeneity do we introduce, where, and of what kind?\nM1 shows how to make rather benign unknown heterogeneity. U is normally distributed and only affects Y. The observed binary variable X is independent of U and affects Y in its own way.\n\nM1 <-\n  declare_model(\n    N = 100,\n    U = rnorm(N),\n    X = rbinom(N, size = 1, prob = 0.5),\n    Y = 0.1 * X + U\n  )\n\nM2 is more worrisome. U now affects X as well, by affecting the probability of X equaling one. See Section (selection-on-observables?) for designs that face this sort of problem.\n\nM2 <-\n  declare_model(\n    N = 100,\n    U = rnorm(N),\n    X = rbinom(N, size = 1, prob = pnorm(U)),\n    Y = 0.1 * X + U\n  )\n\nA further question for U is how much it should vary. This question is tougher to answer. U is an important source of variation in outcomes, so it needs to be calibrated in a way that the simulated data look like the data you expect to generate or encounter in the world. So our best advice here is to follow Principle 3.2: Design agnostically to figure out which ones seem reasonable.\n\n13.1.3 Potential outcomes\nWe declare “potential” outcomes when describing counterfactual quantities.\nThe most straightforward way to declare potential outcomes is to make one variable per potential outcome. Here the two potential outcomes come from coin flip distributions with different success probabilities.\n\nM <-\n  declare_model(\n    N = 100,\n    Y_Z_0 = rbinom(N, size = 1, prob = 0.5),\n    Y_Z_1 = rbinom(N, size = 1, prob = 0.6)\n  )\n\nThe potential_outcomes function can do the same thing, but using R’s “formula” syntax, allowing us to write down potential outcomes in a “regression-like” way.\n\nM <- \n  declare_model(\n    N = 100, \n    potential_outcomes(Y ~ rbinom(N, size = 1, prob = 0.1 * Z + 0.5))\n  )\n\nM()\n\nBy default, potential_outcomes imagines you are making two potential outcomes with respect to a treatment variable Z that can take on two values, 0 and 1.\n\n\n\n\nOne draw of two potential outcomes\n \n ID \n    Y_Z_0 \n    Y_Z_1 \n  \n\n\n 001 \n    0 \n    0 \n  \n\n 002 \n    0 \n    1 \n  \n\n 003 \n    0 \n    0 \n  \n\n 004 \n    0 \n    1 \n  \n\n 005 \n    0 \n    0 \n  \n\n\n\n\nBut we can use potential_outcomes to describe multiple treatment conditions:\n\nM <- \n  declare_model(\n    N = 100, \n    potential_outcomes(\n      Y ~ rbinom(N, 1, prob = 0.1 * (Z == 1) + 0.2 * (Z == 2)), \n      conditions = list(Z = c(0, 1, 2))\n    )\n  )\nM()\n\n\n\n\n\nOne draw of three potential outcomes\n \n ID \n    Y_Z_0 \n    Y_Z_1 \n    Y_Z_2 \n  \n\n\n 001 \n    0 \n    0 \n    1 \n  \n\n 002 \n    0 \n    0 \n    0 \n  \n\n 003 \n    0 \n    0 \n    0 \n  \n\n 004 \n    0 \n    0 \n    0 \n  \n\n 005 \n    0 \n    1 \n    0 \n  \n\n\n\n\nOr to describe multiple treatment factors (see Section (factorial-experiments?) on factorial experiments):\n\nM <- \n  declare_model(\n    N = 100, \n    potential_outcomes(\n      Y ~ rbinom(N, 1, prob = 0.1 * Z1 + 0.2 * Z2 + 0.1 * Z1 * Z2), \n      conditions = list(Z1 = c(0, 1), Z2 = c(0, 1))\n    )\n  )\nM()\n\n\n\n\n\nOne draw of four potential outcomes\n \n ID \n    Y_Z1_0_Z2_0 \n    Y_Z1_1_Z2_0 \n    Y_Z1_0_Z2_1 \n    Y_Z1_1_Z2_1 \n  \n\n\n 001 \n    0 \n    0 \n    1 \n    1 \n  \n\n 002 \n    0 \n    0 \n    0 \n    1 \n  \n\n 003 \n    0 \n    0 \n    0 \n    1 \n  \n\n 004 \n    0 \n    0 \n    0 \n    0 \n  \n\n 005 \n    0 \n    0 \n    0 \n    0 \n  \n\n\n\n\n\n13.1.3.1 Effect sizes\nWe often want to consider a range of plausible effect sizes, for example when estimating the minimum detectable effect of a design. A strategy we commonly use is to sample the treatment effect in the model declaration itself, and then draw the potential outcomes using that single number. When we diagnose many times, then we will get many different treatment effects (here, tau), which we can then summarize in a diagnosis. Section (working-directly-with-the-simulations-data-frame?) describes how to create a plot of the power across values of tau.\n\nM <-\n  declare_model(\n    N = 100, \n    tau = runif(1, min = 0, max = 1), \n    U = rnorm(N), \n    potential_outcomes(Y ~ tau * Z + U)\n  )\n\nWhere do our expectations about effect sizes, and the minimal plausible effect size, come from? We may conduct meta-analysis of past studies when there are more than one sufficiently relevant estimates of the same effect, or a systematic review or literature review when they are less comparable but we may want to find a range (see Section (meta-analysis?) for a discussion of meta-analysis). We may be tempted to conduct a pilot study to estimate the effect size. We need to be careful when doing so what to infer and how much to update from small pilot studies, as we discuss in Section (piloting?), but we can often shrink our uncertainty about them. In the absence of pilot studies or past studies to draw on, we need to make educated guesses and understand under what true effect sizes our design will perform well and when it will not following Principle 3.1: Design holistically.\n\n13.1.3.2 Effect heterogeneity\nSometimes, the inquiry centers on treatment effect heterogeneity by subgroups. This heterogeneity has to be present in the model in order for the simulation to pick it up. Here we declare effect heterogeneity according to a binary covariate X. This example really shows off the utility of the formula syntax in the potential_outcomes function. We can write in our expectations about heterogeneity as if they were regression coefficients. Here, the “interaction term” is equal to 0.1.\n\nM <- \n  declare_model(\n    N = 100, \n    U = rnorm(N), \n    X = rbinom(N, 1, prob = 0.5),\n    potential_outcomes(Y ~  0.3 * Z + 0.2*X + 0.1*Z*X + U)\n  )\n\n\n13.1.3.3 Correlation between potential outcomes\nTreated and untreated potential outcomes are typically highly correlated. When treatment effects are exactly homogeneous, the correlation is equal to 1. Rarely are potential outcomes negatively correlated, but it can occur. The sign and magnitude of the correlation especially affects the standard errors of estimates for causal effects (for more discussion of the correlation of potential outcomes in experiments and also standard error estimators, see Section (analytic-design-diagnosis?)).\nWe described some complexities of generating binary variables above. They transfer over to the generation of correlated potential outcomes in special ways. In the declarations below, M1 generates uncorrelated potential outcomes, because the draws from rbinom are independent of one another. In M2, we still use rbinom, but with a transformation of the normally-distributed variable into a probability via pnorm. This allows the potential outcomes to be correlated because they are both influence by the same latent variable. Finally, in M3, we generate highly correlated potential outcomes because we peel off the layer of randomness introduced by rbinom.\n\nM1 <- \n  declare_model(\n    N = 100, \n    potential_outcomes(Y ~ rbinom(N, 1, prob = 0.2))\n  )\n\nM2 <- \n  declare_model(\n    N = 100,\n    latent = rnorm(N), \n    potential_outcomes(Y ~ rbinom(N, 1, prob = pnorm(latent + 0.2 * Z)))\n  )\n\nM3 <- \n  declare_model(\n    N = 100, \n    latent = rnorm(N), \n    potential_outcomes(Y ~ if_else(latent + 0.2 * Z > 0.5, 1, 0))\n  )"
  },
  {
    "objectID": "declaration-diagnosis-redesign/declaration-in-code.html#inquiry-code",
    "href": "declaration-diagnosis-redesign/declaration-in-code.html#inquiry-code",
    "title": "13  Designing in code",
    "section": "\n13.2 Inquiry",
    "text": "13.2 Inquiry\nAn inquiry is a summary function of events generated by a model. When we declare inquiries in code, we declare this summary function. Here, we declare a causal inquiry, the mean of the differences in two potential outcomes described in the model:\n\nM <- declare_model(N = 100, U = rnorm(N), potential_outcomes(Y ~ Z + U))\nI <- declare_inquiry(ATE = mean(Y_Z_1 - Y_Z_0))\n\nDescriptive inquiries can be declared in a similar way: they are just functions of outcomes rather than potential outcomes.\n\nM <- declare_model(N = 100, Y = rnorm(N))\nI <- declare_inquiry(mean_Y = mean(Y))\n\n\n13.2.1 Inquiries among subsets of units\nWe often want to learn about an inquiry defined among a subgroup of units. For example, if we are interested in the conditional average treatment effect (CATE) among units with X = 1, we can use the subset argument.\n\nM <- \n  declare_model(\n    N = 100,\n    U = rnorm(N),\n    X = rbinom(N, 1, prob = 0.5),\n    potential_outcomes(Y ~  0.3 * Z + 0.2 * X + 0.1 * Z * X + U)\n  )\nI <- \n  declare_inquiry(CATE = mean(Y_Z_1 - Y_Z_0),\n                  subset = X == 1)\n\nEquivalently, we could use R’s [] syntax for subsetting:\n\nI <- declare_inquiry(CATE = mean(Y_Z_1[X == 1] - Y_Z_0[X == 1]))\n\n\n13.2.2 Inquiries with continuous potential outcomes\n“Non-decomposable” inquiries are not as simple as an average over the units in the model. A common example arises with continuous potential outcomes. The regression discontinuity design described in Section (regression-discontinuity-designs?) has an inquiry that is defined by two continuous functions of the running variable. The control function is a polynomial function representing the potential outcome under control and the treatment function is a different polynomial representing treated potential outcomes. The inquiry is the difference in the two functions evaluated at the cut-off point on the running variable. We declare it as follows:\n\ncutoff <- 0.5\ncontrol <- function(X) {\n  as.vector(poly(X, 4, raw = TRUE) %*% c(0.7, -0.8, 0.5, 1.0))}\ntreatment <- function(X) {\n  as.vector(poly(X, 4, raw = TRUE) %*% c(0.0, -1.5, 0.5, 0.8)) + 0.15}\n\nI <- declare_inquiry(LATE = treatment(cutoff) - control(cutoff))\n\n\n13.2.3 Multiple inquiries\nIn some designs, we are interested in the value of an inquiry for many units or for many types of units.\nWe can enumerate them one-by-one, to describe the average treatment effect, two conditional average treatment effects, and the difference between them.\n\nI <- declare_inquiry(\n  ATE = mean(Y_Z_1[X == 1] - Y_Z_0[X == 1]),\n  CATE_X0 = mean(Y_Z_1[X == 0] - Y_Z_0[X == 0]),\n  CATE_X1 = mean(Y_Z_1[X == 1] - Y_Z_0[X == 1]),\n  Difference_in_CATEs = CATE_X1 - CATE_X0\n)\n\nIn the multilevel regression and poststratification (MRP) design in Section (multi-level-regression-and-poststratification?), we want to know what the average opinion is in each state.\nWe declare an inquiry at the county level below. We rely on group_by and summarize from dplyr to write a function MRP_inquiry that uses a pipeline to group the data into counties and take the average of individual values. Now, our design targets an inquiry for each state.\n\nM <- \n  declare_model(\n    counties = add_level(N = 5, county_mean = rnorm(N)),\n    individuals = add_level(N = 50, Y = rnorm(N, mean = county_mean))\n  )\n\nMRP_inquiry <- \n  function(data) {\n    data |> \n      group_by(counties) |> \n      summarize(mean_Y = mean(Y)) |> \n      ungroup()\n  }\n\nI <- declare_inquiry(handler = MRP_inquiry)\n\nWe discuss further in (answer-strategy-code?) how to link inquiries to answer strategies, including the case of multiple inquiries."
  },
  {
    "objectID": "declaration-diagnosis-redesign/declaration-in-code.html#data-strategy-code",
    "href": "declaration-diagnosis-redesign/declaration-in-code.html#data-strategy-code",
    "title": "13  Designing in code",
    "section": "\n13.3 Data strategy",
    "text": "13.3 Data strategy\nThe three data strategy functions, declare_sampling, declare_assignment, and declare_measurement share most features in common. All three all add variables to the running data frame. declare_sampling is special in that it has a filter argument that determines which (if any) of the units should be dropped from the data and which should be retained as the sample. declare_assignment, and declare_measurement work in the exact same way as one another. The reason we separate them is to insist on the features of the data strategy, not for a deep programming reason.\n\n13.3.1 Sampling\nDeclaring a sampling procedure involves constructing a variable indicating whether a unit is sampled or not and then filtering to sampled units. By default, you should create a variable S and declare_sampling will filter to sampled units by selecting those for which S == 1. You can rename your sampling variable or you can create more sampling variable to develop multistage sampling procedures, though you will need to alter the filter argument to reflect your changed procedure.\n\nD <- declare_sampling(S = complete_rs(N = 100, n = 10))\n\nFor a multistage sample of districts then villages then households, we start out with all the data and sample at each stage then combine the three sampling indicators to form the final indicator S.\n\nD <-\n  declare_sampling(\n    # sample 20 districts\n    S_districts = cluster_rs(clusters = districts, n = 20),\n    # within each district, sample 50 villages\n    S_villages  = strata_and_cluster_rs(\n      strata = districts,\n      clusters = villages,\n      strata_n = 10\n    ),\n    # within each village select 25 households\n    S_households  = strata_and_cluster_rs(\n      strata = villages,\n      clusters = households,\n      strata_n = 25\n    ),\n    S = S_districts == 1 & S_villages == 1 & S_households == 1,\n    filter = S == 1\n  )\n\nYou could also perform each of these steps in separate calls, and the data will be filtered appropriately step-to-step.\n\nD <-\n  declare_sampling(S = cluster_rs(clusters = districts, n = 20)) +\n  declare_sampling(S = strata_and_cluster_rs(\n    strata = districts,\n    clusters = villages,\n    strata_n = 10\n  )) +\n  declare_sampling(S = strata_and_cluster_rs(\n    strata = villages,\n    clusters = households,\n    strata_n = 25\n  ))\n\nFor many sampling designs, the probabilities of inclusion are not constant across units. We often need to adjust the answer strategy by reweighting the data according to the inverse of the inclusion probabilities. For common sampling designs in randomizr, we provide a built-in function for calculating these. If you roll your own sampling function, you will need to calculate them yourself. Here we show how to include probabilities from a stratified sampling design.\n\nM <-\n  declare_model(N = 100,\n                X = rbinom(N, 1, prob = 0.5))\n\nD <-\n  declare_sampling(\n    S = strata_rs(strata = X, strata_prob = c(0.2, 0.5)),\n    S_inclusion_probability =\n      strata_rs_probabilities(strata = X,\n                              strata_prob = c(0.2, 0.5))\n  )\n\n\n13.3.2 Treatment assignment\nThe declaration of treatment assignment procedures works similarly to sampling, but we don’t drop any units. Treatment assignment probabilities often come into play just like sampling inclusion probabilities. You can use randomizr to calculate them for many common designs in a similar fashion, except that for treatment assignment in order to know with what probability you were assigned to the condition you are in we have to know what condition you are in. To obtain condition assignment probabilities we can declare:\n\nD <- \n  declare_assignment(\n    Z = complete_ra(N, m = 50),\n    Z_condition_probability = \n      obtain_condition_probabilities(assignment = Z, m = 50)\n  )\n\n\n13.3.3 Measurement\nMeasurement procedures can be declared with declare_measurement. A common use is to generate an observed measurement from a latent value:\n\nM <- declare_model(N = 100, latent = runif(N))\nD <- declare_measurement(observed = rbinom(N, 1, prob = latent))\n\n\n13.3.3.1 Revealing potential outcomes\nThe most common use of declare_measurement in this book, however, is for the “revelation” of potential outcomes according to treatment assignments. We build potential outcomes in declare_model, randomly assign in declare_assignment, then reveal outcomes in declare_measurement. We use the reveal_outcomes function to pick out the right potential outcome to reveal for each unit.\n\nM <-\n  declare_model(\n    N = 100,\n    potential_outcomes(Y ~ rbinom(N, size = 1, prob = 0.1 * Z + 0.5))\n  )\n\nD <-\n  declare_assignment(Z = complete_ra(N, m = 50)) +\n  declare_measurement(Y = reveal_outcomes(Y ~ Z))\n\n\n13.3.3.2 Index creation\nMany designs use multiple measures of the same outcome, which are then combined into an index. For example, here’s a design with three measures of Y that we will combine using factor analysis. Here we use the fa function from the psych package which can be installed with install.packages('psych').\n\n\n\n\nlibrary(psych)\n#> \n#> Attaching package: 'psych'\n#> The following objects are masked from 'package:ggplot2':\n#> \n#>     %+%, alpha\n\nD <- \n  declare_measurement(\n    index = fa(\n      r = cbind(Y_1, Y_2, Y_3),\n      nfactors = 1,\n      rotate = \"varimax\"\n    )$scores\n)"
  },
  {
    "objectID": "declaration-diagnosis-redesign/declaration-in-code.html#answer-strategy-code",
    "href": "declaration-diagnosis-redesign/declaration-in-code.html#answer-strategy-code",
    "title": "13  Designing in code",
    "section": "\n13.4 Answer strategy",
    "text": "13.4 Answer strategy\nAn answer strategy is a function that provides answers to an inquiry. Declaring one in code involves selecting that function and linking the answer or answers it returns to one or more inquiries.\nThe functions we declare in DeclareDesign for answer strategies differ from those for the other elements of research design to reflect the two-step nature of many answer strategies. Often, first a statistical model (e.g., a linear regression) is fit to data, and then summaries of that model fit (e.g., the coefficient on a variable X, its standard error, t-statistic, p-value, and confidence interval) are combined to form an answer and its associated measures of uncertainty.\n\n13.4.1 Statistical modeling functions\nIn DeclareDesign, we call these two steps .method and .summary (these arguments are preceded by .’s to avoid argument conflicts). The .method argument in declare_estimator can take almost any modelling function in R (e.g., lm for linear regression) and .summary consists of a summary function that calculates statistics from the fit such as tidy or glance. You can write your own model or model summary. When your answer strategy does not fit this two-step structure, you can (as with all declare_ functions) write your own handler.\nWe break down each part of a standard answer strategy declaration using the example of a linear regression of the effect of a variable Z on an outcome Y in Declaration @ref(def:declaration-13-1}. The first argument in our declare_estimator step defines the method we will use, here lm_robust which is our function in the estimatr package for running linear regressions with robust standard errors. The second is the main argument for lm_robust, the formula for the regression specification, in this case Y on Z with no controls.\n\nDeclaration 13.1 Linear regression design\n\ndeclaration_13.1 <-\n  declare_model(N = 100,\n                U = rnorm(N),\n                potential_outcomes(Y ~ 0.2 * Z + U)) +\n  declare_inquiry(ATE = mean(Y_Z_1 - Y_Z_0)) +\n  declare_assignment(Z = complete_ra(N)) +\n  declare_measurement(Y = reveal_outcomes(Y ~ Z)) +\n  declare_estimator(\n    Y ~ Z,\n    .method = lm_robust,\n    .summary = tidy,\n    term = \"Z\",\n    inquiry = \"ATE\",\n    label = \"OLS\"\n  )\n\n\\(~\\)\n\n\ndraw_estimates(declaration_13.1)\n\n\n\n\n\nThe estimate from one draw of the linear regression design\n \n term \n    estimator \n    estimate \n    std.error \n    statistic \n    p.value \n    conf.low \n    conf.high \n    df \n    outcome \n    inquiry \n  \n\n Z \n    OLS \n    0.03 \n    0.2 \n    0.16 \n    0.87 \n    -0.36 \n    0.43 \n    98 \n    Y \n    ATE \n  \n\n\n\n\n13.4.2 Tidying statistical modelling function output\nLet’s unpack the .summary argument. In this case we sent the tidy function from the broom package (the default). Understanding what tidy does opens a window into the way we match estimates and estimands. The tidy function takes many model fit objects and returns a data frame in which rows represent estimates and columns represent statistics about that estimate. The columns typically include the estimate itself (estimate), an estimated standard error (std.error), a test statistic of some kind reported by the model function such as a t-statistic or Z-statistic (statistic), a p-value based on the test statistic (p.value), a confidence interval (conf.low, conf.high), and the degrees of freedom of the test statistic if relevant (df).\nA key column in the output of tidy is term, which represents which coefficient (term) is being described in that row. The term column uniquely identifies the row. We will often need to use the term column in conjunction with the name of the estimator to link estimates to estimands when there are more than one. If in the regression we pull out two coefficients (e.g., for treatment indicator 1 and for treatment indicator 2), we need to be able to link those to separate inquiries representing the true effect of treatment 1 and the true effect of treatment 2. Term is our tool for doing so. The default is for term to pick the first coefficient that is not the intercept, so for the regression Y ~ Z there will be an intercept and then the coefficient on Z which is what will be picked.\nThe inquiry argument defines which inquiry or inquiries the estimates will be linked to. In this case, we link to a single inquiry, the ATE. You can also declare an estimator that shoots at multiple inquiries: declare_estimator(Y ~ Z, .method = lm_robust, term = \"Z\", inquiry = c(\"ATE\", \"ATT\")) - useful for learning how well an estimator does for different targets. When we run the answer strategy on data, we get two additional pieces of information tacked on to the model summary data frame: the name of the estimator, which comes from the label argument, and the inquiry. The unit of analysis of a diagnosis is the inquiry-estimator pair, so if you link an estimator to multiple inquiries, then there will be a row for each inquiry.\nWe can use other summary functions to obtain other summaries of the fitted model. For example glance will provide model fit statistics such as the r-squared:\n\nA <- declare_estimator(Y ~ Z, \n                       .method = lm_robust, \n                       .summary = glance)\n\ndeclaration_13.1 |> \n  draw_data() |> \n  A()\n\n\n\n\n\nThe model fit statistics from one draw of the linear regression design\n \n r.squared \n    adj.r.squared \n    statistic \n    p.value \n    df.residual \n    nobs \n    se_type \n  \n\n 0 \n    -0.01 \n    0.16 \n    0.69 \n    98 \n    100 \n    HC2 \n  \n\n\n\nWhen neither tidy nor glance works well for your answer strategy, you can write your own summary function. Below, we build up a tidy function for the lm model from scratch. (One is already built-in to the broom package, but we do so here to illustrate how you can write your own for a function that does not already have one.) Before you start to write your own summary function, check whether one exists on the Broom Web site.\nThere are three sets for a tidy function:\n\nPull out statistics from the model fit object. You can extract out any statistics and transform them in any relevant way.\nReturn a data frame (or tibble).\nName your estimates in a common format that works across all tidy functions. The estimate column should be called “estimate”, the standard error column “std.error”, etc., as described earlier. However, if you want to add other statistics from the model fit that you will diagnose, you can and you can name them whatever you want.\n\n\ntidy_lm <- function(fit) {\n  # calculate estimates by grabbing the coefficients from the model fit\n  estimate <- coef(lm)\n  \n  # get the names of the coefficients (e.g., \"(Intercept)\", \"Z\")\n  #   we will call these \"term\" to represent regression terms\n  term <- names(estimates)\n  \n  # calculate the standard error by grabbing the variance-covariance\n  #   matrix, then pulling the diagonal elements of it and taking the \n  #   square root to transform from variances to standard errors\n  std.error <- sqrt(diag(vcov(lm)))\n  \n  # return a tibble with term, estimate, and std.error\n  tibble(term = term, estimate = unlist(estimate), std.error = std.error)\n}\n\ndeclare_estimator(\n  Y ~ Z,\n  .method = lm,\n  .summary = tidy_lm\n)\n\nIn other cases, you may want to build on functions that inter-operate with the broom functions to do specialized summary tasks like calculating marginal effects or predicted effects. The margins function from the margins package calculates marginal effects and the predictions package from the predictions package are especially useful and work well with the tidy workflow. To calculate marginal effects, run margins and then tidy as your model summary:\n\ntidy_margins <- function(x) {\n  tidy(margins(x, data = x$data), conf.int = TRUE)\n}\n\ndeclare_estimator(\n  Y ~ Z + X,\n  .method = glm,\n  family = binomial(\"logit\"),\n  .summary = tidy_margins,\n  term = \"Z\"\n) \n\n\n13.4.3 Custom answer strategies\nIf your answer strategy does not use a .method function, you’ll need to provide a function that takes data as an input and returns a data frame with the estimate. Set the handler to be label_estimator(your_function_name) to take advantage of DeclareDesign’s mechanism for matching inquiries to estimators. When you use label_estimator, you can provide an inquiry, and DeclareDesign will keep track of which estimates match each inquiry. (It simply adds a column to your tidy estimates data frame for the name of the estimator and the inquiry.) For example, to calculate the mean of an outcome, you could write your own estimator in this way:\n\nmy_estimator <-\n  function(data) {\n    data.frame(estimate = mean(data$Y))\n  }\n\ndeclare_estimator(handler = label_estimator(my_estimator),\n                  label = \"mean\",\n                  inquiry = \"Y_bar\")\n\nOften you may want to construct a test as part of your answer strategy that does not target an inquiry. Our declare_test function works just like declare_estimator except you need not include an inquiry. The label_test infrastructure works just like label_estimator for custom test functions."
  },
  {
    "objectID": "declaration-diagnosis-redesign/declaration-in-code.html#declaration-code",
    "href": "declaration-diagnosis-redesign/declaration-in-code.html#declaration-code",
    "title": "13  Designing in code",
    "section": "\n13.5 Declaration",
    "text": "13.5 Declaration\nTo construct a research design object that we can operate on — diagnose it, redesign it, draw data from it, etc. — we add research design elements together with the + operator. In Declaration 13.2, we first create each design step separately, then concatenate the steps. This style of declaration is useful when you want to mix-and-match design elements. Usually, though, we just add steps together without creating the each step first.\n\nDeclaration 13.2 Declaration of two-arm randomized experiment\n\nmodel <-\n  declare_model(N = 1000,\n                U = rnorm(N),\n                X = U + rnorm(N, sd = 0.5),\n                potential_outcomes(Y ~  0.2 * Z + U))\ninquiry <-\n  declare_inquiry(ATE = mean(Y_Z_1 - Y_Z_0))\nsampling <-\n  declare_sampling(S = simple_rs(N, prob = 0.2), \n                   filter = S == 1)\nassignment <-\n  declare_assignment(Z = complete_ra(N))\nmeasurement <-\n  declare_measurement(Y = reveal_outcomes(Y ~ Z))\nanswer_strategy <-\n  declare_estimator(Y ~ Z, inquiry = \"ATE\", label = \"DIM\") +\n  declare_estimator(Y ~ Z + X, inquiry = \"ATE\", label = \"OLS\")\n\n# as separate elements\ndeclaration_13.2 <-\n  model +\n  inquiry +\n  sampling +\n  assignment +\n  measurement +\n  answer_strategy\n\n# equivalently, and more compactly:\ndeclaration_13.2 <-\n  declare_model(N = 1000,\n                U = rnorm(N),\n                X = U + rnorm(N, sd = 0.5),\n                potential_outcomes(Y ~  0.2 * Z + U)) + \n  declare_inquiry(ATE = mean(Y_Z_1 - Y_Z_0)) +\n  declare_sampling(S = simple_rs(N, prob = 0.2),\n                   filter = S == 1) +\n  declare_assignment(Z = complete_ra(N)) +\n  declare_measurement(Y = reveal_outcomes(Y ~ Z)) +\n  declare_estimator(Y ~ Z, inquiry = \"ATE\", label = \"DIM\") +\n  declare_estimator(Y ~ Z + X, inquiry = \"ATE\", label = \"OLS\")\n\n\nOrder matters in declaring designs. We can think of the order of the declaration as the temporal order in which the elements are realized. Below, since the inquiry comes before sampling and assignment, the inquiry is a population inquiry, the population average treatment effect.\n\nmodel +\n  declare_inquiry(PATE = mean(Y_Z_1 - Y_Z_0)) +\n  sampling +\n  assignment +\n  measurement +\n  answer_strategy\n\nWe could instead define our inquiry as a sample average treatment effect by putting the inquiry after sampling:\n\nmodel +\n  sampling +\n  declare_inquiry(SATE = mean(Y_Z_1 - Y_Z_0)) +\n  assignment +\n  measurement + \n  answer_strategy"
  },
  {
    "objectID": "declaration-diagnosis-redesign/declaration-in-code.html#diagnosis-code",
    "href": "declaration-diagnosis-redesign/declaration-in-code.html#diagnosis-code",
    "title": "13  Designing in code",
    "section": "\n13.6 Diagnosis",
    "text": "13.6 Diagnosis\nOnce a design is declared in code, diagnosing it is usually the easy part. diagnose_design handles all the details and bookkeeping for you. In this section, we outline how to conduct a diagnosis using our default tools for standard diagnoses and also how you might operate directly on the simulations for more complicated analyses.\n\nDiagnosis 13.1 (Diagnosis of two-arm randomized experiment) Here we use diagnose_design to diagnose the design. By default, we conduct 500 simulations and characterize simulation error with 100 bootstraps of the diagnosands. reshape_diagnosis prepares the output for nice printing.\n\ndiagnosis_13.1 <-\n  diagnose_design(declaration_13.2,\n                  sims = 500,\n                  bootstrap_sims = 100)\nreshape_diagnosis(diagnosis_13.1)\n\n\n\n\n\n\n\n\nDesign diagnosis of two-arm randomized experiment\n \n Design \n    Inquiry \n    Estimator \n    Outcome \n    Term \n    N Sims \n    Mean Estimand \n    Mean Estimate \n    Bias \n    SD Estimate \n    RMSE \n    Power \n    Coverage \n  \n\n\n declaration_13.2 \n    ATE \n    DIM \n    Y \n    Z \n    2000 \n    0.20 \n    0.20 \n    -0.00 \n    0.14 \n    0.14 \n    0.29 \n    0.96 \n  \n\n  \n     \n     \n     \n     \n     \n    (0.00) \n    (0.00) \n    (0.00) \n    (0.00) \n    (0.00) \n    (0.01) \n    (0.00) \n  \n\n declaration_13.2 \n    ATE \n    OLS \n    Y \n    Z \n    2000 \n    0.20 \n    0.20 \n    -0.00 \n    0.06 \n    0.06 \n    0.87 \n    0.95 \n  \n\n  \n     \n     \n     \n     \n     \n    (0.00) \n    (0.00) \n    (0.00) \n    (0.00) \n    (0.00) \n    (0.01) \n    (0.00) \n  \n\n\n\n\nWe can “tidy” the results of the diagnosis for easier printing and plotting of the results. Here, each row is an estimate of a diagnosand and then uncertainty statistics such as the standard error and 95% confidence interval around the diagnosand estimate.\n\ntidy(diagnosis_13.1)\n\n\n\n\n\nTidied diagnosis of two-arm randomized experiment\n \n design \n    inquiry \n    estimator \n    outcome \n    term \n    diagnosand \n    estimate \n    std.error \n    conf.low \n    conf.high \n  \n\n\n declaration_13.2 \n    ATE \n    DIM \n    Y \n    Z \n    mean_estimand \n    0.20 \n    0.00 \n    0.20 \n    0.20 \n  \n\n declaration_13.2 \n    ATE \n    DIM \n    Y \n    Z \n    mean_estimate \n    0.20 \n    0.00 \n    0.19 \n    0.20 \n  \n\n declaration_13.2 \n    ATE \n    DIM \n    Y \n    Z \n    bias \n    0.00 \n    0.00 \n    -0.01 \n    0.00 \n  \n\n declaration_13.2 \n    ATE \n    DIM \n    Y \n    Z \n    sd_estimate \n    0.14 \n    0.00 \n    0.13 \n    0.14 \n  \n\n declaration_13.2 \n    ATE \n    DIM \n    Y \n    Z \n    rmse \n    0.14 \n    0.00 \n    0.13 \n    0.14 \n  \n\n declaration_13.2 \n    ATE \n    DIM \n    Y \n    Z \n    power \n    0.29 \n    0.01 \n    0.27 \n    0.31 \n  \n\n declaration_13.2 \n    ATE \n    DIM \n    Y \n    Z \n    coverage \n    0.96 \n    0.00 \n    0.95 \n    0.97 \n  \n\n declaration_13.2 \n    ATE \n    OLS \n    Y \n    Z \n    mean_estimand \n    0.20 \n    0.00 \n    0.20 \n    0.20 \n  \n\n declaration_13.2 \n    ATE \n    OLS \n    Y \n    Z \n    mean_estimate \n    0.20 \n    0.00 \n    0.20 \n    0.20 \n  \n\n declaration_13.2 \n    ATE \n    OLS \n    Y \n    Z \n    bias \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n  \n\n declaration_13.2 \n    ATE \n    OLS \n    Y \n    Z \n    sd_estimate \n    0.06 \n    0.00 \n    0.06 \n    0.06 \n  \n\n declaration_13.2 \n    ATE \n    OLS \n    Y \n    Z \n    rmse \n    0.06 \n    0.00 \n    0.06 \n    0.06 \n  \n\n declaration_13.2 \n    ATE \n    OLS \n    Y \n    Z \n    power \n    0.87 \n    0.01 \n    0.86 \n    0.89 \n  \n\n declaration_13.2 \n    ATE \n    OLS \n    Y \n    Z \n    coverage \n    0.95 \n    0.00 \n    0.94 \n    0.96 \n  \n\n\n\n\n\n\n13.6.1 Working directly with the simulations data frame\nIn some cases, it is useful to operate directly on the simulations produced by diagnose_design. For example, we might want to calculate diagnosands manually or plot simulations.\n\nsimulations_df <- get_simulations(diagnosis_13.1)\n\nOnce we have the simulations, we can summarize them using dplyr tools:\n\nsimulations_df |> \n  group_by(design, inquiry, estimator, term, outcome) |>\n  summarize(\n    bias = mean(estimate - estimand),\n    rmse = sqrt(mean((estimate - estimand)^2)),\n    power = mean(p.value <= 0.05),\n    coverage = mean(estimand <= conf.high & estimand >= conf.low)\n  )\n\nPlotting simulations using ggplot2 is often a great way to gain a deeper sense of the properties of the design.\nThe answer strategy for Declaration 13.1 includes two estimators, so our aim here is to create histograms of the sampling distribution for each of them. This goal is best accomplished using geom_histogram, then faceting by estimator using facet_wrap. We often want to overlay the true value of the estimand on these plots, which we do here with geom_vline. Notice that in order to do, we had to create a summary_df that includes the value of the estimand. The resulting plot shows that both the adjusted and unadjusted estimators are unbiased, but that the sampling distribution of the adjusted estimator is tighter.\n\n# first create summary for vertical lines\nsummary_df <- \n  simulations_df |> \n  group_by(estimator) |> \n  summarize(estimand = mean(estimand))\n\n# then plot simulations\nggplot(simulations_df) +\n  geom_histogram(aes(estimate),\n                 bins = 40, fill = \"#72B4F3\") +\n  geom_vline(data = summary_df,\n             aes(xintercept = estimand),\n             lty = \"dashed\", color = \"#C6227F\") +\n  annotate(\"text\", y = 300, x = 0, label = \"Estimand\",\n           color = \"#C6227F\", hjust = 1) +\n  facet_wrap(~ estimator) +\n  labs(x = \"Estimate\", y = \"Count of simulations\") +\n  theme_minimal()\n\n\n\nFigure 13.2: Example visualization of a diagnosis\n\n\nDiagnosing over model uncertainty is a crucial part of diagnosis. We want to understand when our design performs well and when it does not. A classical example of this in wide practice is the power curve. In a power curve, we display the power of a design (the probability of achieving statistical significance) along different possible effect sizes. In this setting, plotting the simulations directly with an indicator for whether that simulation’s estimate is statistically significant is the easiest way to learn about the properties of the design.\n\ndesign <-\n  declare_model(\n    N = 200,\n    U = rnorm(N),\n    potential_outcomes(Y ~ runif(1, 0.0, 0.5) * Z + U)\n  ) +\n  declare_inquiry(ATE = mean(Y_Z_1 - Y_Z_0)) +\n  declare_assignment(Z = complete_ra(N)) +\n  declare_measurement(Y = reveal_outcomes(Y ~ Z)) +\n  declare_estimator(Y ~ Z, inquiry = \"ATE\") \n\nsimulations_df <- \n  diagnose_design(design) |> \n  get_simulations() |> \n  mutate(significant = if_else(p.value <= 0.05, 1, 0))\n\nggplot(simulations_df) + \n  stat_smooth(aes(estimand, significant), method = 'loess', color = \"#3564ED\", fill = \"#72B4F3\", formula = 'y ~ x') +\n  geom_hline(yintercept = 0.8, color = \"#C6227F\", linetype = \"dashed\") +\n  annotate(\"text\", x = 0, y = 0.85, label = \"Conventional power threshold = 0.8\", hjust = 0, color = \"#C6227F\") + \n  scale_y_continuous(breaks = seq(0, 1, 0.2)) +\n  coord_cartesian(ylim = c(0, 1)) +\n  theme(legend.position = \"none\") +\n  labs(x = \"Model parameter: true effect size\",\n       y = \"Diagnosand: statistical power\") +\n  theme_minimal()\n\n\n\nFigure 13.3: Example visualization of a diagnosis"
  },
  {
    "objectID": "declaration-diagnosis-redesign/declaration-in-code.html#redesign-code",
    "href": "declaration-diagnosis-redesign/declaration-in-code.html#redesign-code",
    "title": "13  Designing in code",
    "section": "\n13.7 Redesign",
    "text": "13.7 Redesign\nThis chapter already displays the main computational approaches to redesign. The basic principle is that we need to create a list of designs which then get passed to simulate_designs or diagnose_designs. (The plural versions of these functions are identical to their singular counterparts, we just provide both to allow the code to speak for itself a little more easily).\nYou can make lists of designs to redesign across directly with list:\n\ndesigns <- list(design1, design2)\n\nMore often, you’ll vary designs over a parameter with redesign. Here, we’re imagining we’ve already declared a design that has an N parameter that we allow to have 3 values.\n\ndesigns <- redesign(design, N = c(100, 200, 300))\n\nWhichever way you use to create designs, you can then diagnose all of the designs in the list with:\n\ndesigns <- diagnose_designs(designs)"
  },
  {
    "objectID": "library/index.html",
    "href": "library/index.html",
    "title": "Research Design Library",
    "section": "",
    "text": "Our goal in this section is not to provide a comprehensive accounting of all empirical research designs. It’s also not to describe any of the particular designs in exhaustive detail, because we are quite sure that in order for these designs to be useful for any practical purpose, they will need to be modified. The entries in the design library are not recipes that will automatically produce high-quality research. Instead, we hope that the entries provide inspiration for how to tailor a particular class of design to your own research setting.\nThe design library is also a corpus of design elements and code that can be mixed-and-matched to fit your particular research setting. We do not have a stepped-wedge experimental design with blocking, but you can create one if that is your design by combining elements from the stepped-wedge design and the block-randomized experimental design.\nWe’ve organized the library by inquiry and by data strategy. Inquiries can be descriptive or causal; data strategies can be observational or experimental. Crossing these two pairs gives rise to four categories of research design: observational descriptive, experimental descriptive, observational causal, and experimental causal. Table ?tbl-designtypes offers examples of each class of design. We dedicate chapters to each of these four as well as a chapter for “complex” designs – designs that involve multiple stages, multiple inquiries, or inferences from multiple distinct projects.\n\n(#tab:designtypes) Research design types with examples\n\n\n\n\n\n\n\n\nData strategy: Observational\nData strategy: Experimental\n\n\n\n\nInquiry: Descriptive\nSample survey or case study\nList experiment or participant observation\n\n\nInquiry: Causal\nRegression discontinuity design or process tracing\nRandomized controlled trial"
  },
  {
    "objectID": "library/observational-descriptive/index.html",
    "href": "library/observational-descriptive/index.html",
    "title": "14  Observational : descriptive",
    "section": "",
    "text": "An observational design for descriptive inference usually has an inquiry like a population mean, covariance, or distribution as the main research goal. In an observational research design, the data strategy includes sampling and measurement components, but no treatments are allocated by the researcher. Put differently, in an observational design for descriptive inference, researchers seek to measure and summarize the world, but not to change it. This class of research designs encompasses a huge portion of research activity – most surveys fall into this class, as do large-scale data collections of economic and sociopolitical indicators, classic case studies focused on “thick description,” and many text analysis projects."
  },
  {
    "objectID": "library/observational-descriptive/simple-random-sampling.html",
    "href": "library/observational-descriptive/simple-random-sampling.html",
    "title": "\n15  Simple random sampling\n",
    "section": "",
    "text": "We declare a design in which a researcher takes a simple random sample of a population and uses a survey instrument to measure a latent quantity. The inquiry is the population average of the measured quantity. We show how to declare this design and an approach to incorporate concerns about non random non response in the design.\nDescriptive inquiries like the population mean of one variable or the population correlation between two variables are defined with respect to a specific group of units – the population about which we want to draw inferences.\nOne approach to studying a population is to conduct a census in which we record data on all \\(N\\) units (written with an upper-case \\(N\\) to represent the population). A census has the clear advantage of being comprehensive, but it usually comes at an overwhelming and prohibitive cost.\nTo avoid those costs, we collect data from only \\(n\\) units (written with a lower-case \\(n\\)), where the sample size \\(n\\) is smaller than the population size \\(N\\). When the \\(n\\) units we sample are chosen at random, we can obtain unbiased estimates of many descriptive inquiries.1\nImagine we seek to estimate the average political ideology of adult residents of the small town of Portola, California (population 2,100). Under our model M, the latent ideology \\(Y^*\\) is drawn from a standard normal distribution.\nThe data strategy \\(D\\) has two components: a survey question \\(Q\\) and a sampling procedure \\(S\\). The survey question asks subjects to place themselves on a left-right scale that varies from 1 (most liberal) to 7 (most conservative). We approximate this measurement procedure with a function that “cuts” the latent ideology into 7 separate groups. This survey question will introduce measurement error insofar as it does not distinguish among people with different latent ideologies who, because of our measurement tool, place themselves at the same place on the seven-point scale.2 Our main hope for this measurement procedure is that all of the people who give themselves higher scores are indeed more conservative than those who give themselves lower scores. The sampling procedure is “complete” random sampling. We draw a sample of exactly \\(n = 100\\), where every member of the population has an equal probability of inclusion in the sample, \\(\\frac{n}{N}\\).\nThe model and data strategy are represented by the DAG in Figure 15.1. The DAG shows that the observed outcome \\(Y\\) is a function of the latent score \\(Y^*\\) and the survey question \\(Q\\). The observed outcome \\(Y\\) is only measured for sampled units, i.e., units that have \\(S = 1\\). This simple diagram represents important design assumptions. First, no arrow leads from \\(Y^*\\) to \\(S\\). If such an arrow were present, then units with higher or lower latent ideologies would be more likely to be sampled. Second, an arrow does lead from \\(Y^*\\) to \\(Y\\), indicating that we assume the measured variable does indeed respond to the latent variable. Finally, the diagram includes an explicit role for the survey question, which helps us to consider how alternative wordings of \\(Q\\) might change the observed variable \\(Y\\).\nOur inquiry \\(I\\) is the population mean of the measured variable \\(Y\\): \\(\\frac{1}{N} \\sum_1^N Y_i = \\bar{Y}\\), rather than the mean of the latent variable \\(Y^*\\). In this sense, our inquiry is “data-strategy dependent”, since we are interested in the average value of what we would measure for any member of the population were we to sample them.\nOur answer strategy is the sample mean estimator: \\(\\widehat{\\overline{Y}} = \\frac{1}{n} \\sum_1^n Y_i\\), implemented here as an ordinary least squares regression to facilitate the easy calculation of auxiliary statistics like the standard error of the estimate and the 95% confidence interval.\nWe incorporate these design features into Declaration 15.1. The portola object is a fixed population of 2100 units with a latent ideology Y_star. The declaration of the measurement strategy comes before the declaration of the inquiry, showing how the inquiry is data strategy dependent.\nTwo main diagnosands for the simple random sampling design are bias and root mean-squared error. We want to know if we get the right answer on average and we want to know, on average, how far off from the truth we are."
  },
  {
    "objectID": "library/observational-descriptive/simple-random-sampling.html#what-can-go-wrong",
    "href": "library/observational-descriptive/simple-random-sampling.html#what-can-go-wrong",
    "title": "\n15  Simple random sampling\n",
    "section": "\n15.1 What can go wrong",
    "text": "15.1 What can go wrong\nThe most serious threat to descriptive inference in a randomized sampling design is nonresponse. Missingness due to nonresponse can lead to bias if missingness is correlated with outcomes. Sometimes this bias is referred to as “selection bias” in the sense that some units select out of responding when sampled.\nDepending on what information is available about the missing units, various answer strategy fix-ups are available to analysts. For example, if we have demographic or other covariate information about the missing units, we can search for similar-seeming units in the observed data, then impute their missing outcomes. This approach depends on the strong assumption that units with the same covariate profile have the same average outcome, regardless of whether they go missing. The imputation process is often done on the basis of a regression model; multiple imputation methods attempt to incorporate the additional uncertainty that accompanies the modeling technique. Answer strategies that employ inverse probability weights adjust for non response by upweighting types of units we have too few of (relative to a population target) and downweighting units we have too many of.\nAvoiding – or dynamically responding to – missingness in the data strategy is usually preferable to the addition of modeling assumptions in the answer strategy. Avoiding missingness often means adding extra effort and expense, such as monetary incentives for participation, multiple rounds of attempted contact, and attempting contact through a variety of modes (phone, mail, email, direct message, text, in-person visit). The best way to allocate extra effort will vary from context to context, as will the payoff from doing so. Our recommendation is to reason about the plausible response rates that would result from different levels of effort, then to consider how to optimize the bias-effort tradeoff. Sometimes, achieving zero bias would be far too costly, so we would be willing to tolerate some bias because effort is too expensive.\nDeclaration 15.2 builds in a dependence between the latent outcome \\(Y*\\) and the probability of responding to the survey. That probability also responds to researcher effort. The diagnosis shows how effort translates into higher response rates and lower bias:\n\nDeclaration 15.2 Survey nonresponse design\n\neffort <- 0 # baseline of no extra effort\n\ndeclaration_15.2 <- \n  declare_model(data = portola) + \n  declare_measurement(Y = as.numeric(cut(Y_star, 7))) + \n  declare_inquiry(Y_bar = mean(Y)) + \n  declare_sampling(S = complete_rs(N, n = 100)) + \n  declare_measurement(\n    R = rbinom(n = N, size = 1, prob = pnorm(Y_star + effort)),\n    Y = if_else(R == 1, Y, NA_real_)\n  ) +\n  declare_estimator(Y ~ 1, inquiry = \"Y_bar\") +\n  declare_estimator(R ~ 1, label = \"Response Rate\")\n\n\n\n\nDiagnosis 15.2 (Survey nonresponse diagnosis) \ndiagnosis_15.2 <- \n  declaration_15.2 |> \n  redesign(effort = seq(0, 5, by = 0.5)) |> \n  diagnose_designs()\n\n\n\n\n\n\nFigure 15.2: Redesigning the random sampling design over researcher effort"
  },
  {
    "objectID": "library/observational-descriptive/simple-random-sampling.html#design-examples",
    "href": "library/observational-descriptive/simple-random-sampling.html#design-examples",
    "title": "\n15  Simple random sampling\n",
    "section": "\n15.2 Design Examples",
    "text": "15.2 Design Examples\n\nBradley et al. (2021) compare “big data” convenience sample surveys (n = 75,000 and 250,000) of COVID-19 vaccine uptake to a 1,000 person simple random sampling design with an inverse probability weighting answer strategy, finding strong support for random sampling over “big data.”\nSimple random sampling is also used when researchers need to take manual measurements of a large number of observations. Merkley and Stecula (2021) hand-code a simple random sample of 3,000 newspaper articles about climate changes out of the many hundreds of thousands of articles about climate change identified by an automatic search process, allowing them to characterize their population of observations without hand-coding each one.\n\n\n\n\n\nBradley, Valerie C., Shiro Kuriwaki, Michael Isakov, Dino Sejdinovic, Xiao-Li Meng, and Seth Flaxman. 2021. “Unrepresentative Big Surveys Significantly Overestimated US Vaccine Uptake.” Nature 600 (7890): 695–700.\n\n\nHedayat, A. S., Hansheng Cheng, and Jennifer Pajda-De La O. 2019. “Existence of Unbiased Estimation for the Minimum, Maximum, and Median in Finite Population Sampling.” Statistics & Probability Letters 153: 192–95.\n\n\nMerkley, Eric, and Dominik A. Stecula. 2021. “Party Cues in the News: Democratic Elites, Republican Backlash, and the Dynamics of Climate Skepticism.” British Journal of Political Science 51 (4): 1439–56."
  },
  {
    "objectID": "library/observational-descriptive/cluster-random-sampling.html",
    "href": "library/observational-descriptive/cluster-random-sampling.html",
    "title": "\n16  Cluster random sampling\n",
    "section": "",
    "text": "We declare a design in which a researcher takes a clustered random sample of a population and uses the design to ask whether they should invest in sampling more clusters or in more individuals per cluster. The design includes a budget constraint that affects how many can be sampled within clusters as a function of the number of clusters sampled.\nResearchers often cannot randomly sample at the individual level because it may, among other reasons, be too costly or logistically impractical. Instead, they may choose to sample clusters at random first, then randomly sample units within clusters. Clusters might be schools, localities, or households.\nHow does clustering change the research design relative to the individual-level design? First, we need to elaborate the model \\(M\\) to make the clustering hierarchy explicit. In the declaration below, we imagine our research site is two states in Nigeria. Localities are nested within states and individuals are nested within localities. Second, we want to respect this hierarchy when thinking about the distribution of outcomes. Individuals living in the same locality are likely to share political viewpoints, either through the explicit transmission of political views or because of common exposure to political influence. The “intra-cluster correlation” or ICC is an extremely important statistic for the design of cluster-sampled studies. It describes what fraction of the total variation in the outcome can be attributed to the across-cluster differences in average outcomes.\nIn the declaration below, the latent outcome Y_star describes subject’s latent political preferences. This latent outcome is a function of locality differences and individual differences. The variances of these two shocks (difference parameters) are determined by the ICC parameter. If ICC were equal to 1, the variance across localities would be equal to 1, and all individuals within a locality would have exactly the same political preferences. If ICC were equal to zero, then the variation in preferences would be entirely at the individual level.\nMany different cluster sampling designs are possible, but a standard choice is a two-stage design in which first, some but not all clusters are sampled, and second, some but not all units within a cluster are sampled. The sampling at either stage may be stratified by covariates at the appropriate level. The first stage can be stratified by cluster-level covariates and the second stage can be stratified by individual-level covariates in order to improve precision. In this declaration, we form cluster-level strata by state.\nThe two stage random-sampling design raises an important tradeoff: Should we invest in sampling more clusters or in more individuals per cluster? Typically, adding the marginal cluster is more expensive than adding the marginal individual. We formalize the tradeoff with a “budget function” that returns the largest individual level inclusion probability that is budget-compatible with a given cluster sampling probability:\nWe use the output of this function to determine the probability of an individual being sampled, conditional on their cluster being sampled.\nLastly, the answer strategy must also respects the data strategy by clustering standard errors at the highest level at which units are sampled, which in this case is the locality.\nWe redesign Declaration 16.1 over various levels of the cluster-level probability of sampling, which in turn sets the probability of sampling at the individual level. Figure 16.1 shows that for a good while, adding additional clusters yields precision gains. At some point, however, the cost to sample size within cluster is too large, and we start seeing precision loss around a probability of 0.75 of a cluster being sampled. The precise combination of design parameters that minimize the standard deviation of the sampling distribution will depend on nearly every aspect of the design declaration, but the most important are the total budget, the relative costs of clusters and individuals, and the ICC. When the ICC is zero, we should invest in few clusters and many individuals. When the ICC is one, we should invest in many clusters and few individuals."
  },
  {
    "objectID": "library/observational-descriptive/cluster-random-sampling.html#design-examples",
    "href": "library/observational-descriptive/cluster-random-sampling.html#design-examples",
    "title": "\n16  Cluster random sampling\n",
    "section": "\n16.1 Design examples",
    "text": "16.1 Design examples\n\nStokes (2005) use a stratified cluster sampling design to estimate the prevalence of vote buying in Argentina. The strata were provinces, the clusters were census tracts. A simple random sample of individuals residing in selected census tracts were ultimately interviewed.\nPaler, Marshall, and Atallah (2018) use a multistage clustered sampling design to study Lebanese citizen’s willingness to sign a petition condemning sectarian politics. The clusters were localities (villages in rural areas, neighborhoods in urban areas), which were stratified by population size. Within selected localities, simple random samples of households were drawn; within households, a single adult was drawn at random. The study also included a randomized treatment that manipulated whether petition signatures were public or private.\n\n\n\n\n\nPaler, Laura, Leslie Marshall, and Sami Atallah. 2018. “The Social Costs of Public Political Participation: Evidence from a Petition Experiment in Lebanon.” The Journal of Politics 80 (4): 1405–10.\n\n\nStokes, Susan C. 2005. “Perverse Accountability: A Formal Model of Machine Politics with Evidence from Argentina.” American Political Science Review 99 (3): 315–25."
  },
  {
    "objectID": "library/observational-descriptive/multilevel-regression-poststratification.html",
    "href": "library/observational-descriptive/multilevel-regression-poststratification.html",
    "title": "\n17  Multi-level regression and poststratification\n",
    "section": "",
    "text": "We declare a design in which researchers reweight the responses of different units in a sample in order to better estimate a population level quantity. Reweighting depends on how much units are thought to “represent” other nonsampled units and requires making decisions about how much units of different types should be pooled together. Design performance of a partially pooled model is compared against designs that involve no pooling and full pooling.\nMulti-level regression and poststratification (MRP) is a technique used primarily for “small area estimation.” In the prototypical setting, we conduct a nationally-representative survey of public opinion, but our goal is to generate estimates of the average level of public opinion for many subnational units. In the United States context, these “small area” units are often the 50 states. The main problem is that in a national poll of 2,000 Americans, we might only have 4 respondents from small states like Alaska, Wyoming, or Vermont, but more than 100 from large states like California, New York, or Texas. Accordingly, it is harder to estimate opinion in small states than in large states. The key insight of an MRP design is that we can “borrow strength” across states and kinds of people in order to improve state level estimates.\nIn an MRP design, the answer strategy includes two steps: a multi-level regression step and a poststratification step. The regression model generates estimates of the average opinion for classes of people within each state. The precise flavor of regression model can vary from application to application. In the simple example below, we use a generalized linear mixed effects model with an individual-level covariate and random effects by state, but regression models of substantial complexity are sometimes used to model important nuances in how opinions covary with individual and state-level characteristics (see, e.g., Bisbee (2019)).\nThe regression model generates estimates of the average opinion for classes of people within each state. The post-stratification step reweights these estimates to known proportions of each class of person within each state. The knowledge of these proportions has to come from outside the survey. The US census is the usual source of these population proportions in the American context, though any reliable source of this information is suitable.\nIn Declaration 17.1, we begin with a dataset of the fifty states that describes what fraction of people in each state has graduated from high school. This code block also establishes the true state means that will form our inquiry. In the model, we draw a nationally-representative sample of size 2,000, respecting the fraction of people within each state with a high school degree. The post-stratification weights are built from the that fraction as well. The binary public opinion variable policy_support is a function of the high school covariate, an individual-level shock, and a state-level shock. The inquiry is the mean policy support at the state level. The tricky part of this design is the two-step answer strategy. The first step is handled by the multilevel regression function glmer. The second step is handled by the post_stratification_helper function (available in the rdddr companion package) that obtains predictions from the model, then reweights them according to the post-stratification weights.\nFigure 17.1 shows one draw from this design, plotting the MRP estimates against the true level of opinion."
  },
  {
    "objectID": "library/observational-descriptive/multilevel-regression-poststratification.html#redesign-over-answer-strategies",
    "href": "library/observational-descriptive/multilevel-regression-poststratification.html#redesign-over-answer-strategies",
    "title": "\n17  Multi-level regression and poststratification\n",
    "section": "\n17.1 Redesign over answer strategies",
    "text": "17.1 Redesign over answer strategies\nThe strengths of the MRP design are best appreciated by contrasting MRP’s partial pooling approach to two alternatives: no pooling and full pooling. Under no pooling, we estimate each state mean separately with a national adjustment for the individual-level high school covariate. Under full pooling, we only adjust for high school and ignore state information altogether. Here we add both estimators to the design and diagnose.\n\nDeclaration 17.2 Redesign over answer strategies\n\ndeclaration_15.5 <- \n  declaration_15.4 +\n  declare_estimator(\n    handler = label_estimator(function(data) {\n      model_fit <- lm_robust(\n        formula = policy_support ~ HS + state,\n        data = data\n      )\n      post_stratification_helper(model_fit, data = data, group = state, weights = PS_weight)\n    }),\n    label = \"No pooling\",\n    inquiry = \"mean_policy_support\") +\n  declare_estimator(\n    handler = label_estimator(function(data) {\n      model_fit <- lm_robust(\n        formula = policy_support ~ HS,\n        data = data\n      )\n      post_stratification_helper(model_fit, data = data, group = state, weights = PS_weight)\n    }),\n    label = \"Full pooling\",\n    inquiry = \"mean_policy_support\")\n\n\n\n\nDiagnosis 17.1 (Diagnosing over answer strategies) \ndiagnosis_15.4 <- diagnose_design(declaration_15.5)\n\nFigure 17.2 compares the three estimators. The first column of facets shows one draw of the estimates against the estimands. The main thing to notice here is that the full pooling estimate is more or less a flat line – regardless of the estimand, the estimates are just above 50%. Relative to partial pooling, the no pooling estimates are further spread around the 45 degree line, with small states bouncing around the most.\nOn the right side of the figure, we see the bias, RMSE, and standard deviation diagnosands for each inquiry under all three answer strategies. Under no pooling, bias is very low, but the RMSE and standard deviation for small states is very high. Under full pooling, the standard deviation is very low, but bias is very positive for states with low support and very negative for states with high support. The resulting RMSE has a funny “V” shape – we only do well for states that happen to have opinion that is very close to the national average.\nPartial pooling represents a Goldilocks compromise between full and no pooling. Yes, we have some positive bias for low-opinion states and negative bias for high opinion states, but variance has been brought under control. As a result, the RMSE for both small and large states is small.\n\n\nFigure 17.2: Comparison of three answer strategies"
  },
  {
    "objectID": "library/observational-descriptive/multilevel-regression-poststratification.html#design-examples",
    "href": "library/observational-descriptive/multilevel-regression-poststratification.html#design-examples",
    "title": "\n17  Multi-level regression and poststratification\n",
    "section": "\n17.2 Design examples",
    "text": "17.2 Design examples\n\nLax and Phillips (2009) apply multilevel regression and poststratification to 41 national polls conducted between 1999 and 2008 to generate state-level estimates of gay rights attitudes in the US.\nTausanovitch and Warshaw (2013) apply multilevel regression and poststratification to large polls of Americans’ policy preferences to generate estimates of policy opinions at the Congressional district, state legislative district, and municipality levels.\n\n\n\n\n\nBisbee, James. 2019. “BARP: Improving Mister p Using Bayesian Additive Regression Trees.” American Political Science Review 113 (4): 1060–65.\n\n\nLax, Jeffrey R., and Justin H. Phillips. 2009. “Gay Rights in the States: Public Opinion and Policy Responsiveness.” American Political Science Review 103 (3): 367–86.\n\n\nTausanovitch, Chris, and Christopher Warshaw. 2013. “Measuring Constituent Policy Preferences in Congress, State Legislatures, and Cities.” The Journal of Politics 75 (2): 330–42."
  },
  {
    "objectID": "library/observational-descriptive/index-creation.html",
    "href": "library/observational-descriptive/index-creation.html",
    "title": "\n18  Index creation\n",
    "section": "",
    "text": "We declare a design is which we take multiple measures and combine them to learn about a latent, unobservable quantity. The design diagnosis shows that it is possible to generate interpretable conditional estimates of this quantity and assess bias even though the metric of the unobservable quantity is unknown. The diagnosis highlights how subtle differences in scale construction generate different biases.\nModels often specify a latent variable (Y_star) that we can’t observe directly. The measurement procedures we use to produce observed variables (Y_1, Y_2, Y_3) are not perfect: observed values may be related to the latent variables but are often on different scales. A common strategy for addressing measurement error is to combine multiple measures of the same latent variable into an index. The basic intuition for this procedure is that when we combine multiple measures, the errors attending to each measure will tend to cancel one another out. When this happens, the index itself has lower measurement error than any of the constituent parts.\nThe first difficult feature of such problems is that we do not have access to the scale on which Y_star is measured and so it may seem like a hopeless exercise to try to assess whether we have got good or bad estimates of Y_star when we combine the measured data.\nOne way around this is to normalize the scale of both the latent variable and the measured variable so that they have a mean of zero and a standard deviation of one. But in that case, we are guaranteed that our estimate of the mean of the normalized variable will be unbiased because we will certainly estimate a mean of 0! That may be — but as we show in the declaration below, if your model is correct this approach may still be useful for calculating other quantities (such as conditional means) that you don’t just get right by construction.\nA second challenge is deciding which measurements to combine into the index. We clearly only want to create indices using measurements of the same latent variable, but it can hard to be sure which latent variable, exactly, is being measured. Just relying on whether the measurements are positively correlated or not is not sufficient, because measurements can be correlated even if they are not measurements of the same underlying variable. Ultimately we have to rely on theory to make these decisions, as uncomfortable as that may make us.\nIn Declaration 18.1, our inquiry is the average level of the latent variable among units whose binary covariate X is equal to one.\nIn the declaration below Y_star has a normal distribution but it is not centered on zero. The measured variables Y_1, Y_2, Y_3 are also normally distributed but each has its own scale; they are all related to Y_star, though some more strongly than others. Many procedures for combining these measured variables into an index exist. Here we’ll consider a few of the common approaches:\nIn Figure 18.1 we show that the correlation between all the indices and the underlying quantity is quite good, even though the strength of the correlations for some of the components is weak. Trickier however is being sure we have an interpretable scale."
  },
  {
    "objectID": "library/observational-descriptive/index-creation.html#design-examples",
    "href": "library/observational-descriptive/index-creation.html#design-examples",
    "title": "\n18  Index creation\n",
    "section": "\n18.1 Design examples",
    "text": "18.1 Design examples\n\nJefferson (2022) introduces the “Respectability Politics Scale,” which is a six-item additive index. Responses on 1-7 Likert scales are rescaled to the 0-1 range, then averaged.\nBroockman and Kalla (2016) conduct a randomized experiment in which the main outcome is an index of attitudes toward transgender people, which is constructed by taking the first factor from a factor analysis of multiple survey questions.\n\n\n\n\n\nBroockman, David, and Joshua Kalla. 2016. “Durably Reducing Transphobia: A Field Experiment on Door-to-Door Canvassing.” Science 352 (6282): 220–24.\n\n\nJefferson, Hakeem. 2022. “Respectability and the Politics of Punishment Among Black Americans.” Unpublished Manuscript.\n\n\nKling, Jeffrey R, Jeffrey B Liebman, and Lawrence F Katz. 2007. “Experimental Analysis of Neighborhood Effects.” Econometrica 75 (1): 83–119."
  },
  {
    "objectID": "library/observational-causal/index.html",
    "href": "library/observational-causal/index.html",
    "title": "19  Observational : causal",
    "section": "",
    "text": "Observational causal inference is hard because it depends on the world generating just the right circumstances. Process tracing requires observation of just the right clues to the causal mystery. Selection-on-observables requires measurement of a set of variables that collectively close all the backdoor paths from treatment to outcome. A difference-in-difference design requires a very specific kind of over-time stability — that untreated potential outcomes move exactly in parallel. Instrumental variables designs require nature to randomly assign something we can measure. Regression discontinuity designs requires a cutoff that determines who is treated and who is not that we can observe and understand.\nThese are five of the big observational causal research designs. Many additional innovative designs have been developed to try to estimate causal quantities from observational research designs. These generally seek clever innovations in \\(A\\) in order to have as few assumptions on \\(M\\) as possible. We refer readers to Angrist and Pischke (2008) and Dunning (2012) for excellent overviews of the design theory behind many of these methods.\n\n\n\n\nAngrist, Joshua D., and Jörn-Steffen Pischke. 2008. Mostly Harmless Econometrics: An Empiricist’s Companion. Princeton: Princeton University Press.\n\n\nDunning, Thad. 2012. Natural Experiments in the Social Sciences: A Design-Based Approach. Cambridge, UK: Cambridge University Press."
  },
  {
    "objectID": "library/observational-causal/process-tracing.html",
    "href": "library/observational-causal/process-tracing.html",
    "title": "20  Process tracing",
    "section": "",
    "text": "We declare a qualitative design in which researchers seek to learn about the effect of a cause on a single unit. The diagnosis helps evaluate the gains from different within-case data gathering strategies.\nIn qualitative research we are often interested in learning about the causal effect for a single unit. For example, for a country unit that underwent sanctions (an “event”), we want to know the causal effect of the sanctions on government behavior. To do so, we need to know what would have happened if the event did not happen, the counterfactual outcome that did not occur as opposed to the factual outcome that did. Due to the fundamental problem of causal inference, we cannot observe what would have happened if that counterfactual case had happened. We have to guess—or infer—what would have happened. Social scientists have developed a large array of tools for guessing missing counterfactual outcomes — what would have happened in the counterfactual case, if the event had not happened.1\nA common inquiry in this setting is whether an outcome was due to a cause. For instance in a case with \\(X=1, Y=1\\), this “causal attribution” inquiry or “Cause of Effect” inquiry can be written \\(\\mathrm{CoE}:=1-Y(0)| X=1 \\mathrm{ \\& } Y=1\\). For a unit with \\(X=1\\) and \\(Y=1\\), \\(\\mathrm{CoE}=1\\) if \\(Y(0)=0\\).\n“Process tracing” is a prominent strategy for assessing causes of effects (Bennett and Checkel 2015; Fairfield and Charman 2017). Here, following for example Humphreys and Jacobs (forthcoming), we think of process tracing as a procedure in which researchers provide a theory in the form of a causal model that is rich enough to characterize the probability of observing ancillary data (“Causal process observations” (Brady 2004)) given underlying causal relations. When equipped with prior beliefs, such a model in turn lets one use Bayes’ rule to form posterior beliefs over causal relations after observing these data.\nFor intuition, say we are interested in whether a policy caused a change in economic outcomes. We theorize that for the policy to matter, it at least had to be implemented. So if we find out that the policy was not implemented we infer that it did not matter. We make theory-dependent inferences that are reasonable insofar as the theory itself is reasonable. In this example, if there are plausible channels through which a policy might have mattered even if not implemented, then our conclusion would not be warranted.\nTo illustrate design choices for a process tracing study we consider a setting in which we have already observed \\(X, Y\\) data and we are interested in figuring out whether \\(Y\\) takes on the value it does because \\(X\\) took on the value it did.\nMore specifically we imagine a model with two ancillary variables, \\(M\\) and \\(W\\). We posit that \\(X\\) causes \\(Y\\) via \\(M\\)—with negative effects of \\(X\\) on \\(M\\) and of \\(M\\) and \\(Y\\) ruled out. And we posit that \\(W\\) is a cause of both \\(M\\) and \\(Y\\). Specifically, our model asserts that if \\(W=1\\) then \\(X\\) causes \\(M\\) and \\(M\\) causes \\(Y\\) for sure. Under this model \\(M\\) and \\(W\\) each serve as “clues” for the causal effect of \\(X\\) on \\(Y\\).\nUsing the language popularized by Van Evera (1997), \\(M\\) provides a “hoop” test: if you look for data on \\(M\\) and find that \\(M=0\\) when \\(X=1\\) and \\(Y=1\\) then you infer that \\(X\\) did not cause \\(Y\\). If on the other hand you examine \\(W\\) and find that \\(W=1\\) then you have a “smoking gun” test: You infer that \\(X\\) did indeed cause \\(Y\\). However, if you find both \\(M=0\\) and \\(W=1\\), then you know your model is wrong.\nThe model can be described using the CausalQueries package thus:\nThe DAG of the causal model is shown in Figure 20.1.\nThis model definition describes the DAG but also specifies a set of restrictions on causal relations. By default, flat priors are then placed over all other possible causal relations, though of course other prior beliefs could also be specified.\nWe now have all we need to assess what inferences we might make given different sorts of observations on \\(W\\) and \\(M\\).\nTable ?tbl-ptprobative shows three types of quantities: beliefs upon observing \\(X=1, Y=1\\), conditional inferences upon observing additional data on \\(M\\) or \\(W\\), and the conditional probability of seeing different outcomes for \\(M\\) or \\(W\\).\nWe see here that \\(M\\) provides a hoop test since we are certain that \\(X\\) does not cause \\(Y\\) when \\(M=0\\), but we are uncertain when \\(M=1\\). (Moreover, we already expect to see \\(M=1\\) given what we have seen for \\(X\\) and \\(Y\\)). \\(W\\) provides a smoking gun test since we are certain that \\(X\\) causes \\(Y\\) when \\(M=1\\) but uncertain otherwise. Since we have access to both conditional inferences and the probability of seeing different types of data, we have enough data in Table ?tbl-ptprobative to calculate how different strategies will perform.\nWe think it useful to fold these quantities into a design declaration so that research consumers can access the data strategies and answer strategies in the same way as they would for any other problem.\nDeclaration 20.1 provides a flexible process tracing design. You can use this design with a different causal model and substituting in different causal queries and process tracing strategies. Given a background causal model, the design first draws a “causal type”—that is a case together with all its causal relations. The value of the estimand (EoC) can then be calculated and the observable data corresponding to the type revealed. The estimation uses a custom function, which simply returns the inferences on the query—like those in the Table ?tbl-ptprobative—but given different possible observed values for all nodes. No data strategy is provided explicitly as this is tied up here in the estimation, that is, the estimation step describes what data will be used."
  },
  {
    "objectID": "library/observational-causal/process-tracing.html#design-examples",
    "href": "library/observational-causal/process-tracing.html#design-examples",
    "title": "20  Process tracing",
    "section": "\n20.1 Design examples",
    "text": "20.1 Design examples\n\nRevkin and Ahram (2020) consider the “rebel social contract” in which rebel groups offer citizens political protections and social benefits in return for citizens’ allegiance. The authors use the presence of formal complaints by citizens about the Islamic State in Iraq and Syria as a hoop test for the causal model of exchange of protections for allegiance.\nSnyder and Borghard (2011) attempt to apply a smoking gun test to audience cost theory, but could find no clear cut cases of settings in which “the public was opposed to military action before a threat was issued and then explicitly punished the leader for not following through with a threat with which it disagreed.”\n\n\n\n\n\nBennett, Andrew, and Jeffrey Checkel. 2015. “Process Tracing: From Philosophical Roots to Best Practices.” In Process Tracing: From Metaphor to Analytic Tool, edited by Andrew Bennett and Jeffrey Checkel, 3–37. New York: Cambridge University Press.\n\n\nBrady, Henry E. 2004. “Data-Set Observations Versus Causal-Process Observations: The 2000 US Presidential Election.” In Rethinking Social Inquiry: Diverse Tools, Shared Standards, 267–72. Lanham: Rowman & Littlefield.\n\n\nDawid, Philip, Macartan Humphreys, and Monica Musio. 2022. “Bounding Causes of Effects with Mediators.” Sociological Methods & Research.\n\n\nFairfield, Tasha, and Andrew E Charman. 2017. “Explicit Bayesian Analysis for Process Tracing: Guidelines, Opportunities, and Caveats.” Political Analysis 25 (3): 363–80.\n\n\nHumphreys, Macartan, and Alan Jacobs. forthcoming. Integrated Inferences. Cambridge: Cambridge University Press.\n\n\nRevkin, Mara Redlich, and Ariel I. Ahram. 2020. “Perspectives on the Rebel Social Contract: Exit, Voice, and Loyalty in the Islamic State in Iraq and Syria.” World Development 132: 104981.\n\n\nSnyder, Jack, and Erica D. Borghard. 2011. “The Cost of Empty Threats: A Penny, Not a Pound.” American Political Science Review 105 (3): 437–56.\n\n\nVan Evera, Stephen. 1997. Guide to Methods for Students of Political Science. Ithaca: Cornell University Press."
  },
  {
    "objectID": "library/observational-causal/selection-on-observables.html",
    "href": "library/observational-causal/selection-on-observables.html",
    "title": "21  Selection-on-observables",
    "section": "",
    "text": "We declare a design in which a researcher tries to address concerns about confounding by conditioning on other observable variables. In the design the researcher has to specify how the other variable creates a risk of confounding and how exactly they will take account of these variables to minimize bias.\nWhen we want to learn about causal effects, but treatments are allocated by the world and not by the researcher, we are sometimes stuck. It is possible that a comparison of treated units to control units will generate biased inferences because of selection – the problem that certain types of units “select” into treatment and others into control. If the average potential outcomes of the groups that come to be treated or untreated are not equal, then a comparison of the realized outcomes of the two groups will yield biased causal inferences.\nSometimes, however, we know enough about selection in order to condition on the variables that cause it. A selection-on-observables design stipulates a family of models \\(M\\) of the world that describe which variables are responsible for selection, then employs a data strategy that measures those variables, rendering them “observable.” In the answer strategy, we draw on substantive knowledge of the causal process to generate an “adjustment set,” or a set of variables that predict selection into treatment. In the language of causal path diagrams, an adjustment set is a set of variables that, when conditioned upon, closes all back door paths from the treatment to the outcome. We can condition on the adjustment set using a variety of alternative answer strategies, for example through regression adjustment, stratification, or matching.\nThe quality of causal inferences we draw comes down to whether our claims about selection into treatment are correct. If we’ve missed a cause (missed a back door path), then our inferences will be prone to bias. It is the nature of the selection-on-observables design that we can’t know if our claims about the processes that cause selection are correct or not; the design amounts to a leap of faith in the theoretical model.\nThe problems don’t end there. We risk bias if we fail to adjust for \\(X\\) under some models — but we will also risk bias if we do adjust for \\(X\\) under other models. In Figure 21.1 we illustrate four of the possible roles for an observable variable \\(X\\): as a confounder of the relationship between \\(D\\) and \\(Y\\); as a collider, which is affected by both \\(D\\) and \\(Y\\); as a mediator of the relationship between \\(D\\) and \\(Y\\); and as a predictor of \\(Y\\) with no connection to \\(D\\). We set aside in these DAGs the possible roles of an unobservable variable \\(U\\) that would introduce additional problems of confounding.\nIf \\(X\\) is a confounder, failing to adjust for it in studying the relationship between the treatment \\(D\\) and outcome \\(Y\\) will lead to confounding bias. We often think of fourth DAG as the alternative to this, where \\(X\\) is a predictor of \\(Y\\) but has no link to \\(D\\). In this circumstance, we still want to adjust for \\(X\\) to seek efficiency improvements by soaking up additional variation in \\(Y\\), but failing to do so will not introduce bias. If the true model is definitely represented by either the first or fourth DAG, we should clearly choose to adjust for \\(X\\). In the first case, we should adjust to close the back-door path and in the fourth case, we will do no worse in terms of bias and may in fact increase precision.\nHowever, the middle two DAGs present problems if we do adjust for \\(X\\). In the first, \\(X\\) is a collider: it is affected by both \\(D\\) and \\(Y\\). Adjusting for \\(X\\) if this is the true model introduces collider bias, because we open a backdoor path between \\(D\\) and \\(Y\\) through \\(X\\). We also introduce bias if we control for \\(X\\) if the mediator model (DAG 3) is the true model, wherein \\(D\\) affects \\(X\\) and \\(Y\\) and \\(X\\) affects \\(Y\\) (i.e., \\(X\\) is a mediator for the relationship between \\(D\\) and \\(Y\\)). But the reason here is different: controlling for a mediator adjusts away part of the true treatment effect.\nIn a selection-on-observables design, we must get many features of the model correct, not only about the factors that affect \\(D\\). We must be sure of all the arrows into \\(X\\), \\(D\\), and \\(Y\\) and the order of the causal arrows. In some cases, in natural experiments where selection processes are not randomized by researchers but are nevertheless known, these assumptions can be sustained. In others, we will be making heroic assumptions.\nDeclaration 21.1 explores these considerations, with a model defining the relationship between a causal factor of interest \\(D\\) and outcome \\(Y\\) and an observable confounder \\(X\\), the average treatment effect as the inquiry, a simple measurement strategy, and two estimators with and without adjustment for \\(X\\). We use exact matching as our adjustment strategy.\nWe declare beliefs about the selection process and how \\(D\\), \\(Y\\), and \\(X\\) are related. The model needs to include potential outcomes for the main outcome of interest (\\(Y\\)) and a specification of the assignment of the key causal variable (here, \\(D\\)). Here, we have defined the assignment process as a function of an observable variable \\(X\\). In fact, \\(X\\) is the only variable that affects selection into treatment: \\(X\\) is a binary variable (i.e., two groups), and the probability of treatment is 0.4 when \\(X=0\\) and 0.6 when \\(X=1\\). In addition, we define the potential outcomes for \\(Y\\), which invoke confounding by \\(X\\) because it affects both \\(D\\) and \\(Y\\). We only invoke one possible relationship between \\(X\\), \\(Y\\), and \\(D\\), and so do not consider the possibilities of colliders or mediators.\nIn our model, \\(U\\) is not a confounder, because it affects \\(Y\\) but not \\(D\\); this is a strong excludability assumption on which our causal inferences depend. The assumption is strong because ruling out all unobservable confounders is typically impossible. Most causal factors in nature are affected by many variables, only some of which we can imagine and measure. The first estimator, with adjustment, uses the weights from the exact matching estimator. The matching procedure adjusts for differences in the probability of selection into treatment according to \\(X\\). The second, unadjusted, estimator does not control for \\(X\\) so suffers from unobserved confounding, because we do not adjust for \\(X\\) which predicts both treatment and the outcome."
  },
  {
    "objectID": "library/observational-causal/selection-on-observables.html#design-examples",
    "href": "library/observational-causal/selection-on-observables.html#design-examples",
    "title": "21  Selection-on-observables",
    "section": "\n21.1 Design examples",
    "text": "21.1 Design examples\n\nBateson (2012) uses a selection-on-observables design to estimate the causal effect of crime victimization on political participation from regional barometer survey data. The main specification is justified by the argument that all back-door paths are closed using Ordinary Least Squares regression and a robustness check uses a nearest-neighbor matching approach under equivalent assumptions.\nPrillaman (2022) uses a matching design to compare Indian villages that featured women’s self-help groups to those that did not on rates of women’s political participation.\n\n\n\n\n\nBateson, Regina. 2012. “Crime Victimization and Political Participation.” American Political Science Review 106 (3): 570–87.\n\n\nPrillaman, Soledad Artiz. 2022. “Strength in Numbers: How Women’s Groups Close India’s Political Gender Gap.” American Journal of Political Science."
  },
  {
    "objectID": "library/observational-causal/difference-in-differences.html",
    "href": "library/observational-causal/difference-in-differences.html",
    "title": "22  Difference-in-differences",
    "section": "",
    "text": "We declare a differences-in-differences design in which the effect of a treatment is assessed by comparing changes over time for a unit that gets treated to changes over time in the same period for a unit the does not get treated. The declaration and diagnosis helps clarify when effect heterogeneity threatens causal inferences drawn from this design.\nThe difference-in-differences design compares the pre-to-post-treatment difference in treated units to that of untreated units. We use the over-time change in the untreated units to account for the changes over time in the treated units that are not due to the treatment. Suppose outcomes were rising over time for all units but we only looked at the pre-to-post difference in the treatment group. We might falsely infer the treatment increased outcomes. The logic of the difference-in-differences design is that we can subtract off the trends affecting all units in order to adjust our causal effect estimates.\nThe difference-in-differences design relies on a strong assumption in \\(M\\): the parallel trends assumption. This assumption asserts that the changes (not the levels) in untreated potential outcomes are the same for treated and untreated units. Because this assumption depends on the values of the unrealized (and thus unobservable) control potential outcomes in the treated units after treatment, it cannot be tested. A widely-used diagnostic is to look at the trends in outcomes between the treated and control unit before treatment; this is only an indirect test because the parallel trends assumption concerns the unobserved control trend of the actually treated unit.\nThe design has been famously used for analyses of two periods (before and after) and two groups (treated and untreated) such as a policy change in one state compared to another before and after the policy change. Today, difference-in-differences is most often used in many-period many-group settings with observational panel data. Here, the logic of the two-period two-group design is extended through analogy. Parallel trends between treated and control groups are assumed on average across treated groups and periods. Unfortunately, the analogy holds only under limited circumstances, a fact only recently discovered.\nDeclaration 22.1 describes a 20-period, 20-unit design in which eventually-treated units become treated at different times, a common setting in empirical social science often referred to as the staggered adoption design. The treatment effect of interest might be a state-level policy adopted in 20 states at some point within a 20-year period, so we draw on comparisons before and after policy adoption within states and comparisons across states that have and have not yet adopted treatment. We use the did_multiplegt_tidy function (available in the rdddr companion package) to prepare the output from the DIDmultiplegt package that implements the estimation procedure.\nWe define hierarchical data with time periods nested within groups, such that each of the 20 units have 20 time periods from 1 to 20. We assign units to be treated at some point in the period (D_unit), and confound treatment assignment with unobservable unit-specific features U_unit. (If there were no confounding, we would not need the parallel pretrends assumption.) In addition, the timing of treatment is randomly assigned (D_time). The assignment D then is jointly determined by whether the unit is treated and whether the current period is after the assigned D_time. We allow for unit-specific variation U_unit and time-specific variation U_time that affects the outcome as well as unit-period characteristics U. Potential outcomes are a function of these unit-, time-, and unit-time-specific characteristics, and a treatment effect that varies according to when units are treated (more on the importance of this treatment effect heterogeneity below).\nThe difference-in-difference design typically targets the average treatment effect on the treated (ATT) inquiry. We leverage over time comparisons within units to isolate the causal effect of treatment, and difference out time-varying characteristics by subtracting off the change in untreated units. Unfortunately, except under extremely limited circumstances — exactly homogeneous treatment effects — we will not be able to recover unbiased estimates of the ATT. We can, however, under some circumstances and with some estimators, recover the ATT for a specific subgroup: units in those periods that just switched from untreated to treated. We declare the ATT for these “switchers” as the inquiry.\nSo many answer strategies have been proposed in recent years to address bias in the difference-in-differences design that we cannot summarize them in this short entry. Instead, we illustrate how to assess the properties of two particular estimators under a range of conditions. First, we define the standard two-way fixed effects estimator with fixed effects by time and unit. The two-way fixed effects fits the empirical goal of difference-in-differences: the time fixed effects net out time-varying unit-invariant variables such as seasonality and time trends. The unit fixed effects net out unit-varying variables that are time-invariant like race or age-at-birth of individuals and long-term histories of violence of territories. However, the two-way fixed effects estimator relies on comparisons between units that are treated and units that are not yet treated. When treatment effects differ across units depending on when they are treated (as they do in the design here), then those comparisons will be biased: part of the treatment effect will be subtracted out of the estimate. Our second estimator, proposed by Chaisemartin and d’Haultfoeuille (2020), addresses this problem when estimating the ATT among switchers."
  },
  {
    "objectID": "library/observational-causal/difference-in-differences.html#design-examples",
    "href": "library/observational-causal/difference-in-differences.html#design-examples",
    "title": "22  Difference-in-differences",
    "section": "\n22.1 Design examples",
    "text": "22.1 Design examples\n\nPaglayan (2019) uses a difference-in-differences design to estimate the causal effect of collective bargaining rights for teachers on education spending in the American states.\nCarreri and Dube (2017) uses a difference-in-differences design to estimate the effect of changes in the international price of oil on the election of right-wing politicians in oil-dependent countries.\n\n\n\n\n\nCarreri, Maria, and Oeindrila Dube. 2017. “Do Natural Resources Influence Who Comes to Power, and How?” The Journal of Politics 79 (2): 502–18.\n\n\nChaisemartin, Clément de, and Xavier d’Haultfoeuille. 2020. “Two-Way Fixed Effects Estimators with Heterogeneous Treatment Effects.” American Economic Review 110 (9): 2964–96.\n\n\nPaglayan, Agustina S. 2019. “Public-Sector Unions and the Size of Government.” American Journal of Political Science 63 (1): 21–36."
  },
  {
    "objectID": "library/observational-causal/instrumental-variables.html",
    "href": "library/observational-causal/instrumental-variables.html",
    "title": "23  Instrumental variables",
    "section": "",
    "text": "We declare a design in which a researcher addresses confounding concerns by using an instrumental variable. Under the right conditions, the approach can generate unbiased estimates for treatment effects for units whose treatment status is sensitive to the instrument.\nWhen we cannot credibly assert that we have controlled for all confounding variables as in a selection-on-observables design (see Section (selection-on-observables?)) and when we do not think the parallel trends assumption is likely to hold in a difference-in-differences design (see Section (difference-in-differences?)), we might have to give up on our goal of drawing causal inferences.\nBut occasionally, the world yields an opportunity to sidestep unobserved confounding by generating as-if random variation in a variable that itself affects the treatment variable but does not directly affect the outcome. We call the variable that is as-if randomly assigned by the world an “instrument.”\nInstruments are special. They are variables that are randomly assigned by nature, the government, or other individuals or groups. Usually, genuine random assignments have to be crafted by researchers as part of a deliberate data strategy. Experimenters go to great lengths to randomly expose some units but not others to treatments. When the world provides bona fide random variation in an instrumental variable, it’s a rare and valuable opportunity. By virtue of as-if random assignment, we can learn about the average causal effects of the instrument itself without any further consternation. Conditional on geography and season, weather conditions are as-if randomly assigned, so we can learn about the average effects of rainfall on many outcomes, like crop yields, voter turnout, and attendance at sporting events. Conditional on gender and birth year, draft numbers are randomly assigned by the government, so we can learn about the average effects of being drafted on educational attainment, future earnings, or public policy preferences.\nWe illustrate the logic of instrumental variables with the DAG shown in Figure 23.1. An instrument \\(Z\\) affects an endogenous treatment \\(D\\) which subsequently affects an outcome \\(Y\\). Many other common causes, summarized in terms of unknown heterogeneity \\(U\\), affect both \\(D\\) and \\(Y\\). It is because of this unmeasured confounding that we cannot learn about the effect of \\(D\\) on \\(Y\\) directly using standard tools.\nWe can naturally imagine an inquiry that represents the effect of the instrument (\\(Z\\)) on an outcome of interest such as the effect of the draft number on future earnings. In the terminology of instrumental variables, this inquiry is called the “reduced form” or the “intention-to-treat” (ITT) effect. If it’s really true that the instrument is randomly assigned by the world, estimation of the reduced form effect is straightforward: we estimate the causal effect of \\(Z\\) on \\(Y\\) using, for example, a difference-in-means estimator. Sometimes, we are also interested in the “first-stage” effect of the instrument on the treatment variable. Similarly, we can target this inquiry by studying the causal effect of \\(Z\\) on \\(D\\), for example using the difference-in-means estimator again.\nThe trouble comes when we want to leverage the random assignment of the instrument (\\(Z\\)) to learn about the average causal effect of the treatment variable (\\(D\\)) on the outcome (\\(Y\\)), where \\(D\\) is not randomly assigned but rather affected by \\(Z\\) and also other variables. To do so, we need to understand better how the instrument affects whether units are treated and then how both affect the outcome of interest. We need to understand “compliance” with the instrument.\nWe can define the compliance types of units in terms of the combinations of values the potential outcomes of \\(D\\) take on as a function of the values of the instrument \\(Z\\). The combinations are made up of the value of the treatment if unit \\(i\\) is assigned to control (\\(D_i(Z_i = 0)\\)) and the value if it is assigned to treatment (\\(D_i(Z_i = 1)\\)). With a binary instrument and binary treatment, there are four possible potential outcomes values and thus four types, enumerated in Table ?tbl-compliancetypes. These compliance types are sometimes known as “principal strata” (Frangakis and Rubin 2002).\nNever-takers are those who never take the treatment, no matter what the value of the instrument. Compliers take the value of the treatment they are assigned by the instrument. Defiers do exactly the opposite. When the instrument assigns them to take the treatment, defiers refuse it, but if it assigns them not to, they do take treatment. Like their name suggests, always-takers take the treatment regardless of the value of the instrument.\nWith these types in mind, we can now define a new inquiry that we will be able to target with instrumental variables under a special set of assumptions: a “local” average treatment effect (LATE) among compliers. The “local” qualifier indicates that the effect applies only to the specific group of units whose treatment status changes as a result of the instrument. The LATE is \\(\\E[Y_i(D_i = 1) - Y_i(D_i = 0) | D_i(Z_i = 1) > D_i(Z_i = 0)]\\). This quantity is the average treatment effect among the compliers. The LATE is different from the ATE because it does not average over the effects for never-takers, always-takers, and defiers, but the ATE does.\nWe can adopt an instrumental variables answer strategy — using two-stage least squares for example — to estimate the LATE. But to do so without bias, we need to invoke new assumptions on top of those for randomized experiments. In the case of a binary instrument and a binary treatment, the five assumptions are:\nIf all five of these assumptions are met, it can be shown that the inquiry \\(\\mathrm{LATE} = \\frac{\\mathrm{ATE}_{ \\mathrm{Z\\rightarrow Y} }}{\\mathrm{ATE}_{Z\\rightarrow D}} = \\frac{\\mathrm{Reduced~Form}}{\\mathrm{First~Stage}}\\). It is the quotient of the ATE of the instrument on the outcome divided by the ATE of the instrument on the treatment. This expression underlines the importance of the fifth assumption: if the instrument doesn’t affect the treatment, the first stage is equal to zero and the ratio is undefined.\nMany of the five assumptions are represented in the DAG in Figure 23.1. The exogeneity of the instrument is represented by the exclusion of a causal effect of \\(U\\) on \\(Z\\). The omission of an arrow from \\(Z\\) to \\(Y\\) directly invokes the exclusion restriction. Non-interference is typically not directly represented in DAGs. Monotonicity and the non zero effect of the instrument on the treatment is represented by the causal arrow from \\(Z\\) to \\(D\\).\nMoving to the answer strategy, a plug-in estimator of the LATE is the difference-in-means of the outcome according to the instrument divided by the difference-in-means of the treatment according to the instrument. Equivalently, we can use two-stage least squares, which will yield the identical answer as the ratio of the difference-in-means estimates when no covariates are included in the regression.\nWith the four elements of the design in hand, we declare the model in code, invoking each of the five assumptions in doing so:\nThe exclusion restriction is invoked in omitting a causal direct effect of \\(Z\\) in the \\(Y\\) potential outcomes. We invoke the monotonicity and non-zero effect of \\(Z\\) on \\(D\\) in the \\(D\\) potential outcomes, which have an effect of \\(Z\\) of 1. We invoke the non-interference assumption by excluding effects of the instrument values from other units in the definition of the \\(D\\) and \\(Y\\) potential outcomes. And we invoke the ignorability of \\(Z\\) by randomly assigning it in the assignment step.\nHow can you use this declaration? Many instrumental variables designs involve historical data, such that most remaining design choices are in the answer strategy. Declaring and diagnosing the design can yield insights about how to construct standard errors and confidence intervals and the implications of analysis procedures in which data-dependent tests are run before fitting instrumental variables models — and only fit if the tests pass. But other instrumental variables papers involve prospective data collection, in which an instrument is identified and outcome data (and possibly instrument and treatment data) are collected anew. In such settings, the design diagnosis and redesign tools are just as useful as in any prospective research design: to help select sampling and measurement procedures from the sample size to the number of outcomes to be collected. The key in this case is to build in the features of the instrument, the potential outcomes of treatment receipt, and the potential outcomes of the ultimate outcome of interest and to explore violations of the five assumptions in the model."
  },
  {
    "objectID": "library/observational-causal/instrumental-variables.html#design-examples",
    "href": "library/observational-causal/instrumental-variables.html#design-examples",
    "title": "23  Instrumental variables",
    "section": "\n23.1 Design examples",
    "text": "23.1 Design examples\n\nNellis and Siddiqui (2018) instrument for the share of secular legislators with the as-if randomly assigned victory of secularist candidates in close elections. The authors use this instrument to study the causal effect of secular legislators on religious violence in Pakistan.\nStokes (2016) studies the causal effects of wind turbines on voting for incumbents, instrumenting for the presence or absence of wind turbines with plausibly exogeneous differences in wind speeds across localities.\n\n\n\n\n\nFrangakis, Constantine E., and Donald B Rubin. 2002. “Principal Stratification in Causal Inference.” Biometrics 58 (1): 21–29.\n\n\nNellis, Gareth, and Niloufer Siddiqui. 2018. “Secular Party Rule and Religious Violence in Pakistan.” American Political Science Review 112 (1): 49–67.\n\n\nStokes, Leah C. 2016. “Electoral Backlash Against Climate Policy: A Natural Experiment on Retrospective Voting and Local Resistance to Public Policy.” American Journal of Political Science 60 (4): 958–74."
  },
  {
    "objectID": "library/observational-causal/regression-discontinuity.html",
    "href": "library/observational-causal/regression-discontinuity.html",
    "title": "\n24  Regression discontinuity designs\n",
    "section": "",
    "text": "We declare a design in which the assignment of a treatment is determined by whether a characteristic of a unit exceeds a certain threshold value. We demonstrate through diagnosis the bias-variance tradeoff at the heart of the choice of answer strategies that target the local average treatment effect inquiry, defined right at the cutoff.\nRegression discontinuity designs exploit substantive knowledge that treatment is assigned in a particular way: everyone above a threshold is assigned to treatment and everyone below it is not. Even though researchers do not control the assignment, substantive knowledge about the threshold serves as a basis for a strong causal identification claim.\nThistlewhite and Campbell introduced the regression discontinuity design in the 1960s to study the impact of scholarships on academic success (Thistlethwaite and Campbell 1960). Their insight was that students with a test score just above a scholarship cutoff were plausibly comparable to students whose scores were just below the cutoff, so any differences in future academic success could be attributed to the scholarship itself.\nJust like instrumental variables, regression discontinuity designs identify a local average treatment effect: in this case, the average effect of treatment local to the cutoff. The main trouble with the design is the vanishing amount of data available as we approach the cutoff, so any answer strategy needs to use data that is some distance away from the cutoff. The further away from the cutoff we move, the larger the threat of bias because units become less and less similar.\nRegression discontinuity designs have four components: a running variable \\(X\\), a cutoff, a treatment variable \\(D\\), and an outcome \\(Y\\). The cutoff determines which units are treated depending on the value of the running variable. The running variable might be the Democratic party’s margin of victory at time \\(t-1\\), and the treatment \\(D\\) might be whether the Democratic party won the election in time \\(t-1\\). The outcome \\(Y\\) might be the Democratic vote margin at time \\(t\\).\nA major assumption required for the regression discontinuity design is that the conditional expectation functions — functions that define the expected value of the outcome at every level of the running variable — for both treatment and control potential outcomes are continuous at the cutoff.1\nThe regression discontinuity design is closely related to the selection-on-observables design in that exact knowledge of the assignment mechanism is needed. In Figure 24.1, we illustrate the DAG for the RD design, which includes a treatment with a known assignment mechanism (that units with \\(X>\\mathrm{cutoff}\\) are treated and those below are not). The running variable \\(X\\), is a common cause both of treatment and the outcome, and so must be adjusted for (indicated by the dashed box) in order to identify the effect of treatment on the outcome and avoid confounding bias.\nFigure 24.2 illustrates the major features of the model for the regression discontinuity using simulated data. The untreated units (blue points) are plotted to the left of the cutoff (vertical line), and the treated units (red points) are plotted to the right. The true conditional expectation functions for both the treated and untreated potential outcomes are displayed as colored lines, dashed where they are unobservable and solid where they are observable.\nThe inquiry in a regression discontinuity design is the effect of the treatment exactly at the cutoff. Formally, it is the difference in the conditional expectation functions of the control and treatment potential outcomes when the running variable is exactly zero. The black vertical line in the plot shows this difference.\nThe data strategy is to collect the outcome data for all units within a interval (bandwidth) around the cutoff. The choice of bandwidth involves a tradeoff between bias and variance. The wider we make the bandwidth, the more data we have, and thus the more precision we may be able to achieve. However, the claim of causal identification comes from the comparability of units just above and just below the threshold. Units far from the cutoff are not likely to be comparable: at a minimum they differ in their values of the running variable and if that is correlated with other unobserved heterogeneity then they may differ in that too. Thus, the more units we compare further from the threshold, the more likely we are to induce bias in our estimates of the treatment effect. The ultimate bandwidth chosen for analysis is a function of the data collected in the data strategy and the bandwidth chosen in the answer strategy, which is often the output of an automated process.\nRegression discontinuity answer strategies approximate the treated and untreated conditional expectation functions to the left and right of the cutoff. The choice of answer strategy involves two key choices: the bandwidth of data around the threshold that is used for estimation, and the model used to fit that data. We declare the local polynomial regression discontinuity estimator with the robust bias-corrected inference procedures described in Calonico, Cattaneo, and Titiunik (2014), which fits a nonparametric model to the data and chooses an optimal bandwidth that minimizes bias. This robust estimator is now widely used in practice, because it navigates the bias-variance tradeoff in selecting bandwidths and statistical model specifications in a data-driven manner. The procedure also obviates the need for researchers to select these parameters themselves.\nFigure 24.3 illustrates the tradeoff in bandwidth selection for a one-degree polynomial (linear interaction). We see the bias-variance tradeoff directly. The bias increases the wider the bandwidth and the more we are relying on data further from the threshold whereas the variance (here, the standard deviation of the estimates) is decreasing the more data we add by widening the bandwidth."
  },
  {
    "objectID": "library/observational-causal/regression-discontinuity.html#design-examples",
    "href": "library/observational-causal/regression-discontinuity.html#design-examples",
    "title": "\n24  Regression discontinuity designs\n",
    "section": "\n24.1 Design examples",
    "text": "24.1 Design examples\n\nKirkland (2021) uses a regression discontinuity design to estimate the effect of just electing a mayor with a business background on infrastructure and redistribution spending.\nCarnegie and Samii (2019) uses a regression discontinuity design to compare countries that are just-eligible for lender status to those that are just-ineligible on human rights and democracy policies.\n\n\n\n\n\nCalonico, Sebastian, Matias D Cattaneo, and Rocio Titiunik. 2014. “Robust Nonparametric Confidence Intervals for Regression-Discontinuity Designs.” Econometrica 82 (6): 2295–2326.\n\n\nCarnegie, Allison, and Cyrus Samii. 2019. “International Institutions and Political Liberalization: Evidence from the World Bank Loans Program.” British Journal of Political Science 49 (4): 1357–79.\n\n\nKirkland, Patricia A. 2021. “Business Owners and Executives as Politicians: The Effect on Public Policy.” The Journal of Politics 83 (4): 1652–68.\n\n\nThistlethwaite, Donald L., and Donald T. Campbell. 1960. “Regression-Discontinuity Analysis: An Alternative to the Ex Post Facto Experiment.” Journal of Educational Psychology 51 (6): 309."
  },
  {
    "objectID": "library/experimental-descriptive/index.html",
    "href": "library/experimental-descriptive/index.html",
    "title": "25  Experimental : descriptive",
    "section": "",
    "text": "We study four kinds of experiments for descriptive inference. Audit experiments estimate the fraction of units that discriminate. List experiments estimate the prevalence rate of holding a sensitive characteristic such as drug use or support for an insurgent group. Conjoint experiments measure (aggregations of) preferences over choices such as candidates for election. Experimental behavioral games measure trust.\nWe need audit experiments because we typically cannot identify whether someone discriminates naturally by measuring a single interaction with another person. We need to see how that person interacts with multiple others, some who they might discriminate against and some not, and then compare their behaviors. We also cannot ask people if they discriminate in a survey and expect useful answers: people typically do not think of themselves as discriminatory so we cannot ask them in a survey. Trust games are motivated by the same idea: people may think of themselves as more trusting than they are. The list experiment is motivated by a related concern: people may not answer sensitive questions when they think others can learn their answer and might punish them socially or physically for giving a sensitive answer. If we measured these sensitive characteristics by asking, people might misreport their answers so we use the experiment to ask in a way that provides plausible deniability (but still allows us to estimate the prevalence rate). Conjoints address the problem that it is difficult to learn about multidimensional preferences over choices by asking about a single choice. In each case, the experiment allows us to randomize people into multiple conditions that lets us back out a descriptive characteristic we could not otherwise measure."
  },
  {
    "objectID": "library/experimental-descriptive/audit-experiments.html",
    "href": "library/experimental-descriptive/audit-experiments.html",
    "title": "\n26  Audit experiments\n",
    "section": "",
    "text": "We declare an audit experiment design in which the name of a citizen requesting service from government is randomized to be Latino-sounding or White-sounding and the government official either responds or does not. We then declare an augmented design in which a treatment to reduce discrimination is cross-randomized. The declaration highlights the behavioral assumptions that must be made to interpret the estimated treatment effect of the name as discrimination, and the diagnosis of the anti-discrimination treatment highlights how large a sample would be required for high power.\nAudit experiments are used to measure discrimination against one group in favor of another group. The design is used commonly to measure whether job applications that are otherwise similar but come from candidates from different genders, races, or social backgrounds receive the same rate of job interview invitations. The same approach has been applied to a very wide range of settings, including education, housing, and requests to politicians.\nThe audit experiment design we’ll explore in this chapter has data and answer strategies that are identical as the two-arm trial for causal inference described in Section (two-arm-randomized-experiments?). The difference between an audit study and the typical randomized experiment lies in the model and inquiry. In a two-arm trial, a common (causal) inquiry is the average difference between the treated and untreated potential outcomes, the ATE. In an audit experiment, by contrast, the inquiry is descriptive: what is the fraction of the sample that discriminates?\nWe can hear our colleagues objecting now – the inquiry in an audit study can of course be conceived of as causal! It’s the average effect of signaling membership in one social group on a resume versus signaling membership in another. We agree, of course, that this interpretation is possible and technically correct. But when we think of the inquiry as descriptive, we can better understand how the audit experiment relies on substantive assumptions about the behaviors of people who do and do not discriminate.\nConsider White, Nathan, and Faller (2015), which seeks to measure discrimination against Latinos by election officials by assessing whether election officials respond to emailed requests for information from putatively Latino or White voters. We imagine three types of election officials: those who would always respond to the request (regardless of the emailer’s ethnicity), those who would never respond to the request (again regardless of the emailer’s ethnicity), and officials who discriminate against Latinos. Here, discriminators are defined by their behavior: they would respond to the White voter but not to the Latino voter. These three types are given in Table ?tbl-audittypes.\nOur descriptive inquiry is the fraction of the sample that discriminates: \\(\\E[\\textrm{Type}_i = \\textrm{Anti}~\\textrm{Latino}~\\textrm{discriminator}]\\). Under the behavioral assumptions about these three types enumerated in Table ?tbl-audittypes (whether these types would respond depending on the ethnicity of the sender), \\(\\E[\\textrm{Type}_i = \\textrm{Anti}~\\textrm{Latino}~\\textrm{discriminator}] = \\E[Y_i(Z_i = \\textrm{White}) - Y_i(Z_i = \\textrm{Latino})]\\). Because this is the expected difference between two potential outcomes, we can use a randomized experiment that randomizes ethnicity to measure this descriptive quantity. In the data strategy, we randomly sample from the \\(Y_i(Z_i = \\textrm{White})\\)’s and from the \\(Y_i(Z_i = \\textrm{Latino})\\)’s, then in the answer strategy, we take a difference-in-means, generating an estimate of the fraction of the sample that discriminates.\nSome finer points about these behavioral assumptions. First, we assume that always-responders and never-responders do not engage in discrimination. It could be that some never-responders don’t respond to Latino voters out of racial animus, but do not respond to White voters out of laziness. In this model, such officials would be not be classified as anti-Latino discriminators by assumption. Second, we assume that there are no anti-White discriminators. If there were, then the difference-in-means would not be unbiased for the fraction of anti-Latino discriminators. Instead, it would be unbiased for “net” discrimination, i.e., how much more election officials discriminate against Latinos versus how much they discriminate against Whites. Anti-Latino discrimination and net discrimination are theoretically separate inquiries. To assess whether the no anti-White discriminators assumption is appropriate in a given setting, substantive knowledge is needed. It is not an assumption that can be directly tested empirically.\nDeclaration 26.1 connects the behavioral assumptions we make about subjects to the randomized experiment we use to infer the value of a descriptive quantity. Only never-responders fail to respond to the White request while only always-responders respond to the Latino request. The inquiry is the proportion of the sample that is an anti-Latino discriminator. The data strategy involves randomly assigning the putative ethnicity of the voter making the request and recording whether it was responded to. The answer strategy is compares average response rates by randomly assigned group."
  },
  {
    "objectID": "library/experimental-descriptive/audit-experiments.html#intervening-to-decrease-discrimination",
    "href": "library/experimental-descriptive/audit-experiments.html#intervening-to-decrease-discrimination",
    "title": "\n26  Audit experiments\n",
    "section": "\n26.1 Intervening to decrease discrimination",
    "text": "26.1 Intervening to decrease discrimination\nButler and Crabtree (2017) prompt researchers to “move beyond measurement” in audit studies. Under the model assumptions in the design, audit experiments measure the level of discrimination, but of course they do not do anything to reduce them. To move beyond measurement, we intervene in the world to reduce discrimination in a treatment group but not in a control group, then measure the level of discrimination in both arms using the audit experiment technology.\nThis two-stage design is illustrated in Declaration 26.2. The first half of the design is about causal inference: we want to learn about the effect of the intervention on discrimination. The second half of the design is about descriptive inference – within each treatment arm. We incorporate both stages of the design in the answer strategy, in which the coefficient on the interaction of the intervention indicator with the audit indicator is our estimator of the effect on discrimination.\n\nDeclaration 26.2 Audit experiment intervention study design\n\ndeclaration_17.2 <-\n  # This part of the design is about causal inference\n  declare_model(\n    N = 5000,\n    type_D_0 = sample(\n      size = N,\n      replace = TRUE,\n      x = c(\"Always-Responder\",\n            \"Anti-Latino Discriminator\",\n            \"Never-Responder\"),\n      prob = c(0.30, 0.05, 0.65)\n    ),\n    type_tau_i = rbinom(N, 1, 0.5),\n    type_D_1 = if_else(\n      type_D_0 == \"Anti-Latino Discriminator\" &\n        type_tau_i == 1,\n      \"Always-Responder\",\n      type_D_0\n    )\n  ) +\n  declare_inquiry(\n    ATE = mean((type_D_1 == \"Anti-Latino Discriminator\") -\n                 (type_D_0 == \"Anti-Latino Discriminator\"))\n  ) +\n  declare_assignment(D = complete_ra(N)) +\n  declare_measurement(type = reveal_outcomes(type ~ D)) +\n  # This part is about descriptive inference in each condition!\n  declare_model(\n    Y_Z_white = if_else(type == \"Never-Responder\", 0, 1),\n    Y_Z_latino = if_else(type == \"Always-Responder\", 1, 0)\n  ) +\n  declare_assignment(\n    Z = complete_ra(N, conditions = c(\"latino\", \"white\"))) +\n  declare_measurement(Y = reveal_outcomes(Y ~ Z)) +\n  declare_estimator(Y ~ Z * D, term = \"Zwhite:D\", inquiry = \"ATE\")\n\n\n\nDiagnosis 26.1 (Audit experiment intervention study diagnosis) Even at 5,000 subjects, the power to detect the effect of the intervention is quite poor, at approximately 15%. This low power stems from the small treatment effect (reducing discrimination by 50% from 5.0% to 2.5%) and from the noisy measurement strategy.\n\ndiagnosis_17.1 <- diagnose_design(declaration_17.2)\n\n\n\n\n\n\n\n\nAudit experiment power analysis\n \n power \n    se(power) \n    n_sims \n  \n\n 0.15 \n    0.01 \n    2000"
  },
  {
    "objectID": "library/experimental-descriptive/audit-experiments.html#avoiding-post-treatment-bias",
    "href": "library/experimental-descriptive/audit-experiments.html#avoiding-post-treatment-bias",
    "title": "\n26  Audit experiments\n",
    "section": "\n26.2 Avoiding post-treatment bias",
    "text": "26.2 Avoiding post-treatment bias\nCoppock (2019) discusses how to avoid post-treatment bias when studying how the audit treatment affects the “quality” of responses, such as the tone of an email or the enthusiasm of the hiring call-back. Interestingly, this causal effect is not defined among the discriminators, because they never send emails to Latinos, so those emails never have a tone. The tone inquiry is defined only among always-responders, but estimating this effect without bias is tricky."
  },
  {
    "objectID": "library/experimental-descriptive/audit-experiments.html#design-examples",
    "href": "library/experimental-descriptive/audit-experiments.html#design-examples",
    "title": "\n26  Audit experiments\n",
    "section": "\n26.3 Design examples",
    "text": "26.3 Design examples\n\nBirkelund et al. (2022) conduct harmonized audit experiments in six countries to measure employment discrimination on the basis of gender.\nFang, Guess, and Humphreys (2019) “move beyond measurement” by randomizing a New York City government intervention designed to stop housing discrimination, which was then measured by an audit study design.\n\n\n\n\n\nBirkelund, Gunn Elisabeth, Bram Lancee, Edvard Nergård Larsen, Javier G Polavieja, Jonas Radl, and Ruta Yemane. 2022. “Gender Discrimination in Hiring: Evidence from a Cross-National Harmonized Field Experiment.” European Sociological Review 38 (3): 337–54.\n\n\nButler, Daniel M., and Charles Crabtree. 2017. “Moving Beyond Measurement: Adapting Audit Studies to Test Bias-Reducing Interventions.” Journal of Experimental Political Science 4 (1): 57–67.\n\n\nCoppock, Alexander. 2019. “Avoiding Post-Treatment Bias in Audit Experiments.” Journal of Experimental Political Science 6 (1): 1–4.\n\n\nFang, Albert H, Andrew M Guess, and Macartan Humphreys. 2019. “Can the Government Deter Discrimination? Evidence from a Randomized Intervention in New York City.” The Journal of Politics 81 (1): 127–41.\n\n\nWhite, Ariel R., Noah L. Nathan, and Julie K. Faller. 2015. “What Do I Need to Vote? Bureaucratic Discretion and Discrimination by Local Election Officials.” American Political Science Review 109 (1): 129–42."
  },
  {
    "objectID": "library/experimental-descriptive/list-experiments.html",
    "href": "library/experimental-descriptive/list-experiments.html",
    "title": "\n27  List experiments\n",
    "section": "",
    "text": "Sometimes, subjects might not tell the truth about certain attitudes or behaviors when asked directly. Responses may be affected by sensitivity bias, or the tendency of survey subjects to misreport their answers for fear of negative repercussions if some individual or group learns their true response (Blair, Coppock, and Moor 2020). In such cases, standard survey estimates based on direct questions will be biased. One class of solutions to this problem is to obscure individual responses, providing protection from social or legal pressures. When we obscure responses systematically through an experiment, we can often still identify average quantities of interest. One such design is the list experiment (introduced in Miller (1984)), which asks respondents for the count of the number of “yes” responses to a series of questions including the sensitive item, rather than for a yes or no answer on the sensitive item itself. List experiments give subjects cover by aggregating their answer to the sensitive item with responses to other questions.\nFor example, Creighton and Jamal (2015) study preferences for religious discrimination in immigration policy among Americans. They worried that direct measures of Americans’ willingness to grant citizenship to Muslims would be distorted by sensitivity bias, so they turned to a list experiment. Subjects in the control and treatment groups were asked: “Below you will read [three/four] things that sometimes people oppose or are against. After you read all [three/four], just tell us HOW MANY of them you OPPOSE. We don’t want to know which ones, just HOW MANY.”\n\n(#tab:listquestions) Creighton and Jamal (2015) list experiment conditions\n\n\n\n\n\nControl\nTreatment\n\n\n\nThe federal government increasing assistance to the poor.\nThe federal government increasing assistance to the poor.\n\n\nProfessional athletes making millions of dollars per year.\nProfessional athletes making millions of dollars per year.\n\n\nLarge corporations polluting the environment.\nLarge corporations polluting the environment.\n\n\n\nGranting citizenship to a legal immigrant who is Muslim.\n\n\n\nThe treatment group averaged 2.123 items while the control group averaged 1.904 items, for a difference-in-means estimate of 0.219. Under the usual assumptions of randomized experiments, the difference-in-means is an unbiased estimator for the average treatment effect of being asked to respond to the treated list versus the control list. But our (descriptive) inquiry is the proportion of people who would grant citizenship to a legal immigrant who is Muslim.\nFor the difference-in-means to be an unbiased estimator for that inquiry, we invoke two additional assumptions (Imai 2011):\n\nNo design effects: The count of “yes” responses to the control items is the same whether a respondent is assigned to the treatment or control group.\nNo liars: Subjects with the sensitive trait truthfully increment their count when assigned to the treatment group.\n\nUnder these two extra assumptions, the list experimental estimate of the prevalence of opposition to granting Muslim immigrants citizenship is 21.9%.\n?fig-daglist represents the list experimental design. The no liars assumption is represented by the lack of an edge from sensitivity bias \\(S\\) to the list experiment outcome \\(Y^L\\). The no design effects assumption is not represented on the DAG.\n\n\nFigure 27.1: Directed acyclic graph for the list experiment.\n\n\nDeclaration 27.1 describes a list experimental design. The model includes subjects’ true attitude (Y_star) and whether or not their direct question answers are contaminated by sensitivity bias (S). These two variables combine to determine how subjects will respond when asked directly about support for the policy. The potential outcomes model combines three types of information to determine how subjects will respond to the list experiment: their responses to the three control items (control_count), their true attitude (Y_star), and whether they are assigned to see the treatment or the control list (Z). Our definition of the potential outcomes embeds the no liars and no design effects assumptions.\nThe inquiry is the prevalence rate of the sensitive item. In the data strategy, we randomly assign 50% of our 500 subjects to treatment and the remainder to control. In the survey, we ask subjects the list experiment question (Y_list). Our answer strategy estimates the prevalence rate by calculating the difference-in-means in the list outcome between treatment and control.\n\nDeclaration 27.1 List experiment design\n\ndeclaration_17.3 <-\n  declare_model(\n    N = 500,\n    control_count = rbinom(N, size = 3, prob = 0.5),\n    Y_star = rbinom(N, size = 1, prob = 0.3),\n    potential_outcomes(Y_list ~ Y_star * Z + control_count) \n  ) +\n  declare_inquiry(prevalence_rate = mean(Y_star)) +\n  declare_assignment(Z = complete_ra(N)) + \n  declare_measurement(Y_list = reveal_outcomes(Y_list ~ Z)) +\n  declare_estimator(Y_list ~ Z, .method = difference_in_means, \n                    inquiry = \"prevalence_rate\")\n\n\n\n\nDiagnosis 27.1 (List experiment diagnosis) \ndiagnosands <- declare_diagnosands(\n  bias = mean(estimate - estimand),\n  mean_CI_width = mean(conf.high - conf.low)\n)\ndiagnosis_17.2 <- diagnose_design(declaration_17.3, diagnosands = diagnosands)\n\n\n\n\n\n\n\n\nDiagnosis of a list experiment.\n \n Bias \n    Mean CI width \n  \n\n -0.002 \n    0.325 \n  \n\n\n\nWe see in the diagnosis that the list experiment generates unbiased estimates of the prevalence rate, but it is extremely imprecise: the average width of the confidence interval is enormous at 33 percentage points. If the estimate from a list experiment using this design is 25%, this would imply a confidence interval ranging from about 9% to 41% holding the sensitive item, ranging from rare to common and thus providing only a limited amount of information about the prevalence rate.\n\nThe diagnosis above shows that the list experiment (under its assumptions) is unbiased but is high variance. In the presence of sensitivity bias, direct questions are biased, but are much lower variance. The choice between these two technologies therefore amounts to a bias-variance tradeoff (see Blair, Coppock, and Moor (2020) for more on this point). Declaration 27.1 wraps up both approaches in one design so we can compare them.\n\nDeclaration 27.2 Comparing list experiments with direct questions\n\ndeclaration_17.4 <- \n  declare_model(\n    N = N,\n    U = rnorm(N),\n    control_count = rbinom(N, size = 3, prob = 0.5),\n    Y_star = rbinom(N, size = 1, prob = 0.3),\n    W = case_when(Y_star == 0 ~ 0L,\n                  Y_star == 1 ~ rbinom(N, size = 1, prob = proportion_hiding)),\n    potential_outcomes(Y_list ~ Y_star * Z + control_count)\n  ) +\n  declare_inquiry(prevalence_rate = mean(Y_star)) +\n  declare_assignment(Z = complete_ra(N)) + \n  declare_measurement(Y_list = reveal_outcomes(Y_list ~ Z),\n                      Y_direct = Y_star - W) +\n  declare_estimator(Y_list ~ Z, inquiry = \"prevalence_rate\", label = \"list\") + \n  declare_estimator(Y_direct ~ 1, inquiry = \"prevalence_rate\", label = \"direct\")\n\n\n\nDiagnosis 27.2 (Comparison of list experiment and direct questions diagnosis) Diagnosing this design, we see that at low levels of sensitivity and low sample sizes, the direct question is preferred on RMSE grounds. This is because though the direct question is biased for the prevalence rate in the presence of any sensitivity bias (positive proportion_hiding), it is much more precise than the list experiment. When we have a large sample size, then we begin to prefer the list experiment for its low bias. At high levels of sensitivity, we prefer the list experiment on RMSE grounds despite its inefficiency, because bias will be so large.\n\ndiagnosis_17.3 <- \n  declaration_17.4 |> \n  redesign(proportion_hiding = seq(from = 0, to = 0.3, by = 0.1), \n           N = seq(from = 500, to = 2500, by = 500)) |> \n  diagnose_design()\n\n\n\n\nFigure 27.2: Redesign to illustrate tradeoffs in RMSE between list experiment and direct question\n\n\n\n27.0.1 Design examples\n\nCoppock (2017) compares direct question and list experimental estimates of support for Donald Trump during the 2016 election, finding no evidence for “shy” Trump supporters who misreport their support for Trump for fear of being perceived as racist or sexist by survey-takers.\nCruz (2019) uses a list experiment to estimate the rate of vote buying in the Philippines, though comparison to direct question estimates yields no evidence of sensitivity bias.\n\n\n\n\n\nBlair, Graeme, Alexander Coppock, and Margaret Moor. 2020. “When to Worry about Sensitivity Bias: A Social Reference Theory and Evidence from 30 Years of List Experiments.” American Political Science Review 114 (4): 1297–1315.\n\n\nCoppock, Alexander. 2017. “Did Shy Trump Supporters Bias the 2016 Polls? Evidence from a Nationally-Representative List Experiment.” Statistics, Politics and Policy 8 (1): 29–40.\n\n\nCreighton, Mathew J., and Amaney Jamal. 2015. “Does Islam Play a Role in Anti-Immigrant Sentiment? An Experimental Approach.” Social Science Research 53: 89–103.\n\n\nCruz, Cesi. 2019. “Social Networks and the Targeting of Vote Buying.” Comparative Political Studies 52 (3): 382–411.\n\n\nImai, Kosuke. 2011. “Multivariate Regression Analysis for the Item Count Technique.” Journal of the American Statistical Association 106 (494): 407–16.\n\n\nMiller, Judith Droitcour. 1984. “A New Survey Technique for Studying Deviant Behavior.” PhD thesis, George Washington University."
  },
  {
    "objectID": "library/experimental-descriptive/conjoint-experiments.html",
    "href": "library/experimental-descriptive/conjoint-experiments.html",
    "title": "\n28  Conjoint experiments\n",
    "section": "",
    "text": "We declare a forced-choice conjoint experiment design in which respondents choose one of two profiles in three sets of tasks, each with three attributes. The design highlights the complexity of defining the inquiries for conjoints, and their lower power at common sample sizes.\nConjoint survey experiments have become hugely popular in political science and beyond for describing multidimensional preferences over profiles (Hainmueller, Hopkins, and Yamamoto 2014). The designs have been used to study preferences over political candidates, types of immigrants to admit, neighborhoods to live in, policies to select, and many more questions. Conjoint experiments come in two basic varieties: the single profile design and the forced-choice design. Throughout this chapter, we’ll discuss these studies in the context of hypothetical candidate experiments, in which candidates are described in terms of a number of attributes each of which can take on multiple values, known as levels. In the single profile design, subjects are asked to rate one profile at a time using, for example, a 1 - 7 support scale. In a forced-choice conjoint experiment, subjects are shown two profiles at a time, then asked to make a binary choice between them. Forced choice conjoint experiments are especially useful for studying electoral behavior because they closely mirror the real-world behavior of choosing between two candidates at the ballot box. A similar logic applies to purchasing behavior when consumers have to choose one product over another. Occasionally, forced-choice conjoint experiments are applied even when no real-world analogue for the binary choice exists. For example, we rarely face a binary choice between two immigrants or between two refugees.\nWe take the unorthodox position that conjoint experiments target descriptive, rather than casual inquiries. The reason can be most easily seen in the single profile design case. For concreteness, imagine subjects are presented with one profile at a time that describes the age (young, middle-aged, old), gender (woman, man), and employment sector (public, private) of a candidate for office and are asked to rate their support for the candidate on a 1-7 Likert scale. This set of attributes and levels generates 2 * 3 * 2 = 12 possible profiles. We could ask subjects to rate all 12, but we typically ask them instead to rate only a random subset. If our goal were to estimate the average ratings of each of the 12 profiles, clearly we would be targeting descriptive quantities.\nThe most common inquiry in conjoint experimentation is the Average Marginal Component Effect or AMCE, which summarizes the average difference in preference between two levels of one attribute, averaging over all of the levels of the other attributes. The AMCE for gender, for example, considers the average difference in preference for women candidates versus men candidates among young candidates who work in the private sector, among middle-aged candidates who work in the public sector, and so on for all six combinations. The overall AMCE is a weighted average of all six of these average preference differences, where the weights are given by the relative frequency of each type of candidate. Despite its name, we think of the AMCE as a descriptive quantity. We of course agree there is a sense in which the AMCE is a causal quantity, since it is the average effect on preferences of describing a hypothetical candidate as a man or a woman. But we can see this quantity as descriptive if we just imagine asking subjects about both candidates and describing the difference in their preferences. We then could aggregate these descriptive differences across profiles. The only reason we don’t ask about all possible profiles is that there are far too many to get through in a typical survey, so we ask subjects about a random subset.\nJust like single-profile conjoints, forced-choice conjoints also target descriptive inquiries, but the inquiry is one step removed from raw preferences over profiles. Instead, we aim to describe the fraction of pairwise contests that a profile would win, averaging over all subjects in the experiment. That is, we aim to describe a profile’s average win rate. We can further describe the differences in the average win rate across profiles, for example, among young candidates who work in the private sector, what is the average difference in win rates for women versus men? Just as in the single profile case, the AMCE is a weighted average of these differences, weighted by the relative frequency of each type of candidate.\nHere again, we could think of the AMCE as a causal effect, i.e., the average effect of describing a profile as a woman versus a man. But we can also imagine asking subjects to consider all 12 * 12 = 144 possible pairwise contests, then using those binary choices to fully describe subjects preferences over contests. A forced-choice conjoint asks subjects to rate just a random subset of those contests, since asking about all of them would be impractical.\nOne final wrinkle about the AMCE inquiries, in both the single-profile and forced-choice cases: they are “data-strategy-dependent” inquiries in the sense that, AMCEs average over the distribution of the other profile attributes, and that distribution is controlled by the researcher.1 The AMCE of gender for profiles that do not include partisanship is different from the AMCE of gender for profiles that include partisanship due to masking (discussed below). Further, and more subtly, the AMCE of gender for profiles that are 75% public sector and 25% private sector is different from the AMCE of gender for profiles that are 50% public sector and 50% private sector, because those relative frequencies are part of the very definition of the inquiry. For contrast, consider a vignette-style hypothetical candidate experiment in which all or most of the other candidate features are fixed, save gender. In that design, we estimate an ATE of gender under only one set of conditions, but in the conjoint design, the AMCE averages over ATEs under many sets of conditions. There is a great benefit of doing so: our inferences are not specific to that one set of conditions. But it also means that what conditions inferences depends crucially on researcher choices about which characteristics are chosen and the randomization scheme.\nThe data strategy for conjoints, then, requires making these four choices, in addition to the usual measurement, sampling, and assignment concerns:\nThe right set of attributes is governed by the “masking/satisficing” tradeoff (Bansak et al. 2021). If we don’t include an important attribute (like partisanship in a candidate choice experiment), we’re worried that subjects will partially infer partisanship from other attributes (like race or gender). If so, partisanship is “masked”, and the estimates for the effects of race or gender will be biased by these “omitted variables.” But if we include too many attributes in order to avoid masking, we may induce “satisficing” among subjects, whereby they only take in a little bit of information, enough to make a “good enough” choice among the candidates.\nThe right set of levels to include is a tricky choice. We want to include all of the most important levels, but every additional level harms statistical precision. If an attribute has three levels, it’s like we’re conducting a three-arm trial, so we’ll want to have enough subjects for each arm. The more levels, the lower the precision.\nHow many profiles to rate at the same time is also tricky. Our point of view is that this choice should be guided by the real-world analogue of the survey task. If we’re learning about binary choices between options in the real world, then the forced-choice, paired design makes good sense. If we’re learning about preferences over many possibilities, the single profile design may be more appropriate. That said, the paired design can yield precision gains over the single profile design in the sense that subjects rate two profiles at the same time, so we effectively generate twice as many observations for perhaps less than twice as much cognitive effort.\nFinally, the right number of choice tasks usually depends on the survey budget. We can always add more conjoint tasks and the only cost is the opportunity cost of asking a different question of the survey that may serve another scientific purpose. If we’re worried that respondents will get bored with the task, we can always throw out profile pairs that come later in the survey. Bansak et al. (2021) suggest that you can ask many tasks without much loss of data quality.\nThe declaration of conjoint experiments is complex, so we provide a series of helper functions specifically for forced-choice conjoint design in the rdddr companion software package.\nWe begin by establishing the number of subjects and the number of tasks they will accomplish. We then establish the attributes and their levels (this design assumes complete random assignment of all attributes with equal probabilities). Finally, we describe a utility function that governs subject preferences. This function can be simple, as we have it here, or it can be complex, building in differences in preferences by subject type or other details.\nIn Declaration 28.1, we imagine a forced-choice candidate choice conjoint in which the attributes are gender, party, and region. We sample 500 subjects and ask them to complete three tasks each."
  },
  {
    "objectID": "library/experimental-descriptive/conjoint-experiments.html#design-examples",
    "href": "library/experimental-descriptive/conjoint-experiments.html#design-examples",
    "title": "\n28  Conjoint experiments\n",
    "section": "\n28.1 Design examples",
    "text": "28.1 Design examples\n\nKao and Revkin (2022) uses a conjoint experiment in an Iraqi city that was controlled by the Islamic State to understand residents’ preferences over punishments for civilian collaborators, depending on the type of collaboration they engaged in.\nAguilar, Cunow, and Desposato (2015) conduct a candidate choice experiment to measure the difference in how voters evaluate candidates depending on whether they are identified as a man or a woman in Brazil.\n\n\n\n\n\nAguilar, Rosario, Saul Cunow, and Scott Desposato. 2015. “Choice Sets, Gender, and Candidate Choice in Brazil.” Electoral Studies 39: 230–42. https://doi.org/https://doi.org/10.1016/j.electstud.2015.03.011.\n\n\nBansak, Kirk, Jens Hainmueller, Daniel J. Hopkins, and Teppei Yamamoto. 2021. “Beyond the Breaking Point? Survey Satisficing in Conjoint Experiments.” Political Science Research and Methods 9 (1): 53–71.\n\n\nHainmueller, Jens, Daniel J. Hopkins, and Teppei Yamamoto. 2014. “Causal Inference in Conjoint Analysis: Understanding Multidimensional Choices via Stated Preference Experiments.” Political Analysis 22 (1): 1–30.\n\n\nKao, Kristen, and Mara R. Revkin. 2022. “Retribution or Reconciliation? Post-Conflict Attitudes Toward Enemy Collaborators.” American Journal of Political Science."
  },
  {
    "objectID": "library/experimental-descriptive/behavioral-games.html",
    "href": "library/experimental-descriptive/behavioral-games.html",
    "title": "\n29  Behavioral games\n",
    "section": "",
    "text": "We declare a trust game and explore the implications of using deception in the game setup. The diagnosis highlights how the choices in the first round of a game, which are not randomized, affect our ability to study the behaviors of the player in the second round without bias.\nBehavioral games are often used to study difficult-to-measure characteristics of subjects like risk attitudes, altruism, prejudice, or trust. The approach involves using labs or other mechanisms to control contexts. A high level of control brings two distinct benefits. First, it can eliminate noise: we obtain estimates under a particular well-defined set of conditions rather than estimates generated from averaging over a range of possibly unknown conditions. Second, more subtly, it can prevent various forms of confounding. For instance, outside the lab we might observe how people act when they work on tasks with an outgroup member. But we only observe the responses among those that do work with out group members, not among those that do not. By studying behaviors in a controlled setting we can see how people would react when put into particular situations.\nThe approach holds enormous value. But, as highlighted by Green and Tusicisny (2012), it also introduces many subtle design choices. Many of these can be revealed through declaration and diagnosis.\nWe illustrate using the “trust” game in which we specify three common inquiries and use a standard design in Declaration 29.1. The design is successful at generating unbiased estimates of the first inquiry but runs into problems with the other two.\nThe trust game has been implemented hundreds of times to understand levels and correlates of social trust. Following the meta-analysis given in Johnson and Mislin (2011) we consider a game in which one player (Player 1, the “trustor”) can invest some share of $1. Whatever is invested is then doubled. A second player (Player 2, “the trustee”) can then decide what share of the doubled amount to keep for themself and what share to return to the trustor.\nAs described by Johnson and Mislin (2011), “trust” is commonly measured by the share given and “trustworthiness” is measured by the share returned. With the MIDA framework in mind, we will be more specific and define the inquiry independent of the measurement. We define “trust” as the share that would be invested by a trustor when confronted with a random trustee, whereas “trustworthiness” is the average share that would be returned over a range of possible investments.\nTo motivate M we assume the following decision making model. We assume that each person \\(i\\) seeks to maximize a weighted average of logged payoffs:\n\\[u_i = (1-a_i) \\log(\\pi_i) + a_i \\log(\\pi_{-i})\\]\nwhere \\(\\pi_i, \\pi_{-i}\\) denotes the monetary payoffs to \\(i\\), \\(-i\\) and \\(a_i\\) (“altruism”) captures the weight players place on the (logged) payoffs of other players.\nLet \\(x\\) denote the amount sent by the trustor from the endowment \\(1\\).\nThe trustee then maximizes:\n\\[u_2 = (1-a_2) \\log((1-\\lambda)2x) + a_2 \\log((1-x) + \\lambda 2x)\\]\nwhere \\(\\lambda\\) denotes the share of \\(2x\\) that the trustee returns. Maximizing with respect to \\(\\lambda\\) yields:\n\\[\\lambda = a_2 + (1-a_2)\\frac{x-1}{2x}\\]\nin the interior. Taking account of boundary constraints,1 we have best response function:\n\\[\\lambda(x):= \\max\\left(0, a_2 + (1-a_2)\\frac{x-1}{2x}\\right)\\]\nInterestingly, the share sent back is increasing in the amount sent because player 2 has greater incentive to compensate player 1 for their investment. If the full amount is sent then the share sent back is simply \\(a_2\\).\nGiven this, the trustor chooses \\(x\\) to maximize:\n\\[u_1 = (1-a_1) \\log\\left(1 - x + \\lambda(x)2x\\right) + a_1 \\log\\left(\\left(1-\\lambda(x)\\right)2x\\right)\\]\nIn the interior this reduces to:\n\\[u_1 = (1-a_1) \\log\\left((1 + x)a_2\\right) + a_1 \\log\\left((1-a_2)(1+x)\\right)\\]\nwith greatest returns at \\(x=1\\).\nFor ranges in which no investment will be returned, utility reduces to:\n\\[u_1 = (1-a_1) \\log\\left(1 - x\\right) + a_1 \\log\\left(2x\\right)\\]\nwhich is maximized at: \\(x = a_1\\).\nThe global maximum depends on which of these yields higher utility.\nFigure 29.1 shows the returns to the trustor from different investments given their own and the trustee’s other-regarding preferences. We see that when other-regarding preferences are weak for both players, nothing is given and nothing is returned. When other regarding preferences are strong for player 1, they offer substantial amounts even when nothing is expected in return. When other-regarding preferences are sufficiently strong for player 2, player 1 invests fully in anticipation of a return.\nThe predictions of this model are then used to define the inquiry and predict outcomes in the model declaration. The model part of the design includes information on underlying preferences. For this we make use of a set of functions that characterize stipulated beliefs about behavior.\nThe inquiries for this design are the expected share offered to different types of trustees, the expected returns, averaged over possible offers, and the expected action by a trustee when the full amount is invested. The data strategy involves assigning players to pairs and orderings. The first half is assigned to the trustor role and matched with the second group that are assigned the trustee role. For the answer strategy, we simply measure average behavior across subjects. As a wrinkle, we include the possibility that the experimenter confronts the returners with random offers rather than the ones actually made by their partners. This aspect of the design is controlled by an argument called deceive and turns out to be important for inference.\nA few features are worth highlighting. First, the inquiries are defined using a set of hypothetical responses under the model using a specified response function. However the inquiry is robust to the model in the sense that it remains well defined even if you stipulate very different behaviors. Second, the declaration involves a step where we shift from a “long” data frame with a row per subject to a “wide” data frame with a row per game. Third, the design orders steps so that an estimation stage is implemented before a measurement stage; this is a little unusual but it is done in this way to allow the researchers to analyze Player 1 investment decisions before (possibly) replacing them with fabricated decisions.\nData generated by this design might look like this:\nWe have a row for each game, we have the (unobserved) \\(a_i, a_j\\) parameters as well as actions by both players in the data.\nDiagnosis (lem:diagnosis-17-5?) illustrates the properties of the trust game design."
  },
  {
    "objectID": "library/experimental-descriptive/behavioral-games.html#design-examples",
    "href": "library/experimental-descriptive/behavioral-games.html#design-examples",
    "title": "\n29  Behavioral games\n",
    "section": "\n29.1 Design examples",
    "text": "29.1 Design examples\n\nAvdeenko and Gilligan (2015) use a trust game to measure outcomes in a randomized controlled trial of the effects of local public infrastructure projects in Sudan.\nIyengar and Westwood (2015) use both dictator and trust games to measure partisan antipathy in the United States.\n\n\n\n\n\nAvdeenko, Alexandra, and Michael J. Gilligan. 2015. “International Interventions to Build Social Capital: Evidence from a Field Experiment in Sudan.” American Political Science Review 109 (3): 427–49.\n\n\nGreen, Donald P., and Andrej Tusicisny. 2012. “Statistical Analysis of Results from Laboratory Studies in Experimental Economics: A Critique of Current Practice.” Available at SSRN 2181654.\n\n\nIyengar, Shanto, and Sean J. Westwood. 2015. “Fear and Loathing Across Party Lines: New Evidence on Group Polarization.” American Journal of Political Science 59 (3): 690–707.\n\n\nJohnson, Noel D, and Alexandra A Mislin. 2011. “Trust Games: A Meta-Analysis.” Journal of Economic Psychology 32 (5): 865–89."
  },
  {
    "objectID": "library/experimental-causal/index.html",
    "href": "library/experimental-causal/index.html",
    "title": "30  Experimental : causal",
    "section": "",
    "text": "Many experimental designs for causal inference in the social sciences take advantage of researcher control over the assignment of treatments to assign treatments at random. In the archetypal two-arm randomized trial, a group of \\(N\\) subjects are recruited, \\(m\\) of them are chosen at random to receive treatment and the remaining \\(N-m\\) of them do not receive treatment and serve as controls. The inquiry is the average treatment effect, the answer strategy is the difference-in-means estimator. The strength of the design can be appreciated by analogy to random sampling. The \\(m\\) outcomes in the treatment group represent a random sample from the treated potential outcomes among all \\(N\\) subjects, so the sample mean in the treatment group is a good estimator of the true average treated potential outcome; an analogous claim holds for the control group.\nThe randomization of treatments to estimate average causal effects is a relatively recent human invention. While glimmers of the idea appeared earlier, it wasn’t until at least the 1920s that explicit randomization appeared in agricultural science, medicine, education, and political science (Jamison 2019). Only a few generations of scientists have had access to this tool. Sometimes critics of experiments will charge “you can’t randomize [important causal variable].” There are of course practical constraints on what treatments researchers can control, be they ethical, financial, or otherwise. We think the main constraint is researcher creativity. The scientific history of randomized experiments is short – just because it hasn’t been randomized yet doesn’t mean it can’t be. (By the same token, just because it can be randomized doesn’t mean that it should be.)\nRandomized experiments are rightly praised for their desirable inferential properties, but of course they can go wrong in many ways that designers of experiments should anticipate and minimize. These problems include problems in the data strategy (randomization implementation failures, excludability violations, noncompliance, attrition, and interference between units), problems in the answer strategy (conditioning on post-treatment variables, failure to account for clustering, \\(p\\)-hacking), and even problems in the inquiry (estimator-inquiry mismatches). Of course all these problems apply a fortiori to non-experimental studies, but they are important to emphasize for experimental studies since they are often characterized as being “unbiased” without qualification.\nThe designs in this chapter proceed from the simplest experimental design – the two arm trial – up through very complex designs like the randomized saturation design.\n\n\n\n\nJamison, Julian C. 2019. “The Entry of Randomized Assignment into the Social Sciences.” Journal of Causal Inference 7 (1): 1–16."
  },
  {
    "objectID": "library/experimental-causal/two-arm-randomized-experiments.html",
    "href": "library/experimental-causal/two-arm-randomized-experiments.html",
    "title": "\n31  Two-arm randomized experiments\n",
    "section": "",
    "text": "We declare a canonical two-arm trial, motivate key diagnosands for assessing the quality of the design, use diagnosis and redesign to explore the properties of two-arm trials, and discuss key risks to inference. This entry includes code for a “designer” which lets you quickly design and redesign two-arm trials.\nAll two-arm randomized trials have in common that subjects are randomly assigned to one of two conditions. Canonically, the two conditions include one treatment condition and one control condition. Some two-arm trials eschew the pure control condition in favor of a placebo control condition, or even a second treatment condition. The uniting feature of all these designs is that the model includes two and only two potential outcomes for each unit and that the data strategy randomly assigns which of these potential outcomes will be revealed by each unit.\nA key choice in the design of two-arm trials is the random assignment procedure. Will we use simple random assignment (coin flip, or Bernoulli) or will we use complete random assignment? Will the randomization be blocked or clustered? Will we “restrict” the randomization so that only randomizations that generate acceptable levels of balance on pre-treatment characteristics are permitted? We will explore the implications of some of these choices in the coming sections, but for the moment, the main point is that saying “treatments were assigned at random” is insufficient. We need to describe the randomization procedure in detail in order to know how to analyze the resulting experiment. See Section (treatment-assignment?) for a description of many different random assignment procedures.\nIn this chapter, we’ll consider a canonical two arm-trial design, with complete random assignment in a fixed population, which uses difference-in-means to estimate the average treatment effect. We’ll now unpack this shorthand into the components of M, I, D, and A.\nThe model specifies a fixed sample of \\(N\\) subjects. Here we aren’t imagining that we are first sampling from a larger population. We have in mind a fixed set of units among which we will conduct our experiment: we are conducting “finite sample inference.” Under the model, each unit is endowed with two latent potential outcomes: a treated potential outcome and an untreated potential outcome. The difference between them is the individual treatment effect. In the canonical design, we assume that potential outcomes are “stable,” in the sense that all \\(N\\) units’ potential outcomes are defined with respect to the same treatment and that units’ potential outcomes do not depend on the treatment status of other units. This assumption is often referred to as the “stable unit treatment value assumption,” or SUTVA (Rubin 1980).\nBecause the model specifies a fixed sample, the inquiries are also defined at the sample level. The most common inquiry for a two-arm trial is the sample average treatment effect, or SATE. It is equal to the average difference between the treated and untreated potential outcomes for the units in the sample: \\(\\E_{i\\in N}[Y_i(1) - Y_i(0)]\\). Two-arm trials can also support other inquiries like the SATE among a subgroup (called a conditional average treatment effect, or CATE), but we’ll leave those inquiries to the side for the moment.\nThe data strategy uses complete random assignment in which exactly \\(m\\) of \\(N\\) units are assigned to treatment (\\(Z_i = 1\\)) and the remainder are assigned to control (\\(Z_i = 0\\)). We measure observed outcomes in such a way that we measure the treated potential outcome in the treatment group and untreated potential outcomes in the control group: \\(Y_i = Y_i(1) \\times Z_i + Y_i(0)\\times(1 - Z_i)\\). This expression is sometimes called the “switching equation” because of the way it “switches” which potential outcome is revealed by the treatment assignment. It also embeds the crucial assumption that units reveal the potential outcome they are assigned to reveal. If the experiment encounters noncompliance, this assumption is violated. It’s also violated if “excludability” is violated, i.e., if something other than treatment moves with assignment to treatment. For example, if the treatment group is measured differently from the control group, excludability would be violated.\nThe answer strategy is the difference-in-means estimator with so-called Neyman standard errors. In mathematical notation, if units are ordered with treated units first and control units after, we can wrote both as:\n\\[\\begin{align}\n\\widehat{DIM} &= \\frac{\\sum_1^mY_i}{m} - \\frac{\\sum_{m + 1}^NY_i}{N-m} \\\\\n\\widehat{\\mathrm{se}(DIM)} &= \\sqrt{\\frac{\\widehat{Var}(Y_i(1))}{m} + \\frac{\\widehat{Var}(Y_i(0))}{N-m}}\\\\\n\\end{align}\\]\nThe estimated standard error can be used as an input for two other statistical procedures: null hypothesis significance testing via a \\(t\\)-test and the construction of a 95% confidence interval.\nThe DAG corresponding to a two-arm randomized trial is very simple. An outcome \\(Y\\) is affected by unknown factors \\(U\\) and a treatment \\(Z\\). The measurement procedure \\(Q\\) affects \\(Y\\) in the sense that it measures a latent \\(Y\\) and records the measurement in a dataset. No arrows lead into \\(Z\\) because it is randomly assigned. No arrow leads from \\(Z\\) to \\(Q\\), because we assume no excludability violations wherein the treatment changes how units are measured. This simple DAG confirms that the average causal effect of \\(Z\\) on \\(Y\\) is nonparametrically identified because no back-door paths lead from \\(Z\\) to \\(Y\\)."
  },
  {
    "objectID": "library/experimental-causal/two-arm-randomized-experiments.html#using-covariates-to-increase-precision",
    "href": "library/experimental-causal/two-arm-randomized-experiments.html#using-covariates-to-increase-precision",
    "title": "\n31  Two-arm randomized experiments\n",
    "section": "\n31.1 Using covariates to increase precision",
    "text": "31.1 Using covariates to increase precision\nWhen treatments are randomized, whether we adjust for pre-treatment covariates makes little difference for bias. By contrast, when treatments are not randomized, we often do need on adjust for covariates in order to account for the confounding introduced by “omitted variables” (see (selection-on-observables?)).\nThe purpose of adjusting for covariates in an experimental study is to increase precision. The more predictive the covariates are of the outcome, the more they help the precision of the estimates.\nOne way to think about how much the inclusion of covariates will help precision is to summarize their predictive power in a statistic like \\(R^2\\). The \\(R^2\\) value from a regression of the outcome on the covariates alone (i.e., without the treatment indicator) gives an understanding of how jointly predictive the covariates are. If \\(R^2\\) is close to zero, including the covariates will make almost no difference for precision. If \\(R^2\\) is close to one, we can achieve dramatic increases in precision and statistical power.\nDeclaration 31.2 draws a summary covariate X and unobserved heterogeneity U from a multivariate normal distribution with a specified covariance between the two variables. By redesigning over the values of that correlation, we can learn how covariate adjustment affects precision depending on the level of \\(R^2\\). The answer strategy uses the estimator proposed in Lin (2013) for reasons explained in the next section.\n\nDeclaration 31.2 Two arm trial with covariate adjustment\n\nN <- 100\nr_sq <- 0\n\ndeclaration_18.2 <-\n  declare_model(N = N,\n                draw_multivariate(c(U, X) ~ MASS::mvrnorm(\n                  n = N,\n                  mu = c(0, 0),\n                  Sigma = matrix(c(1, sqrt(r_sq), sqrt(r_sq), 1), 2, 2)\n                )), \n                potential_outcomes(Y ~ 0.1 * Z + U)) +\n  declare_inquiry(ATE = mean(Y_Z_1 - Y_Z_0)) +\n  declare_assignment(Z = complete_ra(N)) +\n  declare_measurement(Y = reveal_outcomes(Y ~ Z)) +\n  declare_estimator(\n    Y ~ Z, covariates = ~X, .method = lm_lin, inquiry = \"ATE\"\n  )\n\n\n\nDiagnosis 31.2 (Two arm trial diagnosis) \n\ndiagnosis_18.2 <- \n  declaration_18.2 |> \n  redesign(r_sq = seq(0.0, 0.9, by = 0.2)) |> \n  diagnose_designs() \n\n\nFigure 31.2 plots sample size on the horizontal axis and diagnosand estimates on the vertical axis. In the left hand panel, we see that, as usual, statistical power increases with sample size. Different values of \\(R^2\\) are distinguished by colored lines. Higher values of \\(R^2\\) lead to higher statistical power. The gains can be dramatic. The no-adjustment benchmark is represented by the \\(R^2 = 0\\) line. We achieve approximately the same statistical power as a 1,000 unit experiment with no adjustment when we the pre-treatment covariates yield an \\(R^2\\) of 0.8 and we have just 200 units. The right panel tells a similar story, though it emphasizes that the marginal benefits of covariate adjustment get smaller as the sample size gets bigger. In any real experimental scenario, designers should take care to generate informed guesses about the probable \\(R^2\\) of the covariates and then explore the tradeoffs between pre-treatment data collection and additional sample size.\n\n\nFigure 31.2: Power and precision increases from covariate adjustment"
  },
  {
    "objectID": "library/experimental-causal/two-arm-randomized-experiments.html#can-controlling-for-covariates-hurt-precision",
    "href": "library/experimental-causal/two-arm-randomized-experiments.html#can-controlling-for-covariates-hurt-precision",
    "title": "\n31  Two-arm randomized experiments\n",
    "section": "\n31.2 Can controlling for covariates hurt precision?",
    "text": "31.2 Can controlling for covariates hurt precision?\nFreedman (2008) critiques the practice of using OLS regression to adjust experimental data. While the difference-in-means estimator is unbiased for the average treatment effect, the covariate-adjusted OLS estimator exhibits a small sample bias (sometimes called “Freedman bias”) that diminishes quickly as sample sizes increase. More worrying is the critique that covariate adjustment can even hurt precision.\nLin (2013) unpacks the circumstances under which this precision loss occurs and offers an alternative estimator that is guaranteed to be at least as precise as the unadjusted estimator. The trouble occurs when the correlation of covariates with the outcome is quite different in the treatment condition from in the control condition and when designs are strongly imbalanced in the sense of having large proportions of treated or untreated units. We refer the reader to this excellent paper for details and the connection between covariate adjustment in randomized experiments and covariate adjustment in random sampling designs. In sum, the Lin estimator deals with the problem by performing covariate adjustment in each arm of the experiment separately, which is equivalent to the inclusion of a full set of treatment-by-covariate interactions. In a clever bit of regression magic, Lin shows how first pre-processing the data by de-meaning the covariates renders the coefficient on the treatment regressor an estimate of the overall ATE. The lm_lin estimator in the estimatr package implements this pre-processing in one step.\nDeclaration 31.3 will help us to explore the precision of three estimators under a variety of circumstances. We want to understand the performance of the difference-in-means, OLS, and Lin estimators depending on how different the correlation between X and the outcome is by treatment arm, and depending on the fraction of units assigned to treatment.\n\nDeclaration 31.3 Lin estimator design\n\nprob = 0.5\ncontrol_slope = -1\n\ndeclaration_18.3 <-\n  declare_model(N = 100,\n                X = runif(N, 0, 1),\n                U = rnorm(N, sd = 0.1),\n                Y_Z_1 = 1*X + U,\n                Y_Z_0 = control_slope*X + U\n  ) +\n  declare_inquiry(ATE = mean(Y_Z_1 - Y_Z_0)) +\n  declare_assignment(Z = complete_ra(N = N, prob = prob)) + \n  declare_measurement(Y = reveal_outcomes(Y ~ Z)) +\n  declare_estimator(Y ~ Z, inquiry = \"ATE\", label = \"DIM\") +\n  declare_estimator(Y ~ Z + X, .method = lm_robust, inquiry = \"ATE\", label = \"OLS\") +\n  declare_estimator(Y ~ Z, covariates = ~X, .method = lm_lin, inquiry = \"ATE\", label = \"Lin\")\n\n\n\n\nDiagnosis 31.3 (Lin estimator diagnosis) \ndiagnosis_18.3 <- \n  diagnosis_18.3 |> \n  redesign(\n    control_slope = seq(-1, 1, 0.5), \n    prob = seq(0.1, 0.9, 0.1)) |> \n  diagnose_designs()\n\nFigure 31.3 considers a range of designs under three possible models. The three models are described by the top row of facets. In all cases, the slope of the treated potential outcomes with respect to \\(X\\) is set to 1. All the way to the left, the slope with respect to the control potential outcomes is set to -1, and all the way to the right, is set to +1. The bottom row of facets shows the performance of three estimators along a range of treatment assignment probabilities.\nWhen the control slope is -1, we can see Freedman’s precision critique. The standard error of the OLS is larger than difference-in-means for many designs, though they coincide when the fraction treated is 50%. This problem persists in some form until the slope of the control potential outcome with respect to \\(X\\) gets close enough to the slope of the treated potential outcomes with respect to \\(X\\).\nAll along this range, however, the Lin estimator dominates OLS and difference-in-means. Regardless of the fraction assigned to treatment and the model of potential outcomes, the Lin estimator achieves equal or better precision than either difference-in-means or OLS.\n\n\nFigure 31.3: Comparing the Lin estimator to OLS and difference-in-means, varying fraction assigned to treatment and correlation of potential outcomes with the covariate."
  },
  {
    "objectID": "library/experimental-causal/two-arm-randomized-experiments.html#examples",
    "href": "library/experimental-causal/two-arm-randomized-experiments.html#examples",
    "title": "\n31  Two-arm randomized experiments\n",
    "section": "\n31.3 Examples",
    "text": "31.3 Examples\n\nPeyton, Sierra-Arévalo, and Rand (2019) conduct a two-arm randomized experiment in which treatment households were assigned to receive a nonenforcement visit from police and control households were not. Outcomes were measured via follow-up survey.\nBalcells, Palanza, and Voytas (2022) use a two-arm randomized experiment to study the effects of a visit to a transitional justice museum in Chile on support for democratic institutions.\n\n\n\n\n\nBalcells, Laia, Valeria Palanza, and Elsa Voytas. 2022. “Do Transitional Justice Museums Persuade Visitors? Evidence from a Field Experiment.” The Journal of Politics 84 (1).\n\n\nFreedman, David A. 2008. “On Regression Adjustments to Experimental Data.” Advances in Applied Mathematics 40 (2): 180–93.\n\n\nLin, Winston. 2013. “Agnostic Notes on Regression Adjustments to Experimental Data: Reexamining Freedman’s Critique.” Annals of Applied Statistics 7 (1): 295–318.\n\n\nPeyton, Kyle, Michael Sierra-Arévalo, and David G. Rand. 2019. “A Field Experiment on Community Policing and Police Legitimacy.” Proceedings of the National Academy of Sciences 116 (40): 19894–98.\n\n\nRubin, Donald B. 1980. “Randomization Analysis of Experimental Data: The Fisher Randomization Test Comment.” Journal of the American Statistical Association 75 (371): 591–93."
  },
  {
    "objectID": "library/experimental-causal/block-randomized-designs.html",
    "href": "library/experimental-causal/block-randomized-designs.html",
    "title": "\n32  Block-randomized experiments\n",
    "section": "",
    "text": "We declare a block-randomized trial in which subjects are assigned to treatment and control conditions within groups. We use design diagnosis to assess the reductions in variance in estimation that can be achieved from block randomization.\nIn a block-randomized experimental design, homogeneous sets of units are grouped together into blocks on the basis of covariates. The ideal blocking would group together units with identical potential outcomes, but since we don’t have access to any outcome information at the moment of treatment assignment, let alone the full set of potential outcomes, we have to make do grouping together units on the basis of covariates we hope are strongly correlated with potential outcomes. The stronger the correlation between blocking variable and the potential outcomes, the more effective the blocking in terms of increasing precision.\nBlocks can be formed in many ways. The can be constructed based on the levels of a single discrete covariate. We might be able to do better by blocking on the intersection of the levels of two discrete covariates. We could coarsen a continuous variable in order to create strata. We could even create matched quartets of units, partitioning the sample into sets of four units that are as similar as possible on many covariates. In any of these cases, we then randomize units within blocks to treatment. All of these procedures fall under the rubric of block random assignment. Methodologists have crafted many algorithms for creating blocks, each with their own tradeoffs in terms of computational speed and efficiency guarantees.\nIn Declaration 32.1, we block our assignment on a binary covariate X. We assign different fractions of each block to treatment to illustrate the notion that probabilities of assignment need not be constant across blocks, and if they aren’t, we need to weight units by the inverse of the probability of assignment to the condition that they are in. In the answer strategy, we adjust for blocks using the Lin (2013) regression adjustment estimator including IPW weights."
  },
  {
    "objectID": "library/experimental-causal/block-randomized-designs.html#why-does-blocking-help",
    "href": "library/experimental-causal/block-randomized-designs.html#why-does-blocking-help",
    "title": "\n32  Block-randomized experiments\n",
    "section": "\n32.1 Why does blocking help?",
    "text": "32.1 Why does blocking help?\nWhy does blocking increase the precision with which we estimate the ATE? One piece of intuition is that blocking rules out “bad” random assignments that exhibit imbalance on the blocking variable. If \\(N\\) = 12 and \\(m\\) = 6, complete random assignment allows choose(12, 6) = 924 possible permutations. If we form two blocks of six units and conduct block random assignment, then there are choose(6, 3) * choose(6, 3) = 400 remaining possible assignments. The assignments that are ruled are those in which too many or too few units in a block are assigned to treatment, because blocking requires that exactly \\(m_B\\) units be treated in each block \\(B\\). When potential outcomes are correlated with the blocking variable, those “extreme” assignments produce estimates that are in the tails of the sampling distribution associated with complete random assignment.1\n\nDiagnosis 32.1 (Diagnosis comparing block random assignment and complete random assignment) This intuition behind blocking is illustrated in Figure 32.1, which shows the sampling distribution of the difference-in-means estimator under complete random assignment. The histogram is shaded according to whether the particular random assignment is permissible under a procedure that blocks on the binary covariate \\(X\\). The sampling distribution of the estimator among the set of assignments that are permissible under blocking is more tightly distributed around the true average treatment effect than the estimates associated with assignments that are not perfectly balanced. Here we can see the value of a blocking procedure – it it rules out by design those assignments that are not perfectly balanced.\n\n\nFigure 32.1: Sampling distribution under complete random assignment, by covariate balance"
  },
  {
    "objectID": "library/experimental-causal/block-randomized-designs.html#design-examples",
    "href": "library/experimental-causal/block-randomized-designs.html#design-examples",
    "title": "\n32  Block-randomized experiments\n",
    "section": "\n32.2 Design examples",
    "text": "32.2 Design examples\n\nKalla, Rosenbluth, and Teele (2018) conduct an audit experiment among legislators in which the gender of a student asking for advice about starting a career in politics was randomized. Units were block-randomized into treatments on the basis the legislators own gender and their state.\nLyall, Zhou, and Imai (2020) use a block-randomized design to evaluate the effect of vocational training and cash transfers on support for combatants among youth in Afghanistan. Matched quartet blocks were created on the basis of district, gender, employment status, displacement status, and exposure to violence.\n\n\n\n\n\nKalla, Joshua, Frances Rosenbluth, and Dawn Langan Teele. 2018. “Are You My Mentor? A Field Experiment on Gender, Ethnicity, and Political Self-Starters.” The Journal of Politics 80 (1): 337–41.\n\n\nLin, Winston. 2013. “Agnostic Notes on Regression Adjustments to Experimental Data: Reexamining Freedman’s Critique.” Annals of Applied Statistics 7 (1): 295–318.\n\n\nLyall, Jason, Yang-Yang Zhou, and Kosuke Imai. 2020. “Can Economic Assistance Shape Combatant Support in Wartime? Experimental Evidence from Afghanistan.” American Political Science Review 114 (1): 126–43."
  },
  {
    "objectID": "library/experimental-causal/cluster-randomized-designs.html",
    "href": "library/experimental-causal/cluster-randomized-designs.html",
    "title": "\n33  Cluster-randomized experiments\n",
    "section": "",
    "text": "We declare a cluster-randomized trial in which subjects are assigned to treatment and control conditions in groups. We use design diagnosis to quantify how the magnitude of the efficiency losses from clustering depend on the intra-cluster correlation.\nWhen whole groups of units are assigned to treatment conditions together, we say that the assignment procedure is clustered. A common example is an education experiment in which the treatment randomized at the classroom level. All students in a classroom are assigned to either treatment or control together; assignments do not vary within classroom. Clusters can be localities, like villages, precincts, or neighborhood. Clusters can be households if treatments are assigned at the household level.\nTypically, cluster randomized trials exhibit higher variance than the equivalent individually-randomized trial. How much higher variance depends on a statistic that can be hard to think about, the intra-cluster correlation (ICC) of the outcome. The total variance can be decomposed into the variance of the cluster means \\(\\sigma^2_{\\textrm{between}}\\) plus the individual variance of the cluster-demeaned outcome \\(\\sigma^2_{\\textrm{within}}\\). The ICC is a number between zero and one that describes the fraction of the total variance that is due to the between variance: \\(ICC = \\frac{\\sigma^2_{\\textrm{between}}}{\\sigma^2_{\\textrm{between}} + \\sigma^2_{\\textrm{within}}}\\). If ICC equals one, then all units within a cluster express the same outcome, and all of the variation in outcomes is due to cluster-level differences. If ICC equals zero, then the cluster means are all identical, but the individuals vary within each cluster. When ICC is one, the effective sample size is equal to the number of clusters. When ICC is zero, the effective sample size is equal to the number of individuals. Since ICC is usually somewhere between these two values, we can see that clustering decreases the effective sample size from the number of individuals. The size of this decrease depends on how similar outcomes are within cluster compared to how similar outcomes are across clusters.\nFor these reasons clustered random assignment is not usually a desirable feature of a design. Sometimes, however, it is useful or even necessary for logistical or ethical reasons for subjects to be assigned together in groups.\nTo demonstrate the consequences of clustering, Declaration 33.1 shows a design in which both the untreated outcome Y_Z_0 and the treatment effect tau_i exhibit intra-cluster correlation. The inquiry is the average treatment effect over individuals which can be defined without reference to the clustered structure of the data. The data strategy employs clustered random assignment. We highlight two features of the clustered assignment. First, the clustered nature of the data does not itself require for clustered assignment. In principle, one could assign treatments at the individual level or subgroup level even if outcomes are correlated within groups. Second, surprisingly, random assignment of clusters to conditions does not guarantee unbiasedness of outcomes when clusters are of unequal size (Middleton 2008; Imai, King, and Nall 2009). The bias stems from the possibility that potential outcomes could be correlated with cluster size. With uneven cluster sizes, the total number of units (the denominator in the mean estimation) in each group bounces around from assignment to assignment. Since the expectation of a ratio is not, in general, equal to the ratio of expectations, any dependence between cluster size and potential outcomes will cause bias. We can address this problem by blocking clusters into groups according to cluster size. If all clusters in a block are of the same size, then then overall size of the treatment group will remain stable from assignment to assignment. For this reason the design below uses clustered assignment blocked on cluster size."
  },
  {
    "objectID": "library/experimental-causal/cluster-randomized-designs.html#design-examples",
    "href": "library/experimental-causal/cluster-randomized-designs.html#design-examples",
    "title": "\n33  Cluster-randomized experiments\n",
    "section": "\n33.1 Design examples",
    "text": "33.1 Design examples\n\nMousa (2020) studies the effects of inter-group contact on tolerance by cluster assigning players in a Christian football league in Iraq to play with four new Muslim teammates or four new Christian teammates, where the clusters are the teams.\nPaluck and Green (2009) cluster assigned communities in Rwanda to radio programs encouraging dissent and disobedience to authorities and measured individual-level outcomes via survey.\n\n\n\n\n\nImai, Kosuke, Gary King, and Clayton Nall. 2009. “The Essential Role of Pair Matching in Cluster-Randomized Experiments, with Application to the Mexican Universal Health Insurance Evaluation.” Statistical Science 24 (1): 29–53.\n\n\nMiddleton, Joel A. 2008. “Bias of the Regression Estimator for Experiments Using Clustered Random Assignment.” Statistics & Probability Letters 78 (16): 2654–59.\n\n\nMousa, Salma. 2020. “Building Social Cohesion Between Christians and Muslims Through Soccer in Post-ISIS Iraq.” Science 369 (6505): 866–70.\n\n\nPaluck, Elizabeth Levy, and Donald P. Green. 2009. “Deference, Dissent, and Dispute Resolution: An Experimental Intervention Using Mass Media to Change Norms and Behavior in Rwanda.” American Political Science Review 103 (4): 622–44."
  },
  {
    "objectID": "library/experimental-causal/subgroup-designs.html",
    "href": "library/experimental-causal/subgroup-designs.html",
    "title": "\n34  Subgroup designs\n",
    "section": "",
    "text": "We declare and diagnose a design that is targeted at understanding the difference in treatment effects between subgroups. The design combines a sampling strategy that ensures reasonable numbers within each group of interest and a blocking assignment strategy to minimize variance.\nSubgroup designs are experimental designs that have been tailored to a specific inquiry, the difference-in-CATEs. A CATE is a “conditional average treatment effect,” or the average treatment effect conditional on membership in some group. A difference-in-CATEs is simply the difference between two CATEs.\nFor example, studies of political communication often have the difference in response to a party cue by subject partisanship as the main inquiry, since Republican subjects tend to respond positively to a Republican party cue, whereas Democratic subjects tend to respond negatively.\nSubgroup designs share much in common with factorial designs, discussed in detail in Section (factorial-experiments?). The main source of commonality is the answer strategy for the difference-in-CATEs inquiry. In subgroup designs and factorial designs, the usual approach is to inspect the interaction term from an OLS regression. The two designs differ because in the subgroup design, the difference-in-CATEs is a descriptive difference. We don’t randomly assign partisanship, so we can’t attribute the difference in response to treatment to partisanship, which could just be marker for the true causes of the difference in response. In the factorial design, we randomize the levels of all treatments, so the differences-in-CATEs carry with them a causal interpretation.\nSince we don’t randomly assign membership in subgroups, how can we optimize the design to target the difference-in-CATEs? Our main data strategy choice comes in sampling. We need to obtain sufficient numbers of both groups in order to generate sharp enough estimates of each CATE, the better to estimate their difference. For example, at the time of this writing, many sources of convenience samples (Mechanical Turk, Lucid, Prolific, and many others) appear to under-represent Republicans, so researchers sometimes need to make special efforts to increase their numbers in the eventual sample.1\nDeclaration 34.1 describes a fixed population of 10,000 units, among whom people with X = 1 are relatively rare (only 20%). In the potential_outcomes call, we build in both baseline differences in the outcome, and also responses to treatment that are oppositely-signed across the two subgroups. Those with X = 0 have a CATE of 0.1 and those with X = 1 have a CATE of 0.1 - 0.2 = -0.1. The true difference-in-CATEs is therefore 20 percentage points.\nIf we were to draw a sample of 1,000 at random, we would expect to yield only 200 people with X = 1. Here we improve upon that through stratified sampling. We deliberately sample 500 units with X = 1 and 500 with X = 0, then block-random-assign the treatment within groups defined by X."
  },
  {
    "objectID": "library/experimental-causal/subgroup-designs.html#design-examples",
    "href": "library/experimental-causal/subgroup-designs.html#design-examples",
    "title": "\n34  Subgroup designs\n",
    "section": "\n34.1 Design Examples",
    "text": "34.1 Design Examples\n\nCollins (2021) conducts a survey experiment that measures the effects of school board meeting style (standard, participatory, or deliberative) on willingness to attend meetings. The author hypothesized that the effects of participatory and deliberative meeting styles would be larger for nonwhite subjects, which motivated an oversample of this type of respondent. The final sample included 1,061 nonwhite subjects and 1,122 white subjects, so the design was well-poised to estimate the conditional average effects of the treatments for both groups, which ended up being quite similar.\nSwire et al. (2017) conduct an experimental study of misinformation and corrections that compares Republican and Democratic subjects beliefs in false statements. These authors oversampled Republicans from the Mechanical Turk platform, since usual samples from the platform underrepresent Republicans. Republicans believed false claims more when they were attributed to President Trump and Democrats believed them less; this difference-in-CATEs is especially precisely estimated because of the over sample. Both partisan groups responded to corrections of false claims by believing them less, by similar amounts.\n\n\n\n\n\nCollins, Jonathan E. 2021. “Does the Meeting Style Matter? The Effects of Exposure to Participatory and Deliberative School Board Meetings.” American Political Science Review 115 (3): 790–804.\n\n\nKlar, Samara, and Thomas J Leeper. 2019. “Identities and Intersectionality: A Case for Purposive Sampling in Survey-Experimental Research.” Experimental Methods in Survey Research: Techniques That Combine Random Sampling with Random Assignment, 419–33.\n\n\nSwire, Briony, Adam J. Berinsky, Stephan Lewandowsky, and Ullrich K. H. Ecker. 2017. “Processing Political Misinformation: Comprehending the Trump Phenomenon.” Royal Society Open Science 4 (3): 1–21."
  },
  {
    "objectID": "library/experimental-causal/factorial-designs.html",
    "href": "library/experimental-causal/factorial-designs.html",
    "title": "\n35  Factorial experiments\n",
    "section": "",
    "text": "We declare and diagnose a simple factorial design in which two different treatments are crossed. The design allows for unbiased estimation of a number of estimands including conditional effects and interaction effects. We highlight the difficulty of achieving statistical power for interaction terms and the risks of treating a difference between a significant conditional effect and a non-significant effect as itself significant.\nIn factorial experiments, researchers randomly assign the level of not just one treatment, but multiple treatments. The prototypical factorial design is a “two-by-two” factorial design in which factor 1 has two levels and so does factor 2. Similarly, a “three-by-three” factorial design has two factors, each of which has three levels. We can entertain any number of factors with any number of levels. For example, a “two-by-three-by-two” factorial design has three factors, two of which have two levels and one of which has three levels. Conjoint experiments are highly factorial, often including six or more factors with two or more levels each (see Section (conjoint-experiments?)).\nFactorial designs can help researchers answer many inquiries, so it is crucial to design factorials with a particular set in mind. Let’s consider the two-by-two case, which is complicated enough. Let’s call the first factor Z1 and the second factor Z2, each of which can take on the values of zero or one. Considering only average effects, this design can support seven separate inquiries:\nThe reason we distinguish between the ATE of Z1 versus the CATEs of Z1 depending on the level of Z2 is that the two factors may “interact.” When factors interact, the effects of Z1 are heterogeneous in the sense that they differ depending on the level of Z2. We often care about the difference-in-CATEs inquiry when we think the effects of one treatment will depend on the level of another treatment.\nHowever, if we are not so interested in the difference-in-CATEs, then factorial experiments have another good justification – we can learn about the ATEs of each treatment for half price, in the sense that we apply treatments to the same subject pool using the same measurement strategy. Conjoint experiments are a kind of factorial design (discussed in Section (conjoint-experiments?)) that often target average treatments effects that average over the levels of the other factors.\nHere we declare a factorial design with two treatments and a normally distributed outcome variable. We imagine that the CATE of Z1 given Z2 = 0 is 0.2 standard units, the CATE of Z2 given Z1 = 0 is equal to 0.1, and the interaction of the two treatments is 0.1."
  },
  {
    "objectID": "library/experimental-causal/factorial-designs.html#avoiding-misleading-inferences",
    "href": "library/experimental-causal/factorial-designs.html#avoiding-misleading-inferences",
    "title": "\n35  Factorial experiments\n",
    "section": "\n35.1 Avoiding misleading inferences",
    "text": "35.1 Avoiding misleading inferences\nThe very poor power for the difference-in-CATEs sometimes leads researchers to rely on a different answer strategy for considering whether the effects of Z1 depend on the level of Z2. Sometimes, researchers will consider the statistical significance of each of Z1’s CATEs separately, then conclude the CATEs are “different” if the effect is significant for one CATE but not the other. This is a bad practice and we’ll show why.\nHere we diagnose over the true values of the Z1 ATE, setting the true interaction term to zero. Our diagnostic question will be, how frequently do we conclude the two CATEs are different, using two different strategies. The first is the usual approach, i.e., we consider the statistical significance of the interaction term. The second considers whether one, but not the other, of the two CATE estimates is significant.\n\n\nDiagnosis 35.2 (Redesign over models) \ndiagnosis_18.8 <- \n  declaration_18.7 |> \n  redesign(\n    CATE_Z1_Z2_0 = seq(0, 0.5, 0.05),\n    CATE_Z2_Z1_0 = 0.2,\n    interaction = 0\n  ) |> \n  diagnose_designs()\n\nFigure 35.2 shows that the error rate when we consider the statistical significance of the interaction term is nominal. Only 5% of the time do we falsely reject the null that the difference-in-CATEs is zero, which is what we expect when we adopt an \\(\\alpha = 0.05\\) threshold. But when we claim “treatment effect heterogeneity!” when one CATE is significant but not the other, we make egregious errors. When the true (constant) average effect of Z1 approaches 0.2, we falsely conclude that the treatment has heterogeneous effects nearly 50% of the time!\n\n\nFigure 35.2: Comparing the significance of CATE estimates generates misleading inferences."
  },
  {
    "objectID": "library/experimental-causal/factorial-designs.html#design-examples",
    "href": "library/experimental-causal/factorial-designs.html#design-examples",
    "title": "\n35  Factorial experiments\n",
    "section": "\n35.2 Design examples",
    "text": "35.2 Design examples\n\nKarpowitz, Monson, and Preece (2017) use a two-by-two factorial design in their experimental study of interventions to increase the number of women elected officials. The first factor is a “demand” treatment in which caucus meetings of party members are read a letter encouraging them to vote for women. The second factor is a “supply” treatment in which caucus leaders encourage specific women to stand for election. Caucus meetings could be assigned to the demand treatment, the supply treatment, both, or neither. Both treatments increase the number of women elected. The difference-in-CATEs (the interaction term) is negative, suggesting diminishing marginal returns to the interventions, though it is imprecisely estimated.\nWilke (2021) conducts a field experiment in South Africa in which treated households are assigned to receive an alarm that directly alerts police to criminal activity, in order to understand how increased access to formal policing channels may discourage mob violence. Outcomes are measured via survey, and embedded in the survey were two additional information treatments about how the police fight crime or fight mob violence. The design is therefore a 2x2x2 design, and indeed the author finds that the mob violence information treatment is more effective among those assigned an alarm in the field experiment.\n\n\n\n\n\nKarpowitz, Christopher F., J. Quin Monson, and Jessica Robinson Preece. 2017. “How to Elect More Women: Gender and Candidate Success in a Field Experiment.” American Journal of Political Science 61 (4): 927–43.\n\n\nWilke, Anna M. 2021. “How Does the State Replace the Community? Experimental Evidence on Crime Control from South Africa.” Unpublished Manuscript."
  },
  {
    "objectID": "library/experimental-causal/encouragement-designs.html",
    "href": "library/experimental-causal/encouragement-designs.html",
    "title": "\n36  Encouragement designs\n",
    "section": "",
    "text": "We declare an encouragement design in which units are assigned to be encouraged to take up a treatment and the average treatment effect is measured among those who comply with the encouragement. The declaration highlights the many changes to the design that are needed to consider noncompliance and what inquiries can be estimated with the design.\nIn many experimental settings, we cannot require units we assign to take treatment to actually take treatment. Nor can we require units assigned to the control group not to take treatment. Instead, we have to content ourselves with “encouraging” units assigned to the treatment group to take treatment and “encouraging” units assigned to the control group not to.\nEncouragements are often only partially successful. Some units assigned to treatment refuse treatment and some units assigned to control find a way to obtain treatment after all. In these settings, we say that experiments encounter “noncompliance.” This section will describe the most common approach to the design and analysis of encouragement trials, and will point out potential pitfalls along the way.\nAny time a data strategy entails contacting subjects in order to deliver a treatment like a bundle of information or some good, noncompliance is a potential problem. Emails go undelivered, unopened, and unread. Letters get lost in the mail. Phone calls are screened, text messages get blocked, direct messages on social media are ignored. People don’t come to the door when you knock, either because they aren’t home or they don’t trust strangers. Noncompliance can affect non-informational treatments as well: goods may be difficult to deliver to remote locations, subjects may refuse to participate in assigned experimental activities, or research staff might simply fail to respect the realized treatment schedule.\nExperimenters who anticipate noncompliance should make compensating adjustments to their research designs (relative to the canonical two arm design). These adjustments ripple through M, I, D, and A."
  },
  {
    "objectID": "library/experimental-causal/encouragement-designs.html#changes-to-the-model",
    "href": "library/experimental-causal/encouragement-designs.html#changes-to-the-model",
    "title": "\n36  Encouragement designs\n",
    "section": "\n36.1 Changes to the model",
    "text": "36.1 Changes to the model\nThe biggest change to M is developing beliefs about compliance types, also called “principal strata” (Frangakis and Rubin 2002). In a two-arm trial, subjects can be one of four compliance types, depending on how their treatment status responds to their treatment assignment. The four types are described in Table ?tbl-compliancetypes2. \\(D_i(Z_i = 0)\\) is a potential outcome – it is the treatment status that unit \\(i\\) would express if assigned to control. Likewise, \\(D_i(Z_i = 1)\\) is the treatment status that unit \\(i\\) would express if assigned to treatment. These potential outcomes can take each take on a value of 0 or 1, so their intersection allows for four types. For Always-takers, \\(D_i\\) is equal to 1 regardless of the value of \\(Z\\) – they always take treatment. Never-takers are the opposite – \\(D_i\\) is equal to 0 regardless of the value of \\(Z_i\\). For Always-takers and Never-takers, assignment to treatment does not change whether they take treatment.\nCompliers are units that take treatment if assigned to treatment and do not take treatment if assigned to control. Their name “compliers” connotes that something about their disposition as subjects makes them “compliant” or otherwise docile, but this connotation is misleading. Compliance types are generated by the confluence of subject behavior and data strategy choices. Whether or not a subject answers the door when the canvasser comes calling is a function many things, including whether the subject is at home and whether they open the door to canvassers. Data strategies that attempt to deliver treatments in the evenings or on weekends might generate more (or different) compliers than those that attempt treatment during working hours.\n\n(#tab:compliancetypes2) Compliance types\n\nCompliance Type\n\\(D_i(Z_i = 0)\\)\n\\(D_i(Z_i = 1)\\)\n\n\n\nNever-taker\n0\n0\n\n\nComplier\n0\n1\n\n\nDefier\n1\n0\n\n\nAlways-taker\n1\n1\n\n\n\nThe last compliance type to describe are defiers. These strange birds refuse treatment when assigned to treatment, but find a way to obtain treatment when assigned to control. Whether or not “defiers” exist turns out to be a consequential assumption that must be made in the model. We have good reason to believe that defiers are rare – assignment to treatment almost always has a positive average effect on treatment take-up.\nA unit’s compliance type is usually not possible to observe directly. Subjects assigned to the control group who take take treatment (\\(D_i(0) = 1\\)) could be defiers or always-takers. Subjects assigned to the treatment group who do not take treatment (\\(D_i(1) = 0\\)) could be defiers or never-takers. Our inability to be sure of compliance types is another facet of the fundamental problem of causal inference. Even though a subject’s compliance type (with respect to a given design) is a stable trait, it is defined by how the subject would act in multiple counterfactual worlds. We can’t tell what type a unit is because we would need to see whether they take treatment when assigned to treatment and also when assigned to control."
  },
  {
    "objectID": "library/experimental-causal/encouragement-designs.html#changes-to-the-inquiry",
    "href": "library/experimental-causal/encouragement-designs.html#changes-to-the-inquiry",
    "title": "\n36  Encouragement designs\n",
    "section": "\n36.2 Changes to the inquiry",
    "text": "36.2 Changes to the inquiry\nThe inclusion of compliance types in the model also necessitate changes to the inquiry. Always-takers and never-takers present a real problem for causal inference. Even with the power to randomly assign, we can’t change what treatments these units take. As a result, we don’t get to learn about the effects of treatment among these groups. Even if our inquiry were the average effect of treatment among the never-takers, the experiment (as designed) would not be able to generate empirical estimates of it.1 Our inquiry has to fall back to the average effects among those units that whose treatment status we can successfully encourage to change – the compliers.\nThis inquiry is called the complier average causal effect (the CACE). It is defined as \\(\\E[Y_i(1) - Y_i(0) | d_i(1) > d_i(0)]\\). Just like the average treatment effect, it refers to an average over individual causal effects, but this average is taken over a specific subset of units, the compliers. Compliers are the only units for whom \\(d_i(1) > d_i(0)\\), because for compliers, \\(d_i(1) = 1\\) and \\(d_i(0) = 0\\). When assignments and treatments are binary, the CACE is mathematically identical to the local average treatment effect (LATE) described in Chapter (sec?): p3iv. Whether we write CACE or LATE sometimes depends on academic discipline, with LATE being more common among economists and CATE more common among political scientists. An advantage of “CACE” over “LATE” is that it is specific about which units the effect is “local” to – it is local to the compliers.\nWhen experiments encounter noncompliance, the CACE is usually the most important inquiry for theory, since it refers to an average effect of the causal variable, at least for a subset of the units in the study. However, two other common inquiries are important to address here as well.\nThe first is the intention-to-treat (ITT) inquiry, which is defined as \\(\\E[Y_i(D_i(Z = 1), Z = 1) - Y_i(D_i(Z = 0), Z = 0)]\\). The encouragement itself \\(Z\\) has a total effect on \\(Y\\) that is mediated in whole or in part by the treatment status. Sometimes the ITT is the policy-relevant inquiry, since it describes what would happen if a policy maker implemented the policy in the same way as the experiment, inclusive of noncompliance. Consider an encouragement design to study the effectiveness of a tax webinar on tax compliance. Even if the webinar is very effective among people willing to watch it (the CACE is large), the main trouble faced by the policy maker will be getting people to sit through the webinar. The ITT describes the average effect of inviting people to the webinar, which could be quite small if very few people are willing to join.\nThe second additional inquiry is the compliance rate, sometimes referred to as the \\(\\mathrm{ITT}_{\\textrm{D}}\\). It describes the average effect of assignment on treatment, and is written \\(\\E[(D_i(Z = 1) - D_i(Z = 0)]\\). A small bit of algebra shows that the \\(\\mathrm{ITT}_{\\textrm{D}}\\) is equal to the fraction of the sample that are compliers minus the fraction that are defiers.\nThese three inquiries are tightly related. Under five very important assumptions (described below), we can write:\n\\[\\begin{align*}\n\\mathrm{CACE} = \\frac{\\mathrm{ITT}}{\\mathrm{ITT}_{\\mathrm{D}}}\n\\end{align*}\\]\nA derivation of this relationship is given in Section (instrumental-variables?) on instrumental variables. The five assumptions described in that section are identical to the assumptions required here. In an experimental setting, “exogeneity of the instrument” is guaranteed by features of the data strategy. Since we use random assignment, we know for sure that the “instrument” (the encouragement) is exogenous. Excludability of the instrument refers to the idea that the effect of the encouragement on the outcome is fully mediated by the treatment. This assumption could be violated if the mere act of encouragement changes outcomes. Stated differently, if never-takers or always-takers reveal different potential outcomes in treatment and control (\\(Y_i(D_i(Z = 1), Z = 1) \\neq Y_i(D_i(Z = 0), Z = 0)\\)), it must be because encouragement itself changes outcomes. Non-interference in this setting means that units’ treatment status and outcomes do not depend on the assignment or treatment status of other units. In an experimental context, the assumption of monotonicity rules out the existence of defiers. This assumption is often made plausible by features of the data strategy (perhaps it is impossible for those who are not assigned to treatment to obtain treatment) or features of the model (“defiant” responses to encouragement are behaviorally unlikely). The final assumption – nonzero effect of the instrument on the treatment – can also be assured by features of the data strategy. In order to learn about the effects of treatment, data strategies must successfully encourage at least some units to take treatment."
  },
  {
    "objectID": "library/experimental-causal/encouragement-designs.html#changes-to-the-data-strategy",
    "href": "library/experimental-causal/encouragement-designs.html#changes-to-the-data-strategy",
    "title": "\n36  Encouragement designs\n",
    "section": "\n36.3 Changes to the data strategy",
    "text": "36.3 Changes to the data strategy\nWhen experimenters expect that noncompliance will be a problem, they should take steps to mitigate that problem in the data strategy. Sometimes doing so just means trying harder: investigating the patterns of noncompliance, attempting to deliver treatment on multiple occasions, or offering subjects incentives for participation. “Trying harder” is about turning more subjects into compliers by choosing a data strategy that encounters less noncompliance.\nA second important change to the data strategy is the explicit measurement of treatment status as distinct from treatment assignment. For some designs, measuring treatment status is easy. We just record which units were treated and which were untreated. But in some settings, measuring compliance is trickier. For example, if treatments are emailed, we might never know if subjects read the email. Perhaps our email service will track read receipts, in which case one facet of this measurement problem is solved. We won’t know, however, how many subjects read the subject line – and if the subject line contains any treatment information, then even subjects who don’t click on the email may be “partially” treated. Our main advice is to measure compliance in the most conservative way: if treatment emails bounce altogether, then subjects are not treated.\nIn multiarm trials or with continuous rather than binary instruments, noncompliance becomes a more complex problem to define and address through the data strategy and answer strategy. We must define complier types according to all of the possible treatment conditions. For multiarm trials, the complier types for the first treatment may not be the same for the second treatment; in other words, units will comply at different rates to different treatments. Apparent differences in complier average treatment effects and intent-to-treat effects, as a result, may reflect not differences in treatment effects but different rates of compliance."
  },
  {
    "objectID": "library/experimental-causal/encouragement-designs.html#changes-to-the-answer-strategy",
    "href": "library/experimental-causal/encouragement-designs.html#changes-to-the-answer-strategy",
    "title": "\n36  Encouragement designs\n",
    "section": "\n36.4 Changes to the answer strategy",
    "text": "36.4 Changes to the answer strategy\nEstimation of the CACE is not as straightforward subsetting the analysis to compliers. A plug-in estimator of the CACE with good properties takes the ratio of the \\(ITT\\) estimate to the \\(ITT_d\\) estimate. Since the \\(ITT_d\\) must be a number between zero and one, this estimator “inflates” the \\(ITT\\) by the compliance rate. Another way of thinking about this is that the \\(ITT\\) is deflated by all the never-takers and always-takers, among whom the \\(ITT\\) is by construction 0, so instead of “inflating”, we are “re-inflating” the ITT to the level of the CACE. Two-stage least squares in which we instrument the treatment with the random assignment is a numerically equivalent procedure when treatment and assignments are binary. Two-stage least squares has the further advantage of being able to seamlessly incorporate covariate information to increase precision.\nTwo alternative answer strategies are biased and should be avoided. An “as-treated” analysis ignores the encouragement \\(Z\\) and instead compares units by their revealed treatment status \\(D\\). This procedure is prone to bias because those who come to be treated may differ systematically from those who do not. The “per protocol” analysis is similarly biased. It drops any unit that fails to comply with its assignment, but those who take treatment in the treatment group (compliers and always-takers) may differ systematically from those who do not take treatment in the control group (compliers and never-takers). Both the “as-treated” and “per-protocol” answer strategies suffer from a special case of post-treatment bias, wherein conditioning on a post-assignment variable (treatment status) essentially de-randomizes the study.\nDeclaration 37.1 elaborates the model to include the four compliance types, setting the share of defiers to zero to match the assumption of monotonicity. It imagines that the potential outcomes of the outcomes \\(Y\\) with respect to the treatment \\(D\\) are different for each compliance type, reflecting the idea that compliance type could be correlated with potential outcomes. The declaration also links compliance type to the potential outcomes of the treatment \\(D\\) with respect to the randomized encouragement \\(Z\\). We then move on to declaring two inquiries (the CACE and the ATE) and three answer strategies (two-stage least squares, as-treated analysis, and per-protocol analysis).\n\nDeclaration 36.1 Encouragement design\n\ndeclaration_18.8 <-\n  declare_model(\n    N = 100,\n    type = \n      rep(c(\"Always-Taker\", \"Never-Taker\", \"Complier\", \"Defier\"),\n          c(0.2, 0.2, 0.6, 0.0)*N),\n    U = rnorm(N),\n    # potential outcomes of Y with respect to D\n    potential_outcomes(\n      Y ~ case_when(\n        type == \"Always-Taker\" ~ -0.25 - 0.50 * D + U,\n        type == \"Never-Taker\" ~ 0.75 - 0.25 * D + U,\n        type == \"Complier\" ~ 0.25 + 0.50 * D + U,\n        type == \"Defier\" ~ -0.25 - 0.50 * D + U\n      ),\n      conditions = list(D = c(0, 1))\n    ),\n    # potential outcomes of D with respect to Z\n    potential_outcomes(\n      D ~ case_when(\n        Z == 1 & type %in% c(\"Always-Taker\", \"Complier\") ~ 1,\n        Z == 1 & type %in% c(\"Never-Taker\", \"Defier\") ~ 0,\n        Z == 0 & type %in% c(\"Never-Taker\", \"Complier\") ~ 0,\n        Z == 0 & type %in% c(\"Always-Taker\", \"Defier\") ~ 1\n      ),\n      conditions = list(Z = c(0, 1))\n    )\n  ) +\n  declare_inquiry(\n    ATE = mean(Y_D_1 - Y_D_0),\n    CACE = mean(Y_D_1[type == \"Complier\"] - Y_D_0[type == \"Complier\"])) +\n  declare_assignment(Z = conduct_ra(N = N)) +\n  declare_measurement(D = reveal_outcomes(D ~ Z),\n                      Y = reveal_outcomes(Y ~ D)) +\n  declare_estimator(\n    Y ~ D | Z,\n    .method = iv_robust,\n    inquiry = c(\"ATE\", \"CACE\"),\n    label = \"Two stage least squares\"\n  ) +\n  declare_estimator(\n    Y ~ D,\n    .method = lm_robust,\n    inquiry = c(\"ATE\", \"CACE\"),\n    label = \"As treated\"\n  ) +\n  declare_estimator(\n    Y ~ D,\n    .method = lm_robust,\n    inquiry = c(\"ATE\", \"CACE\"),\n    subset = D == Z,\n    label = \"Per protocol\"\n  )\n\n\nFigure 36.1 represents the encouragement design as a DAG. No arrows lead into \\(Z\\), because the treatment was randomly assigned. The compliance type \\(C\\), the assignment \\(Z\\), and unobserved heterogeneity \\(U\\) conspire to set the level of \\(D\\). The outcome \\(Y\\) is affected by the treatment \\(D\\) of course, but also by compliance type \\(C\\) and unobserved heterogeneity \\(U\\). The required exclusion restriction that \\(Z\\) only affect \\(Y\\) through \\(D\\) is represented by the lack of an arrow from \\(Z\\) to \\(Y\\). The deficiencies of the as-treated and per-protocol analysis strategies can be learned from the DAG as well. \\(D\\) is a collider, so conditioning on it would open up back-door paths between \\(Z\\), \\(C\\), and \\(U\\), leading to bias of unknown direction and magnitude.\n\n\nFigure 36.1: Directed acyclic graph of the encouragement design\n\n\n\nDiagnosis 36.1 (Diagnosis of encouragement design) The design diagnosis shows the sampling distribution of the three answer strategies and compares it to two potential inquiries: the complier average causal effect and the average treatment effect. Our preferred method, two-stage least squares, is biased for the ATE. Because we can’t learn about the effects of treatment among never-takers or always-takers, any estimate of the true ATE will be necessarily be prone to bias, except in the happy circumstance that never-takers and always-takers happen to be just like compliers in terms of their potential outcomes.\nTwo-stage least squares does a much better job of estimating the complier average causal effect. Even though the sampling distribution is wider than those for the per-protocol and as-treated analysis, it is at least centered on a well-defined inquiry. By contrast, the other two answer strategies are biased for either target.\n\ndiagnosis_18.9 <- diagnose_design(declaration_18.8)\n\n\n\nFigure 36.2: Sampling distributions of the two-stage least squares, per protocol, and as-treated answer strategies."
  },
  {
    "objectID": "library/experimental-causal/encouragement-designs.html#design-examples",
    "href": "library/experimental-causal/encouragement-designs.html#design-examples",
    "title": "\n36  Encouragement designs\n",
    "section": "\n36.5 Design examples",
    "text": "36.5 Design examples\n\nScacco and Warren (2018) randomize young men in Nigeria to participate in a vocational training program – 84% of subjects assigned to the training participated, but the remainder did not. The authors attempted to measure outcomes for all subjects, regardless of treatment or compliance status and estimated intention-to-treat effects in all cases.\nBlair et al. (2022) randomize communities in Colombia to receive a program aimed at improving local governance through enhanced cooperation between state and local agencies. Some communities assigned to participate in the program did not, or did not participate fully. The authors present the intention-to-treat estimates of treatment effects as well as complier average causal effect estimates, varying the definition of compliance to include or exclude partial compliance (defining compliance as “any compliance including partial compliance” is the conservative choice, as defining partial compliers as noncompliers could violate excludability.)\n\n\n\n\n\nBlair, Robert A., Manuel Moscoso-Rojas, Andres Vargas Castillo, and Michael Weintraub. 2022. “Preventing Rebel Resurgence After Civil War: A Field Experiment in Security and Justice Provision in Rural Colombia.” American Political Science Review, 1–20.\n\n\nFrangakis, Constantine E., and Donald B Rubin. 2002. “Principal Stratification in Causal Inference.” Biometrics 58 (1): 21–29.\n\n\nScacco, Alexandra, and Shana S. Warren. 2018. “Can Social Contact Reduce Prejudice and Discrimination? Evidence from a Field Experiment in Nigeria.” American Political Science Review 112 (3): 654–77."
  },
  {
    "objectID": "library/experimental-causal/placebo-controlled-experiments.html",
    "href": "library/experimental-causal/placebo-controlled-experiments.html",
    "title": "\n37  Placebo-controlled experiments\n",
    "section": "",
    "text": "We compare an encouragement design to a placebo-controlled trial in which units are selected into treatment based on whether they receive either treatment or a placebo treatment with a similar deployment method. At low levels of compliance, the diagnosis reveals the placebo-controlled design is preferred, but then the encouragement design is preferred as compliance increases.\nIn common usage, the notion of a placebo is a treatment that carries with it everything about the bona fide treatment – except the active ingredient. We’re used to thinking about placebos in terms of the “placebo effect” in medical trials. Some portion of the total effect of the actual treatment is due to the mere act of getting treated, so the administration of placebo treatments can difference this portion off. Placebo-controlled designs abound in the social sciences too for similar purposes (see Porter and Velez 2021). Media treatments often work through a bundle of priming effects and new information; a placebo treatment might include only the prime but not the information. The main use of placebos is to difference off the many small excludability violations involved in bundled treatments the better to understand the main causal variable of interest.\nIn this chapter, we study the use of placebos for a different purpose: to combat the negative design consequences of noncompliance in experiments. As described in the previous chapter, a challenge for experiments that encounter noncompliance is that we do not know for sure who the compliers are. Compliers are units that would take treatment if assigned to treatment, but would not do so if assigned to control. Compliers are different from always-takers and never-takers in that assignment to treatment actually changes which potential outcome they reveal.\nIn the placebo-controlled design, we attempt to deliver a real treatment to the treatment group and a placebo treatment to the placebo group, then we conduct our analysis among those units that accept either treatment. This design solves two problems at once. First, it lets us answer a descriptive question: “Who are the compliers?” Second, it lets answer a causal causal question: “What is the average effect of treatment among compliers?”\nEmploying a placebo control can seem like an odd design choice – you go to all the effort of contacting a unit but at the very moment you get in touch, you deliver a placebo message instead of the treatment message. It turns out that despite this apparent waste, the placebo-controlled design can often lead to more precise estimates than the standard encouragement design. Whether it does or not depends in large part on the underlying compliance rate.\nDeclaration 37.1 actually includes two separate designs. Here we’ll directly compare the standard encouragement design to the placebo-controlled design. They have identical models and inquiries, so we’ll just declare those once, before declaring the specifics of the empirical strategies for each design."
  },
  {
    "objectID": "library/experimental-causal/placebo-controlled-experiments.html#design-examples",
    "href": "library/experimental-causal/placebo-controlled-experiments.html#design-examples",
    "title": "\n37  Placebo-controlled experiments\n",
    "section": "\n37.1 Design examples",
    "text": "37.1 Design examples\n\nD. Broockman and Kalla (2016) use a placebo-controlled design in their study of a transphobia-reduction canvassing treatment. Households were assigned either a placebo (a conversation about recycling) or the treatment; analysis was conducted among those who opened the door to the canvasser.\nWilke, Green, and Cooper (2020) extend the placebo-controlled design in a media experiment in Uganda. Film festival attendees were assigned to watch public service announcements on one or two of three topics; post-treatment attitudes about all three topics were measured for all subjects. Subjects who saw the treatment on a given topic served as placebo controls for subjects who saw treatments on other topics. Under the maintained placebo assumption that treatments on one topic won’t affect attitudes on other topics, this design allows for efficient, unbiased inference for the effects of multiple treatments on their targeted outcomes.\n\n\n\n\n\nBroockman, David E., Joshua L. Kalla, and Jasjeet S. Sekhon. 2017. “The Design of Field Experiments with Survey Outcomes: A Framework for Selecting More Efficient, Robust, and Ethical Designs.” Political Analysis 25 (4): 435–64.\n\n\nBroockman, David, and Joshua Kalla. 2016. “Durably Reducing Transphobia: A Field Experiment on Door-to-Door Canvassing.” Science 352 (6282): 220–24.\n\n\nPorter, Ethan, and Yamil Velez. 2021. “Placebo Selection in Survey Experiments: An Agnostic Approach.” Political Analysis, 1–14.\n\n\nWilke, Anna M., Donald P. Green, and Jasper Cooper. 2020. “A Placebo Design to Detect Spillovers from an Education–Entertainment Experiment in Uganda.” Journal of the Royal Statistical Society: Series A (Statistics in Society) 183 (3): 1075–96."
  },
  {
    "objectID": "library/experimental-causal/stepped-wedge-designs.html",
    "href": "library/experimental-causal/stepped-wedge-designs.html",
    "title": "\n38  Stepped-wedge experiments\n",
    "section": "",
    "text": "We declare a stepped-wedge design in which units are assigned a sequence of treatments across multiple periods. In each period, one third are treated successively. Diagnosis of this design compared to similar-cost standard two-arm trials, and suggests that a double-sized two-arm trial is preferable in terms of power but that the stepped-wedge is useful when the number of study units is limited.\nWe often face an ethical dilemma in allocating treatments to some units but not others, since we would rather not withhold treatment from anyone. However, practical constraints often make it impossible to allocate treatments to everyone at the same time. In these circumstances, a stepped-wedge experiment, also known as a waitlist design, can help. Under a stepped-wedge design, we follow an allocation rule that randomly assigns a portion of units to treatment in each of one or more periods, and then in a final period, everyone is allocated treatment. We conduct posttreatment measurement after each period except for the last one. Figure 38.1 illustrates the allocation procedure. A common design is allocating one third to treatment in the first period, an additional third in the second period, and the remaining third in the final period.\nOur model describes unit-specific effects, time-specific effects, and time trends in the potential outcomes. Our inquiry is the average treatment effect among time periods before the last period, since in the stepped-wedge design, we don’t obtain information about the control potential outcome in the final period. In the data strategy, the assign treatment by randomly assigning the wave each unit will receive treatment. We use cluster assignment at the unit level because the data is at the unit-period level. We then transform this treatment variable into a unit-period treatment indicator, if the time period is at or after the treatment wave. The answer strategy also only uses the data from the first two periods (we probably would not collect outcome data after the last period for this reason). We fit a two-way fixed effects regression model by periods and units with standard errors clustered at the unit level.\nThe stepped-wedge experimental design, described in Declaration 38.1, shares much in common with the observational difference-in-differences design. We show in section (difference-in-differences?), the two-way fixed effects estimator is biased for the average treatment effect on the treated in the presence of treatment effect by time interactions. However, in the stepped-wedge design, we randomize treatment, so we do not need to make a parallel trends assumption. Our diagnosis below shows no bias when estimating the average treatment effect with the two-way fixed effects estimator in the stepped-wedge design even when treatment effects vary by period. A regression with only period effects would also return unbiased answers as would a design with inverse assignment probability weights described in Gerber and Green (2012, ch. 8), but if there are large unit differences the two-way design will be more efficient. Only including unit fixed effects, by contrast, without period effects will yield biased answers, because the probabilities of assignment vary by round."
  },
  {
    "objectID": "library/experimental-causal/stepped-wedge-designs.html#when-to-use-a-stepped-wedge-experiment",
    "href": "library/experimental-causal/stepped-wedge-designs.html#when-to-use-a-stepped-wedge-experiment",
    "title": "\n38  Stepped-wedge experiments\n",
    "section": "\n38.1 When to use a stepped wedge experiment",
    "text": "38.1 When to use a stepped wedge experiment\nCompared to the equivalent two-arm randomized experiment, a stepped-wedge experiment involves the same number of units, but more treatment (all versus half) and more measurement (all units are measured at least twice). The decision of whether to adopt the stepped-wedge design, then, rides on budget, the relative costs of measurement and treatment, ethical and logistical constraints such as the imperative to treat all units, and beliefs about effect sizes and outcome variances.\nWe compare the stepped-wedge design to a two-arm randomized experiment with varying sample sizes to assess these tradeoffs. First we compare designs with the same number of units, which would be the relevant comparison if the number of units is fixed. The second comparison is a two-arm experiment with double the number of units, which would be the right comparison if the number of units can be increased at some cost. We summarize each design in terms of the number of study units, the number that are treated, and the number of unit measurements taken.\n\n(#tab:steppedwedgedesigns) Design parameters in the comparison between stepped-wedge and two-arm experimental designs.\n\nDesign\nN\nm treated\nn measurements\n\n\n\nStepped-wedge\n100\n100\n200\n\n\nTwo-arm v1\n100\n50\n100\n\n\nTwo-arm v2\n200\n100\n200\n\n\n\nWe declare a comparable two-arm experimental design in Declaration 38.2, with the wrinkle being that the estimand is slightly different by necessity. In the stepped-wedge design, we target the average treatment effect averaging over all periods up to the penultimate one, because there is no information about the control group from the last period. In a single period design, by its nature, we cannot average over time. We would obtain a biased answer if we targeted an out-of-sample time period. The average treatment effect we target is the current-period ATE for the period that is chosen. We cannot extrapolate beyond that if treatment effects vary over time. If we expect time heterogeneity in effects, we may not want to use a stepped-wedge design but instead design a new experiment that efficiently targets the conditional average treatment effects within each period. Then we could describe both the average effect and how effects vary over time.\n\nDeclaration 38.2 Comparison single-period two arm trial design\n\ndeclaration_18.11 <-\n  declare_model(\n    N = n_units, \n    U_unit = rnorm(N),\n    U = rnorm(N),\n    effect_size = effect_size,\n    potential_outcomes(Y ~ scale(U_unit + U) + effect_size * Z)\n  ) +\n  declare_assignment(Z = complete_ra(N, m = n_units / 2)) +\n  declare_inquiry(ATE = mean(Y_Z_1 - Y_Z_0)) + \n  declare_measurement(Y = reveal_outcomes(Y ~ Z)) +\n  declare_estimator(Y ~ Z, inquiry = \"ATE\", label = \"DIM\")\n\n\n\n\nDiagnosis 38.1 (Diagnosis of stepped-wedge design compared to two single-period two arm trial designs) \ndesign_stepped_wedge <- \n  declaration_18.11 |> \n  redesign(n_units = 100, effect_size = seq(from = 0, to = 0.75, by = 0.05))\n\ndesign_single_period_100 <- \n  declaration_18.11 |> \n  redesign(n_units = 100, effect_size = seq(from = 0, to = 0.75, by = 0.05))\n  \ndesign_single_period_200 <-\n  declaration_18.11 |> \n  redesign(n_units = 200, effect_size = seq(from = 0, to = 0.75, by = 0.05))\n\ndesigns <- c(design_stepped_wedge, design_single_period_100, design_single_period_200)\nattr(designs, \"names\") <- paste0(\"design_\", 1:length(designs))\n\ndiagnosis_18.11 <- diagnose_design(designs)\n\nWe plot power curves for the three comparison designs in Figure 38.2. The top line (blue dashed) is the 200-unit study, which is preferred in terms of power, and by a considerable margin. That design involves the same amount of measurement and treatment as the stepped-wedge so may be the same cost. However, if only 100 units are available for study, then the relevant comparison is between the stepped-wedge and the 100-unit two arm study. Here, the stepped-wedge design is preferable in terms of power and may satisfy ethical requirements to eventually treat all subjects.\n\n\nFigure 38.2: Power analysis of three designs: stepped wedge with 100 units and 1/3-1/3-1/3 allocation, two-arm experiment with 100 units, and two-arm experiment with 200 units."
  },
  {
    "objectID": "library/experimental-causal/stepped-wedge-designs.html#design-examples",
    "href": "library/experimental-causal/stepped-wedge-designs.html#design-examples",
    "title": "\n38  Stepped-wedge experiments\n",
    "section": "\n38.2 Design examples",
    "text": "38.2 Design examples\n\nGerber et al. (2011) use a stepped-wedge design to randomize the timing of political television ads in 18 media markets in Texas in advance of a primary election.\nPennycook et al. (2021) conduct an online field experiment with Twitter users who had shared links to untrustworthy websites. The authors randomized the timing of a direct messages to those users, asking them to rate the accuracy of a non-political headline, then observed the quality of the news articles they subsequently shared.\n\n\n\n\n\nGerber, Alan S., James G. Gimpel, Donald P. Green, and Daron R. Shaw. 2011. “How Large and Long-Lasting Are the Persuasive Effects of Televised Campaign Ads? Results from a Randomized Field Experiment.” American Political Science Review 105 (1): 135–50.\n\n\nGerber, Alan S., and Donald P. Green. 2012. Field Experiments: Design, Analysis, and Interpretation. New York: W.W. Norton.\n\n\nPennycook, Gordon, Ziv Epstein, Mohsen Mosleh, Antonio A. Arechar, Dean Eckles, and David G. Rand. 2021. “Shifting Attention to Accuracy Can Reduce Misinformation Online.” Nature 592 (7855): 590–95."
  },
  {
    "objectID": "library/experimental-causal/randomized-saturation-designs.html",
    "href": "library/experimental-causal/randomized-saturation-designs.html",
    "title": "\n39  Randomized saturation experiments\n",
    "section": "",
    "text": "We declare a multilevel design in which we randomize first the “saturation” or probability of assignment with clusters and then randomly assign within the clusters based on that saturation probability. The diagnosis highlights that the efficiency for estimating the causal effect of the saturation level is low, because assignment is clustered, and higher for estimating the individual treatment effect within clusters of a given saturation.\nWe study most treatments at an isolated, atomized, individualistic level. We define potential outcomes with respect to a unit’s own treatment status, ignoring the treatment status of all other units in the study. Accordingly, our inquiries tend to be averages of individual-level causal effects and our data strategies tend to assign treatments at the individual level as well. All of the experimental designs we have considered to this point have been of this flavor.\nHowever, when the potential outcome revealed by a unit depends on the treatment status of other units, then we have to make adjustments to every part of the design. We have to redefine the model M to specify what potential outcomes are possible. Under a no-spillover model, we might only have the treated and untreated potential outcome \\(Y_i(1)\\) and \\(Y_i(0)\\). But under spillover models, we have to expand the set of possibilities. For example, we might imagine that unit \\(i\\)’s potential outcomes can be written as a function of their own treatment status and that of their housemate, unit \\(j\\): \\(Y_i(Z_i, Z_j)\\). We have to redefine our inquiry I with respect to those reimagined potential outcomes. The average treatment effect is typically defined as \\(\\E[Y_i(1) - Y_i(0)]\\), but if \\(Y_i(1)\\) and \\(Y_i(0)\\) no longer exist, we need to choose a new inquiry, like the average direct effect of treatment when unit \\(j\\) is not treated: \\(\\E[Y_i(1, 0) - Y_i(0, 0)]\\). We have to alter our data strategy D so that the randomization procedure produces healthy samples of all of the potential outcomes involved in the inquiries, and we have to amend our answer strategy A to account for the important features of the new randomization protocol.\nWe divide up our investigation of experimental designs to learn about spillovers into two sets. This chapter addresses randomized saturation designs, which are appropriate when we can exploit a hierarchical clustering of subjects into groups within which spillover can occur but across which spillover can’t occur. The next chapter addresses experiments over networks, which are appropriate when spillover occurs over geographic, temporal, or social networks.\nThe randomized saturation design (sometimes called the partial population design, as in Baird et al. (2018)) is purpose-built for scenarios in we have good reason to imagine that a unit’s potential outcomes depend on the fraction of treated units within the same cluster. For example, we might want to consider the fraction of people within a neighborhood assigned to receive a vaccine: a person’s health outcomes could easily depend on whether two-thirds or one-third of neighbors have been treated.\nIn the model, we now have to define potential outcomes with respect to both the individual level treatment and also the saturation level. We can imagine a variety of different kinds of potential outcomes functions. Consider the vaccine example, imagining a 100% effective vaccine against infection. Directly treated individuals never contract the illness, but the probability of infection for untreated units depends on the fraction who are treated nearby. If the treatment is a persuasive message to vote for a particular candidate, we might imagine that direct treatment is ineffective when only a few people around you hear the message, but becomes much more effective when many people hear the message at the same time. The main challenge in developing intuitions about complex interactions like this is articulating the discrete potential outcomes that each subject could express, then reasoning about the plausible values for each potential outcome.\nThe randomized saturation design is a factorial design of sorts, and like any factorial design can support a number of different inquiries. We can describe the average effect of direct treatment at low saturation, at high saturation, the average of the two, or the difference between the two. Similarly, we could describe the average effect of high versus low saturation among the untreated, among the treated, the average of the two, or the difference between the two. In some settings, all eight of these inquiries might be appropriate to report, in others just a subset.\nThe design employs a two-stage data strategy. First, pre-defined clusters of units are randomly assigned to treatment saturation levels, for example 25% or 75%. Then, in each cluster, individual units are assigned to treatment or control with probabilities determined by their clusters’ saturation level. The main answer strategy complication is that now there are two levels of randomization that must be respected. The saturation of treatment varies at the cluster level, so whenever we are estimating saturation effects, we have to cluster standard errors at the level saturation was assigned. The direct treatments are assigned at the individual level, so we do not need to cluster.\nDeclaration 39.1 describes 50 groups of 20 individuals each. We imagine one source of unobserved variation at the group level (the group_shock) and another at the individual level (the individual shock). We built potential outcomes in which the individual and saturation treatment assignments each have additive (non-interacting) effects, though more complex potential outcomes functions are of course possible. We choose two inquiries in particular: the conditional average effect of saturation among the untreated and the conditional average effect of treatment when saturation is low.\nWe can learn about the effects of the dosage of indirect treatment by comparing units with the same individual treatment status across the levels of dosage. For example, we could compare untreated units across the 25% and 75% saturation clusters. We can also learn about the direct effects of treatment at either saturation level, e.g., the effect of treatment when saturation is low. We use difference-in-means estimators of both inquiries, subsetted and clustered appropriately."
  },
  {
    "objectID": "library/experimental-causal/randomized-saturation-designs.html#design-examples",
    "href": "library/experimental-causal/randomized-saturation-designs.html#design-examples",
    "title": "\n39  Randomized saturation experiments\n",
    "section": "\n39.1 Design examples",
    "text": "39.1 Design examples\n\nCheema et al. (2022) used a randomized saturation design in their study of a get-out-the-vote campaign in Pakistan. Wards could be assigned to one of three treatment conditions or a control condition; within treated wards, four of five study households received the assigned condition but the fifth was assigned to control. A comparison of untreated household in treated wards to untreated household in untreated wards generates an estimate of the spillover effect (small and nonsignificant in this case).\nEgger et al. (2019) study how a cash transfer program implemented in one locality may affect outcomes in neighboring localities. In Kenya, the authors grouped villages into “saturation groups” and randomized the saturation groups to have one-third or two-thirds of their constituent villages assigned to treatment. A comparison of the untreated villages in the one-third and two-thirds saturation groups yields an estimate of the spillover effect.\n\n\n\n\n\nBaird, Sarah, J. Aislinn Bohren, Craig McIntosh, and Berk Ozler. 2018. “Optimal Design of Experiments in the Presence of Interference.” Review of Economics & Statistics 5 (100): 844–60.\n\n\nCheema, Ali, Sarah Khan, Asad Liaqat, and Shandana Khan Mohmand. 2022. “Canvassing the Gatekeepers: A Field Experiment to Increase Women Voters’ Turnout in Pakistan.” American Political Science Review, 1–21.\n\n\nEgger, Dennis, Johannes Haushofer, Edward Miguel, Paul Niehaus, and Michael W Walker. 2019. “General Equilibrium Effects of Cash Transfers: Experimental Evidence from Kenya.” Working Paper 26600. National Bureau of Economic Research."
  },
  {
    "objectID": "library/experimental-causal/experiments-over-networks.html",
    "href": "library/experimental-causal/experiments-over-networks.html",
    "title": "\n40  Experiments over networks\n",
    "section": "",
    "text": "We declare a design for a randomized trial in which the researcher controls the assignment of direct treatment, but then assess the effects of direct treatment and of being indirectly treated by being geographically-proximate to a unit directly treated. The diagnosis demonstrates both can be estimated without bias if the probability of indirect treatment is estimated through simulation, but that common estimators differ greatly in efficiency.\nWhen experimental subjects are embedded in a network, units’ outcomes may depend on the treatment statuses of nearby units. In other words, treatments map spill over across the network. For example, in a geographic network, vote margin in one precinct may depend on outdoor advertisements in neighboring precincts. In a social network, information delivered to a treated subjects might be shared with friends or followers. In a temporal network, treatments in the past might affect outcomes in the future.\nThis chapter describes the special challenges associated with experiments over networks. In the previous chapter, we discussed randomized saturation designs, which are appropriate when we can describe a hierarchy of units embedded in clusters, within which spillovers can occur but across which spillovers cannot occur. In other words, the randomized saturation design is appropriate when the network is composed of many disconnected network components (the clusters). But most networks are not disconnected. Instead, all or almost all units are typically connected in a vast web. This chapter describes how we need to modify the model, inquiry, data strategy, and answer strategy to learn from experiments over networks.\nIn the model, our main challenge is to define how far apart (in social, geographic, or temporal space) units have to be in order for unit \\(i\\)’s potential outcomes not to depend on unit \\(j\\). We might say units within 5km matter but units further away do not. We might say that units within two friendship links matter but more distal connections do not. We might allow the treatment statuses of three, two, or one periods ago to impact present outcomes differently from one another. For example, we might stipulate that each unit has only four potential outcomes that depend on whether a unit is directly treated or indirectly treated by virtue of being adjacent to a directly treated unit as in Table ?tbl-networksconditions.\nWith potential outcomes defined, we can define inquiries. With four potential outcomes, there are six pairwise contrasts that we could contemplate. For example, the direct effect in the absence of indirect treatment is defined as \\(\\E[Y_i(\\mathrm{direct} = 1, \\mathrm{indirect} = 0) - Y_i(\\mathrm{direct} = 0, \\mathrm{indirect} = 0)]\\) and the direct effect in the presence of indirect treatment is \\(\\E[Y_i(\\mathrm{direct} = 1, \\mathrm{indirect} = 1) - Y_i(\\mathrm{direct} = 0, \\mathrm{indirect} = 1)]\\). We could similarly define indirect effects as \\(\\E[Y_i(\\mathrm{direct} = 0, \\mathrm{indirect} = 1) - Y_i(\\mathrm{direct} = 0, \\mathrm{indirect} = 0)]\\) or \\(\\E[Y_i(\\mathrm{direct} = 1, \\mathrm{indirect} = 1) - Y_i(\\mathrm{direct} = 1, \\mathrm{indirect} = 0)]\\). We may be interested in how direct and indirect treatments interact, which would require taking the difference between the two direct effect inquiries or taking the difference between the two indirect effect inquiries. Which inquiry is most appropriate will depend on the theoretical setting.\nThe data strategy for an experiment over networks still involves random assignment. Typically, however, experimenters are only in control of the direct treatment application, and exposure to indirect treatment results from the natural channels through which spillovers occur. The mapping from a direct treatment vector to the assumed set of treatment conditions is described by Aronow and Samii (2017) as an “exposure mapping.” The exposure mapping defines how the randomized treatment results in the exposures that reveal each potential outcome. The probabilities of assignment to each of the four conditions are importantly not constant across units, for the main reason that units with more neighbors are more likely to receive indirect treatment. Furthermore, exposures are dependent across units: if one unit is directly treated, then all adjacent units must be indirectly treated.\nWe need to adjust our answer strategy to compensate for the differential probabilities generated by this complex data strategy. As usual, we need to weight units by the inverse of the probability of assignment to the condition that they are in. In the networked setting we have to further account for dependence in treatment assignment probabilities. This dependence tends to increase sampling variability. For intuition, consider how clustering (an extreme form of across-unit dependence in treatment conditions) similarly tends to increase sampling variability. Aronow and Samii (2017) propose Hajek- and Horvitz-Thompson-style point and variance estimators that account for these complex joint probabilities of assignment, which are themselves estimated by simulating the exposures that would result from many thousands of possible random assignments.\nTo illustrate these ideas, we declare a hypothetical experimental design to estimate the effects of lawn signs (modeled after Green et al. 2016). The units are the lowest level at which we can observe vote margin, the voting precinct. In our model, we define four potential outcomes. Precincts can be both directly and indirectly treated, only directly treated, only indirectly treated, or neither. Indirect treatment occurs when a neighboring precinct is treated. This model could support many possible inquiries, but here we will focus on three: the direct effect of treatment when the precinct is not indirectly treated, the effect of indirect treatment when the precinct is not directly treated, and the total effect of direct and indirect treatment versus pure control. The data strategy will involve randomly assigning some units to direct treatment, which will in turn cause other units to be indirectly treated. We will need to learn via simulation the probabilities of assignments to conditions that this procedure produces. We’ll make use of two answer strategies: the Horvitz-Thompson and Hajek estimators proposed by Aronow and Samii (2017), along with their associated variance estimators, as implemented in the interference package.\nFirst, we load the Fairfax County, Virginia, voting precincts shapefile, removing one singleton voting precinct, and we plot the precincts in ?fig-ch18num15.\nTo declare the full design, shown in Declaration 40.1, we first need to obtain the adjacency matrix of precincts in Fairfax. Second, we obtain a permutation matrix of possible random assignments, from which probabilities of assignment to each condition can be calculated. The declare_model call builds in a dependence of potential outcomes on the length of each precinct’s perimeter to reflect the idea that outcomes are correlated with geography in some way. The declare_inquiry call describes the three inquiries in terms of potential outcomes. The declare_assignment call first conducts a random assignment according to the procedure describes by declare_ra earlier in the code, then obtains the exposures that the assignment generates. Finally, all the relevant information is fed into the Aronow and Samii estimation functions via estimator_AS (the get_exposure_AS and estimator_AS helper functions are available in the rdddr package).\nThe maps in Figure 40.2 show how this procedure generates differential probabilities of assignment to each exposure condition. Units that are in denser areas of the county are more likely to be in the Indirect Exposure Only and Direct and Indirect Exposure conditions than those in less dense areas."
  },
  {
    "objectID": "library/experimental-causal/experiments-over-networks.html#design-examples",
    "href": "library/experimental-causal/experiments-over-networks.html#design-examples",
    "title": "\n40  Experiments over networks\n",
    "section": "\n40.1 Design examples",
    "text": "40.1 Design examples\n\nZelizer (2019) conducts experiments within a legislative network defined by office-sharing. Legislators were randomly assigned briefings on a subset of bills and their decision to cosponsor the bill (or not) was recorded. The design was able to estimate spillover effects bill by comparing legislators whose officemates were and were not assigned to treatment on various bills.\nGreen et al. (2016) conducts a randomized experiment within geographic network of adjacent voting precincts. Treated units were assigned many lawn signs supporting a candidate. The design supported inference on the direct effect (treated units relative to untreated units surrounded by untreated units) and the indirect effects (untreated units adjacent to treated units relative to untreated units surrounded by untreated units). The answer strategy accounted for the differential probabilities of assignment to each of these conditions depending on geographic network position.\n\n\n\n\n\nAronow, Peter M., and Cyrus Samii. 2017. “Estimating Average Causal Effects Under General Interference, with Application to a Social Network Experiment.” The Annals of Applied Statistics 11 (4): 1912–47.\n\n\nGreen, Donald P., Jonathan S. Krasno, Alexander Coppock, Benjamin D. Farrer, Brandon Lenoir, and Joshua N. Zingher. 2016. “The Effects of Lawn Signs on Vote Outcomes: Results from Four Randomized Field Experiments.” Electoral Studies 41: 143–50.\n\n\nZelizer, Adam. 2019. “Is Position-Taking Contagious? Evidence of Cue-Taking from Two Field Experiments in a State Legislature.” American Political Science Review 113 (2): 340–52."
  },
  {
    "objectID": "library/complex/index.html",
    "href": "library/complex/index.html",
    "title": "41  Complex designs",
    "section": "",
    "text": "Very few published studies have this actual form. Instead, most draw in complex ways on a series of research designs, each targeted to a different inquiry, that when brought together answer the deeper theoretical question at the heart of the research. Most published studies are complex in this way.\nBut studies can also be “complex” in other ways. For instance, although we have assumed researchers start with well defined inquiries, some studies focus on first figuring out what question to ask and then proceed to ask and answer it. The study engages first in model building, then reports the results of a follow-on research design targeted at questions posed by the new model.\nSome studies seek not to learn about levels and effects but search explicitly for a model of a phenomenon, asking for instance “what causes \\(Y\\)?” or “what model, in some class, best accounts for observed data”? These studies have complex inquiries. Other studies have complex data and answer strategies, for instance, mixing qualitative and quantitative inference strategies or gathering together findings from multiple sub-studies in order to arrive at an overall conclusion."
  },
  {
    "objectID": "library/complex/discovery-causal-forests.html",
    "href": "library/complex/discovery-causal-forests.html",
    "title": "\n42  Discovery using causal forests\n",
    "section": "",
    "text": "We declare a design in which a researcher examines a large set of continuous covariates to discover (i) which covariate best explains heterogeneity in the effects of a treatment and (ii) which subjects experiences the weakest or strongest effects. The design declaration clarifies the inquiries when the goal of the study is discovery and can be used to guide design decisions regarding how best to split data into training and testing sets.\nIn most designs that we have discussed, researchers have a clear idea what they are looking for when they begin the research. How big is some population? What is the size of some effect? But some research involves questions that are much more open in nature. We focus here on discovery research that has two types of more open questions. The first inquiry poses an open question of the form “what matters?” (rather than the more common closed question of the form “what is the effect of this on that?”). The second inquiry poses a question about an as-yet unspecified group — who are the people for whom effects are especially strong or weak?\nWe imagine a setting in which a researcher has access to a large group of covariates \\(X\\) and has conducted an experiment to assess the effects of \\(Z\\) on \\(Y\\). The researcher is interested in the heterogeneity of effects of \\(Z\\) as a function of variables in \\(X\\) and in poses two inquiries. First, which covariate in \\(X\\) best “explains” variation in the effect of \\(Z\\)? Second, what combination of values in \\(X\\) characterize individuals for whom effects are particularly weak?\nDeclaration 42.1 proceeds as follows.\nFor the model, we imagine a possibly complex function linking \\(Z\\) to \\(Y\\) in which the effect of \\(Z\\) depends nonlinearly on some but not all variables in \\(X\\). As always, this model can and should be altered to help understand whether diagnosis depends on the particular type of structure assumed.\nWe declare two inquiries beyond the standard average treatment effect. For the first inquiry we need to be specific about what we mean by “best explains.” We will imagine asking which covariate \\(X\\) produces the lowest conditional variance \\(\\mathbb{E}(\\mathbb{V}(Y_i(1)- Y_i(0)|X_i = x))\\). Specifically, we partition each covariate into quantiles and take the average variance in treatment effect across each quantile. We will call this the best_predictor inquiry and calculate it using the \\(R^2\\) from a fixed-effects model applied to the true treatment effects. The best_predictor function in rdddr calculates this estimand, dividing covariates into 20 quantiles by default.\nThere is both a simple and a more complex understanding of the second inquiry. The simple understanding is that we are interested in the average effect among the units whose effects are in the bottom 20% (say) of all effects. We will call this the worst_effects inquiry. This is a natural notion of the worst affected. But it is a very difficult quantity to estimate.\nThe more complex understanding involves examining the realized data to do our best to identify a set of units (say of size 20%) that we think will have weak effects, and with this set identified return to M and ask what is the average effect for this set. We will call this the weak_effects inquiry, to acknowledge the fact that the effects for this group may not be the worst effects. This data-dependent inquiry is more complicated to articulate theoretically, but it is more straightforward to estimate empirically.\nWe assume the data strategy is the same as for a simple two arm trial (see Section (two-arm-randomized-experiments?)).\nThis causal forests design gets its name from the answer strategy. The “causal forests” algorithm randomly partitions data into a training and testing group. Within the training group it repeatedly generates “trees” by repartitioning the covariates (generating “branches”) to identify subgroups (“leafs”) with similar treatment effects. At each step, partitioning is implemented to yield estimated minimum variance in treatment effects. Unit level predictions of treatment effects (in both the training and the testing sets) are then generated by combining estimates of effects for units over different trees (see Wager and Athey 2018 for full details of the approach). Our estimate of the best_predictor is based on the variable that is most frequently partitioned to reduce variance, though we note that this indicator was not designed to capture the variable that induces the greatest reduction in variance. Including it here allows us to illustrate the ability of diagnosis to assess the performance of an estimator for a task for which it was not designed.\nTo implement causal forests in DeclareDesign, we wrote a “handler” that calls a function from the grf (generalized random forests) package. This handler produces two types of quantities: estimates of unit level causal effects and the name of the variable that is most frequently partitioned to reduce variance (var_imp). Because the output includes a prediction for each unit it is natural to add the output to the primary data frame. For that reason, the causal forests estimation is introduced as data strategy step and not specifically in an answer strategy step. The estimates of the unit level causal effects are used in order to identify the weakly performing group (within the test set and within the full set) which is in turn used to calculate the two versions of the weak_effects estimands, one for the test set and one for the full sample. To accommodate these complexities, the final inquiry declaration takes place after the implementation of the causal forests algorithm.\nFinally, we use regression to estimate the ATE as well as the weak effects and worst effects estimands using both the identified low performing group in the test set data and the identified low performing group in the full data. We assess the performance of these against both the weak_effects inquiry and the worst_effects inquiry.\nBefore turning to diagnosis we can get a feel for the performance of the causal forest estimator by comparing the predicted effects for each unit generated by the estimator, with the actual unit level treatment effects generated by M, as shown in Figure 42.1.\nWe see that we get some traction — but we also get a lot wrong. Estimating unit level causal effects is hard. We see in particular that that the range of the predictions is narrower than the range of the true effects, which will mean that the average effects in the groups with the best or worst predicted effects will generally not be the same as the effects for the groups with the best and worst actual effects.\nTo see how the design is choosing the best predictor, we illustrate the adjusted r-squared for a regression predicting the treatment effect with a partitioning of each variable, as well as the rank of predictors given the r-squared number in Figure 42.2. We see that the third covariate X.3 has a very high adjusted r-squared, and is ranked first, and most of the covariates are bunched at a very low r-squared."
  },
  {
    "objectID": "library/complex/discovery-causal-forests.html#design-examples",
    "href": "library/complex/discovery-causal-forests.html#design-examples",
    "title": "\n42  Discovery using causal forests\n",
    "section": "\n42.1 Design examples",
    "text": "42.1 Design examples\n\nBauer and Hohenberg (2021) uses a causal forests algorithm to explore the heterogeneous effects of a manipulation of the source (real or fake) of a political news. The average effect of a real (versus fake) source on belief in the facts reported in the article is positive, and it is more positive for those with greater trust in media and more political knowledge.\nGreen et al. (2022) study the effects of messages designed to increase vaccine intentions with a survey experiment. They apply a causal forests algorithm to uncover the possibly heterogeneous effects of the treatment depending on observed covariates like income, local disease burden, and political ideology, but find that responses to treatment are mostly homogeneous.\n\n\n\n\n\nBauer, Paul C., and Bernhard Clemm von Hohenberg. 2021. “Believing and Sharing Information by Fake Sources: An Experiment.” Political Communication 38 (6): 647–71.\n\n\nGreen, Jon, James N. Druckman, Matthew A. Baum, David Lazer, Katherine Ognyanova, Matthew Simonson, Jennifer Lin, Mauricio Santillana, and Roy H. Perlis. 2022. “Using General Messages to Persuade on a Politicized Scientific Issue.” British Journal of Political Science.\n\n\nWager, Stefan, and Susan Athey. 2018. “Estimation and Inference of Heterogeneous Treatment Effects Using Random Forests.” Journal of the American Statistical Association 113 (523): 1228–42."
  },
  {
    "objectID": "library/complex/structural-estimation.html",
    "href": "library/complex/structural-estimation.html",
    "title": "\n43  Structural estimation\n",
    "section": "",
    "text": "We declare a design in which a researcher wants to use experimental data to estimate the parameters of a game theoretic model. The premise of the approach is that, to the extent that the model is correct, in-sample parameter estimation allows the researcher to make interesting external inferences to effects in other settings. The risk is that bad models will produce bad inferences.\nStructural estimation is used when researchers have in mind a model for how processes work and their goal is to estimate the parameters of that model. If only they knew the parameters of the model, they could draw inferences about levels of unobserved variables, treatment effects, or other quantities. They might even extrapolate to estimate counterfactual quantities, such as the effects of interventions that have not been implemented (Reiss and Wolak 2007).\nWe illustrate this design with a bargaining game, drawing on an example used in Wilke and Humphreys (2020). We imagine a customer \\(i\\) makes payments from some endowment to a taxi driver. Bargaining proceeds as one player makes an offer that is accepted or rejected; if rejected the other player makes a counteroffer. The game continues for \\(n\\) rounds with zero payoffs if no deal is agreed. Our main interest is the share of the endowment retained by the customer.\nIn Declaration 43.1, we imagine two types of customers, strategic (\\(\\theta_i = 0\\)) and nonstrategic (\\(\\theta_i = 1\\)). If a customer is strategic, the equilibrium offer made by the first mover is the one predicted by the solution given in Rubinstein (1982). If however, the player is nonstrategic then they always (successfully) insist on invoking a payment norm that lets them retaining a fixed share of their endowment, \\(\\tilde{\\pi}\\). We let \\(\\alpha\\) denote the probability that a player is nonstrategic. A player’s payoff then is given by:\n\\[\\pi_i =  (1-\\theta_i)(z_i\\hat{\\pi} + (1-z_i)(1-\\pi)) + \\theta_i\\tilde{\\pi}\\]\nwhere \\[\\hat{\\pi} = \\sum_{j = 2}^n(-1^{j})\\delta^{j-1}\\]\nis the Rubinstein solution. The customer’s payoff depends on whether they goes first (\\(z_i = 1\\)) or second (\\(z_i = 0\\)) and on the common discount factor \\(\\delta\\).\nOne complication with structural estimation is that it’s hard to know what to infer when an action is taken that the model says shouldn’t happen – suppose negotiations didn’t reach \\(\\tilde{\\pi}\\) when they should have. A common way to address this challenge is to allow for the possibility that implemented actions are noisy reflections of predicted actions; noisy either because of measurement error or because in fact players deviate randomly from optimal behavior. Here we will allow for a disturbance like this by assuming that measured payments, \\(y\\), are a random draw from a Beta distribution with expectation given by the expected payment \\(\\pi\\) and variance \\(\\kappa\\).\nIn our data strategy, we randomly assign \\(Z\\) (who goes first) and measure payments \\(y\\). We will also assume we know what norms nonstrategic players are following, specifically that \\(\\tilde{\\pi} = 0.75\\). Our goal is to use observed payments along with treatment allocation to estimate the model parameters, \\(\\alpha\\), \\(\\delta\\), \\(\\kappa\\). These in turn can be used to estimate treatment effects and other counterfactual quantities (if we assume the model is true).\nOur inquiries are the parameters k, d, and a (corresponding to \\(\\kappa\\), \\(\\delta\\) and \\(\\alpha\\)). We set up the design so that we can vary these features easily but also so that we can vary \\(n\\) (the total number of rounds in the bargaining game).\nThe answer strategy is implemented using maximum likelihood to identify which parameter values are most consistent with the data. More specifically this approach takes the model as true and asks for what collection of parameter values is the observed data most likely. See King (1998) for an introduction to the method of maximum likelihood. One nice feature of the method is that the problem of maximizing the likelihood is (asymptotically) equivalent to finding the probability distribution that minimizes the Kullback–Leibler divergence to the true probability distribution (of course, all that within the class of distributions that can be specified under the model). For more see White (1982).\nThe heart of the estimation strategy is the specification of likelihood function. The likelihood function reports the probability of the data given particular stipulated values of the parameters. This value is different depending on \\(n\\) so in fact we write down a function that writes a function given \\(n\\). The likelihood function reflects the theoretical model described above, which means that in this declaration, the same event generating process is used in M and in the construction of the likelihood function in A. An optimistic assumption."
  },
  {
    "objectID": "library/complex/structural-estimation.html#design-examples",
    "href": "library/complex/structural-estimation.html#design-examples",
    "title": "\n43  Structural estimation\n",
    "section": "\n43.1 Design examples",
    "text": "43.1 Design examples\n\nSamii (2016) enumerates some examples of structural estimation in economics and predicts future political scientists will take to structural estimation in earnest.\nFrancois, Rainer, and Trebbi (2015) provides a structural model to explain how leaders allocate cabinet positions to bolseter coalitions; the analysis compares the performance of a preferred theory to rival theories.\nFrey, López-Moctezuma, and Montero (2022) estimates a structural model of party competition and coalitions on the basis of a regression discontinuity design. With the model parameters estimated, the authors simulate counterfactual scenarios without party coalitions.\n\n\n\n\n\nFrancois, Patrick, Ilia Rainer, and Francesco Trebbi. 2015. “How Is Power Shared in Africa?” Econometrica 83 (2): 465–503.\n\n\nFrey, Anderson, Gabriel López-Moctezuma, and Sergio Montero. 2022. “Sleeping with the Enemy: Effective Representation Under Dynamic Electoral Competition.” American Journal of Political Science.\n\n\nKing, Gary. 1998. Unifying Political Methodology: The Likelihood Theory of Statistical Inference. Ann Arbor, Michigan: University of Michigan Press.\n\n\nReiss, Peter C, and Frank A Wolak. 2007. “Structural Econometric Modeling: Rationales and Examples from Industrial Organization.” Handbook of Econometrics 6: 4277–4415.\n\n\nRubinstein, Ariel. 1982. “Perfect Equilibrium in a Bargaining Model.” Econometrica: Journal of the Econometric Society, 97–109.\n\n\nSamii, Cyrus. 2016. “Causal Empiricism in Quantitative Research.” The Journal of Politics 78 (3): 941–55.\n\n\nWhite, Halbert. 1982. “Maximum Likelihood Estimation of Misspecified Models.” Econometrica: Journal of the Econometric Society 50 (1): 1–25.\n\n\nWilke, Anna M., and Macartan Humphreys. 2020. “Field Experiments, Theory, and External Validity.” SAGE Handbook of Research Methods in Political Science; International Relations."
  },
  {
    "objectID": "library/complex/meta-analysis.html",
    "href": "library/complex/meta-analysis.html",
    "title": "\n44  Meta-analysis\n",
    "section": "",
    "text": "We declare a design for meta-analytic study in which a researcher seeks to combine findings from a collection of existing studies to form an overall conclusion. Declaration helps clarify the estimand in such studies. Diagnosis highlights risks associated with a common estimator used in meta-analysis, the fixed effect estimator.\nIn a meta-analysis, inquiries are typically population-level quantities like the population average treatment effect (PATE). The relevant population might be the full set of units that happened to participate in past research studies or it might be a broader population, like all human adults since 1950. The data strategy for a meta-analysis involves collecting all (or a subset) of the estimates generated by past research studies on the topic and standardizing them so they can be meaningfully compared or combined. Meta-analyses are valuable because they can tell us about the empirical consensus on a particular inquiry. Because they typically aggregate a large amount of information, meta-analyses are usually more precise than individual studies. We can also learn what we don’t know from a meta-analysis. After aggregating all the available evidence on a given inquiry, we may find that we don’t know very much at all.\nThe PATE inquiry, however, might not be so interesting if the constituent inquiries – the site-level ATEs – vary greatly from context to context. The ATEs that make up the PATE might vary because of the contextual features that condition the effect. Galos and Coppock (2022), for example, reanalyze audit studies of gender-based employment discrimination to find that the ATE on callbacks of being a woman is strongly positive in women-dominated sectors and is strongly negative in men dominated sections. For this reason, meta-analyses often include inquiries about the variance of effects across studies or the covariance of effects with groups.1\nThe largest choice in the data strategy for a meta-analysis is the set of study inclusion and exclusion criteria. These criteria should be guided by the inquiries of the meta-analysis and whether the designs of the constituent studies appropriately inform the meta-analytic inquiry. If the inquiry is the population mean and standard deviation of the site level ATEs of treatment \\(Z\\) on outcome \\(Y\\), we want to include only studies that credibly estimate the effect of \\(Z\\) on \\(Y\\). This requirement means checking that all included studies use similar-enough treatments and similar-enough measurements of the outcome. It also means excluding studies that are prone to bias. For example, Pettigrew and Tropp (2006) assemble 515 studies of the contact hypothesis, but Paluck, Green, and Green (2019) exclude all but 5% of these studies in their updated meta-analysis for failing to randomize intergroup contact. Meta-analyses that include biased studies can compound biases, giving us falsely precise and incorrect answers. Finally, inclusion decisions should be made on the basis of the designs of the constituent studies and not their results. For example, we should not exclude studies that fail to reach statistical significance or yield unexpected answers.\nThe answer strategies for meta-analysis often amount to a weighted average of the individual study estimates. We take a weighted average instead of a simple average because we want to give different studies different amounts of pull in the overall estimate. In particular, we want to give studies that are more precise more weight and studies that are less precise less weight. In fixed-effects estimation, for example, study weights are proportional to the inverse of the estimated variance from the study. In random-effects estimation, by contrast, the weights are proportional to the inverse of the estimated variance from the study, plus an estimate of the between-study variance in effects. With this adjustment, the study weights are less extreme in random effects relative to fixed effects (for more see Borenstein et al. 2021, ch. 13). Fixed-effects meta-analysis may be appropriate in settings in which we have a strong theoretical reason to think the site-level inquiries are all equal to the PATE, but typically, we think effects vary from site-to-site, so random effects is usually the meta-analytic answer strategy of choice.\nIn Declaration 44.1, we declare a meta-analytic research design for 100 studies with a true PATE (\\(\\mu\\)) of 0.2. We represent the standard deviation of the study-level ATEs with \\(\\tau\\), which we vary between 0 and 1. When \\(\\tau>0\\) the true effects vary across contexts. The studies each have different measurement precision, with standard errors between 0.025 and 0.6. The inquiries are \\(\\mu\\) and \\(\\tau^2\\). In the answer strategy, we use both fixed and random effects meta-analysis."
  },
  {
    "objectID": "library/complex/meta-analysis.html#design-examples",
    "href": "library/complex/meta-analysis.html#design-examples",
    "title": "\n44  Meta-analysis\n",
    "section": "\n44.1 Design examples",
    "text": "44.1 Design examples\n\nBlair, Christensen, and Rudkin (2021) meta-analyze 46 difference-in-difference studies of the effects of commodity price shocks on conflict, excluding over 300 studies of the same effect that relied on weaker identification strategies.\nSchwarz and Coppock (2022) collect 67 candidate choice conjoint experiments that randomized candidate gender to estimate the average effect of gender on electoral support.\n\n\n\n\n\nBlair, Graeme, Darin Christensen, and Aaron Rudkin. 2021. “Do Commodity Price Shocks Cause Armed Conflict? A Meta-Analysis of Natural Experiments.” American Political Science Review 115 (2): 709–16.\n\n\nBorenstein, Michael, Larry V Hedges, Julian PT Higgins, and Hannah R Rothstein. 2021. Introduction to Meta-Analysis. John Wiley & Sons.\n\n\nGalos, Diana, and Alexander Coppock. 2022. “What Have Employment Audits Taught Us about the Effects of Gender on Hiring? A Meta-Analyis.” Unpublished Manuscript.\n\n\nPaluck, Elizabeth Levy, Seth A. Green, and Donald P. Green. 2019. “The Contact Hypothesis Re-Evaluated.” Behavioural Public Policy 3 (2): 129–58.\n\n\nPettigrew, Thomas F., and Linda R. Tropp. 2006. “A Meta-Analytic Test of Intergroup Contact Theory.” Journal of Personality and Social Psychology 90 (5): 751–83.\n\n\nSchwarz, Susanne, and Alexander Coppock. 2022. “What Have We Learned About Gender From Candidate Choice Experiments? A Meta-analysis of 67 Factorial Survey Experiments.” Journal of Politics 84 (2): 655–68."
  },
  {
    "objectID": "library/complex/multi-site-studies.html",
    "href": "library/complex/multi-site-studies.html",
    "title": "\n45  Multi-site studies\n",
    "section": "",
    "text": "We declare a design for a coordinated research project in which multiple research teams combine data gathering and analysis strategies to address a common inquiry. Diagnosis clarifies the benefits and risks of coordination versus tailoring of treatments to contexts.\nNearly all social science is produced atomistically: individual scientists identify a question and a research strategy for answering it, apply the research strategy, and try to publish the results of that one study. Increasingly, scholars have come to realize that, though the standard research process may promote discovery, it is not optimized for the accumulation of general insights. One reason is that scientists are opportunistic in the contexts and units they study. If inquiry values are idiosyncratic to the contexts and units that scientists choose to study, then our studies will not generalize.\nScholars have addressed themselves to the problem of generalizability in a variety of ways. One approach is the replication of past studies in new contexts or with new populations. A second has been the application of statistical methods for extrapolating from single studies, relying on variation within studies in unit characteristics and effect heterogeneity. Both of these approaches are important.\nWe explore a third approach here: coordinated multi-site studies. The idea here is to implement the same research design (or as close as possible) across multiple contexts at the same time, each targeting the same inquiry with similar methods with the explicit intention of meta-analyzing the eventual results. In the previous chapter, we described retrospective meta-analysis in which a meta-analyst gathers together previous scholarship on a research question; harmonization of the studies is achieved ex post by including the studies that target the same theoretical quantity well and excluding those that do not. A coordinated multi-site study is a prospective meta-analysis; harmonization is achieved ex ante through the sometimes painstaking process of ensuring comparability of treatments, outcomes, and units across contexts.\nHow much we can learn about the generalizability of effects depends on how many sites we study, how we select the sites, our beliefs about effect heterogeneity, and the level of harmonization across studies. Of these, the harmonization level is most important. Without harmonization of research designs across sites, researchers risk answering different questions in different sites. If the goal is to understand a common inquiry and how values of the inquiry vary across contexts, then each site must be providing answers to (close to) the same inquiry. To do so, researchers coordinate measurement strategies so as to collect the same outcome measures and coordinate the details of treatment procedures to ensure they are studying the effects of the same intervention. They may also wish to coordinate on details like sampling to ensure the same types of subjects are enrolled at each site and on the consent and enrollment procedures so as to avoid introducing selection or demand effects differentially across sites.\nThe disadvantage of harmonization is that researchers end up answering a different question than they started out with. In the worst case, they end up answering one common question across all sites that is interesting in none. We explore these tradeoffs by declaring a modified multi-site design in which different treatments have different effects in different sites.\nDeclaration 45.1 describes a multi-site study with five sites. A conceptual challenge with multi-site studies is that in practice a single design has to take account of various differences across sites, such as differences in sample sizes, assignment probabilities, or expectations. The design below shows how these multisite features can be introduced in a simple way as part of a single “meta-design.” The design allows for variation across sites, implements analysis separately using data from different sites, and then combines findings for an overall conclusion. Note that the inquiries here are defined by creating site level inquiries and the averaging over these. Implicitly each site is weighted equally in the inquiry. Written like this you can see a clear similarity between I and A. One could alternatively use different weighting schemes if you wanted each unit in the sample to to have equal weight or if you wanted each each unit in the population to have equal weight—and of course, adjust A accordingly.\nWe imagine two possible treatments that might be implemented in each site. One, with treatment effect tau_1 (possibly different in each site), is the treatment that researchers are meant to coordinate on. The second, with treatment effect tau_2 is the treatment that researchers could implement if they were not coordinating. We imagine that if there is not strong coordination then researchers they select which version of the treatment to implement (here, we imagine they implement the treatment they think will yield larger effects).\nWe specify two inquiries. The first is for the coordinated treatment, with a focus on the average effect across studies. The second is less easy to describe: it is the average effect, again across cases, of whatever treatment was implemented in those cases. Though unusual, this inquiry is still well defined.\nEstimation proceeds in two steps: first we calculate site-level effects, just as we do for a meta-analysis, then we meta-analyze those site-level effects. We use here a random effects model, reflecting our expectation that true site effects differ (see Section meta-analysis? for a discussion of alternative answer strategies for meta analysis)."
  },
  {
    "objectID": "library/complex/multi-site-studies.html#design-examples",
    "href": "library/complex/multi-site-studies.html#design-examples",
    "title": "\n45  Multi-site studies\n",
    "section": "\n45.1 Design examples",
    "text": "45.1 Design examples\n\nDunning et al. (2019) present the results of a “Metaketa” study (a term for coordinated multi site studies) of the effects of voter information campaigns on political accountability in five countries: Benin, Burkina Faso, Brazil, Uganda (two studies), and Mexico.\nCoordinated multi-site studies need not be enormously expensive undertakings. Frederiksen (2022) reports the results of the same conjoint survey experiment measuring the effects of undemocratic behavior on electoral support in five countries (the United States, the United Kingdom, the Czech Republic, Mexico, and South Korea)\n\n\n\n\n\nDunning, Thad, Guy Grossman, Macartan Humphreys, Susan D. Hyde, Craig McIntosh, Gareth Nellis, Claire L. Adida, et al. 2019. “Voter Information Campaigns and Political Accountability: Cumulative Findings from a Preregistered Meta-Analysis of Coordinated Trials.” Science Advances 5 (7): 1–10.\n\n\nFrederiksen, Kristian Vrede Skaaning. 2022. “Does Competence Make Citizens Tolerate Undemocratic Behavior?” American Political Science Review, 1–7."
  },
  {
    "objectID": "lifecycle/index.html",
    "href": "lifecycle/index.html",
    "title": "Research Design Lifecycle",
    "section": "",
    "text": "This part of the book works through stages of the research design lifecycle, illustrated in Figure -1.1. As the nonlinear, circular path in the figure illustrates, the steps are all intertwined by their shared connection to the research design. We describe in each entry how we can use the declaration, diagnosis, and redesign framework to make progress at each step. Of course not every research project will feature each and every stage but the list should serve we hope as a useful checklist as you work through the major phases of your research.\n\n\nFigure -1.1: The research design lifecycle\n\n\nWe divide the research design lifecycle into three broad categories: planning, realization, and integration. Planning includes all of the activities you undertake before data collection starts: conducting ethical reviews, obtaining approvals, organizing partnerships, securing funding, running pilots, gathering criticism, and filing analysis plans. Realization begins with the execution of the data strategy as planned, and continues through the inevitable changes that come with analytic challenges and scientific surprises. We trace realization through implementation, pivoting in response to unexpected developments, writing up results, reconciling planned and implemented designs, and responding to peer reviewers. In the final phase of a research project, the results are integrated into the scientific literature. Integration includes how the study will inform theories and decisions and also how the study will later be reanalyzed, replicated, and someday – meta-analyzed."
  },
  {
    "objectID": "lifecycle/planning/index.html",
    "href": "lifecycle/planning/index.html",
    "title": "\n46  Planning\n",
    "section": "",
    "text": "The planning process changes designs. We work out designs that meet ethical as well as scientific standards, accommodate the needs of research partners, and operate within financial and logistical constraints. When we are insufficiently sure of key inputs to design declarations, we can run pilots, but we need to be careful about how we incorporate what we learn from them. Finally, when we write up a declaration or a PAP with a declaration, this can be a useful moment to get feedback from our peers to improve the design. We discuss each of these steps in this chapter."
  },
  {
    "objectID": "lifecycle/planning/ethics.html",
    "href": "lifecycle/planning/ethics.html",
    "title": "\n47  Ethics\n",
    "section": "",
    "text": "As research designers, we have ethical obligations beyond the requirements of national laws and the regulations of institutional review boards.\nFor a long time thinking about research ethics have been guided by the ideas in the Belmont report, that emphasize beneficence, respect for persons, and autonomy. Recently, more attention has been given to principles that extend beyond care for human subjects to include considerations for the well-being of collaborators and partners and the broader social impact of research. Social scientific professional associations have developed principles and guidelines to help think through these issues. Key references include:\nThe considerations at play vary across context and methods. For example, Teele (2021) describes ethical considerations in field experimentation, Humphreys (2015) focuses on development settings, Slough (2020) considers the ethics of field experimentation in the context of elections, and Wood (2006) and Baron and Young (2021) consider ethical challenges specific to field research in conflict settings.\nHowever a common meta-principle underlying many of these contributions is the injunction to consider the ethical dimensions of your work ex ante and to report on ethical implications ex post. Lyall (2022) specifically connects ethical reflection to ex ante design considerations.\nWe encourage you to engage with ethical considerations in this way, early in the research design lifecycle. Some design declarations and diagnoses elide ethical considerations. For instance, a declaration that is diagnosand-complete for statistical power may tell you little about the level of care and respect accorded to subjects. Many declarations are diagnosand-complete for bias, but obtaining an unbiased treatment effect estimate is not always the highest goal.\nEthical diagnosands can be directly incorporated into the declare-diagnose-redesign framework. Diagnosands could include the total cost to participants, how many participants were harmed, the average level of informed consent measured by a survey about comprehension of study goals, or the risks of adverse events. More complex ethical diagnosands may be possible as well: Slough (2020) provides a formal analysis of the “aggregate electoral impact” diagnosand for experiments that take place in the context of elections. We consider two specific ethical diagnosands here, costs and potential harms, though many others may apply in particular research scenarios.\nCosts. A common concern is that measurement imposes a cost on subjects, if only by wasting their time. Subjects’ time is a valuable resource they often donate willingly to the scientific enterprise by participating in a survey or other measurement. Although subjects’ generosity is sometimes repaid with financial compensation, in many scenarios direct payments are not feasible. Regardless of whether subjects are paid, the costs to subjects should be top of mind when designing the study.\nPotential harms. Different realizations of the data from the same data strategy may differ in their ethical status. Ex-post, a study may not have ended up harming subjects, but ex-ante, there may have been a risk of harm (Baron and Young 2021). The project’s ethical status depends on judgments about potential harms and potential participants: not only what did happen, but what could have happened. The potential harm diagnosand might be formalized as the maximum harm that could eventuate under any realization of the data strategy. Researchers could then follow a minimax redesign procedure to find the design that minimizes this maximum potential harm.\nWhen the design is diagnosed, we can characterize the ethical status of possible realizations of the design as well as the ethical status of the distribution of these realizations. Is the probability of harm minimal “enough”? Is the degree of informed consent sufficient? Given that these characteristics vary across designs and across realizations of the same design, writing down concretely both the measure of the ethical status and the ethical threshold can help structure thinking. These diagnoses and the considerations that inspire them can be shared in funding proposals, preanalysis plans, or other report. Articulating them in a design may help clarify whether proper account was taken of risks ex ante, or, more usefully, remind researchers to be sure to take account of them."
  },
  {
    "objectID": "lifecycle/planning/ethics.html#illustration-estimating-expected-costs-and-expected-learning",
    "href": "lifecycle/planning/ethics.html#illustration-estimating-expected-costs-and-expected-learning",
    "title": "\n47  Ethics\n",
    "section": "\n47.1 Illustration: Estimating expected costs and expected learning",
    "text": "47.1 Illustration: Estimating expected costs and expected learning\nWe illustrate how researchers can weigh the tradeoffs between the value of research and its ethical costs with a hypothetical audit study of discrimination in government hiring. We imagine that three characteristics of applicants to a municipal government job are randomized and whether the applicant receives a callback is recorded. The three candidate characteristics are race (Black or White), area or residence (urban or suburban), or high school attended (East or West).\nSuppose we could rank the scientific importance of measuring discrimination along all three dimensions and we judged race-based discrimination of high importance and discrimination on the basis of residence or high school to be medium and low importance, respectively.\nThe value of the research then is a function of the importance of the inquiries of course, but also how much we learn about it. We proxy for the learning from the experiment by sample size: the higher the \\(N\\), the more we learn, but with decreasing marginal returns (it’s a lot better to have a sample of 100 compared to 10; it matters less if it 1010 or 1100). Figure 12.1 shows the three expected learning curves labeled by the importance of the inquiry.\nTurning now to the expected costs side of the calculation. Because the job applicants are fictitious but appear real, alongside concerns around deception, a primary ethical concern in audit experiments is how much time hiring managers waste reviewing fictitious applications. In the case of government hiring, it is public money spent on their review. Suppose the cost to participants is linear in the number of applications since application takes about ten minutes to review. We represent the cost to participants as the purple line in Figure 12.1.\nWe have placed the costs to participants on the same scale as the value of the research, by placing a value to society of the research and a value to society of the administrative time. When benefit exceeds cost (upper blue region), we decide to move forward with the research; if costs exceed benefits (lower gray region), we do not conduct the study.\nThe key take away from the graph is that there is a region at low sample sizes where the cost to participants exceed the benefits from the research, because of the very imprecise answers we get from the research. We don’t learn enough about the inquiry, despite its importance, to make it worth wasting the hiring managers time. In the medium importance cases there is a “goldilocks” range: there a region of the benefits curve (highlighted in blue) where it is worth doing the study, but two regions (highlighted in gray) above and below it where it is not worth it. The left region is where the sample is too small so the value of the research is low both because of its medium importance and we do not learn enough about it. The second gray region at right in the medium importance curve is where though we learn a lot about the inquiry, the cost is too high from the many hours of hiring manager time to justify what we learn because the inquiry is not important enough.\nIn short, in principle, ethical decisions can be informed by diagnosis both of how much we learn and how much it costs to participants (along with other ethical costs). The key gain comes from getting a sharper understanding of the gains from design decisions on the research side; the problem of placing value on those gains is not addressed within the framework of course.\nCalculations of this form however always open up risks that researchers overestimate the importance of their research or are blind to ethical costs (in this case for instance an additional cost might arise if because of the study subjects, or other officials, became more skeptical of future real applicants). In practice strategies that maximize autonomy for implicated actors can make these choices easier. In this case for instance if prior consultations with members of the subject population that assessed how they saw the benefits of the research relative to the costs, and how they assessed the costs from this type of deception, would go a long way to ally ethical concerns.\n\n\nFigure 47.1: Tradeoffs between ethical costs and scientific benefits. A design might have too many subjects but also too few subjects."
  },
  {
    "objectID": "lifecycle/planning/ethics.html#institutional-review-boards",
    "href": "lifecycle/planning/ethics.html#institutional-review-boards",
    "title": "\n47  Ethics\n",
    "section": "\n47.2 Institutional review boards",
    "text": "47.2 Institutional review boards\nWhen researchers sit at universities in the United States, research must be approved by the university’s institutional review board (IRB) under the federal regulation known as the “Common Rule.” Similar research review bodies exist at universities worldwide and at many independent research organizations and think tanks. Though these boards are commonly thought to judge research ethics, but they have a second function to protect their institution from liability for research gone awry Schrag (2010). Insofar as institutional and ethical goals are aligned, IRBs help ensure responsible research practices, but as a general matter institutional approval is not a substitute for ethical engagement by researchers.\n\n\n\n\nBaron, Hannah, and Lauren E. Young. 2021. “From Principles to Practice: Methods to Increase the Transparency of Research Ethics in Violent Contexts.” Political Science Research and Methods, 1–8.\n\n\nHumphreys, Macartan. 2015. “Reflections on the Ethics of Social Experimentation.” Journal of Globalization and Development 6 (1): 87–112.\n\n\nKing, Gary, and Melissa Sands. 2015. “How Human Subjects Research Rules Mislead You and Your University, and What to Do about It.” Unpublished Manuscript.\n\n\nLyall, Jason. 2022. “Preregister Your Ethical Redlines: Vulnerable Populations, Policy Engagement, and the Perils of e-Hacking.” Unpublished Manuscript.\n\n\nSchrag, Zachary M. 2010. Ethical Imperialism: Institutional Review Boards and the Social Sciences, 1965–2009. Johns Hopkins University Press.\n\n\nSlough, Tara. 2020. “The Ethics of Electoral Experimentation: Design-Based Recommendations.” Unpublished Manuscript.\n\n\nTeele, Dawn. 2021. “Virtual Consent: A Bronze Standard for Experimental Ethics.” In Cambridge Handbook of Experimental Political Science, edited by Donald P. Green and James N. Druckman. New York: Cambridge University Press.\n\n\nWood, Elisabeth Jean. 2006. “The Ethical Challenges of Field Research in Conflict Zones.” Qualitative Sociology 29 (3): 373–86."
  },
  {
    "objectID": "lifecycle/planning/partners.html",
    "href": "lifecycle/planning/partners.html",
    "title": "\n48  Partners\n",
    "section": "",
    "text": "In the best-case scenario, the goals of the researchers and partner organizations are aligned. When the scientific question to be answered is the same as the practical question the organization cares about, the gains from cooperation are clear. The research team gains access to the organization’s financial and logistical capacity to act in the world, and the partner organization gains access to the researchers’ scientific expertise. Finding the right research partner almost always amounts to finding an organization with a common – or at least not conflicting – goal. Selecting a research design amenable to both parties requires understanding each partners’ private goals. Research design declaration and diagnosis can help with this problem by formalizing tradeoffs between the two sets of goals.\nOne frequent divergence between partner and researcher goals is that partner organizations often want to learn, but they care most about their primary mission (Levine 2021). This dynamic is sometimes referred to as the “learning versus doing” tradeoff. (In business settings, this tradeoff goes by names like “learning versus earning” or “exploration versus exploitation”). An aid organization cares about delivering their program to as many people as possible. Learning whether the program has the intended effects on the outcomes of interest is obviously also important, but resources spent on evaluation are resources not spent on program delivery.\n\nDiagnosis 48.1 (Learning versus doing diagnosis) Research design diagnosis can help navigate the learning versus doing tradeoff. One instance of the tradeoff is that the proportion of units that receive a treatment represents the rate of “doing,” but this rate also affects the amount of learning. In the extreme, if all units are treated, we can’t measure the effect of the treatment. The tradeoff here is represented in Figure 48.1, which shows the study’s power versus the proportion treated (top facet) and the partner’s utility (bottom facet). The researchers have a power cutoff at the standard 80% threshold. The partner also has a strict cutoff: they need to treat at least 2/3 of the sample to fulfill a donor requirement.\nResearchers might simply ignore the proportion treated and select the design with the highest power in the absence of partners. With a partner organization, the researcher might use this graph in conversation with the partner to jointly select the design that has the highest power that has a sufficiently high proportion treated to meet the partner’s needs. This is represented in the “zone of agreement” in gray: in this region, the design has at least 80% power and at least two-thirds of the sample are treated. Deciding within this region involves a tradeoff between power (which is decreasing in the proportion treated here) and the partner’s utility (which is increasing in proportion treated). The diagnosis surfaces the zone of the agreement and clarifies the choice between designs in that region. Unfortunately, some partnerships simply will not work out if the zone of agreement is empty.\n\n\nFigure 48.1: Finding the zone of agreement in a research partnership.\n\n\nChoosing the proportion treated is one example of integrating partner constraints into research designs. A second common problem is that there are a set of units that must be treated or that must not be treated for ethical or political reasons (e.g., the home district of a government partner must receive the treatment). If these constraints are discovered after treatment assignment, they lead to noncompliance, which may substantially complicate the analysis of the experiment and even prevent providing an answer to the original inquiry. Gerber and Green (2012) recommend, before randomizing treatment, exploring possible treatment assignments with the partner organization and using this exercise to elicit the set of units that must or cannot be treated. King et al. (2007) describe a “politically-robust” design, which uses pair-matched block randomization. In this design, when any unit is dropped due to political constraints, the whole pair is dropped from the study.1\nA major benefit of working with partners is their deep knowledge of the substantive area. For this reason, we recommend involving them in the design declaration and diagnosis process. How can we develop intuitions about the means, variances, and covariances of the variables to be measured? Ask your partner for their best guesses, which may be far more educated than your own. For experimental studies, solicit your partner’s beliefs about the magnitude of the treatment effect on each outcome variable, subgroup by subgroup if possible. Engaging partners in the declaration process improves design – and it very quickly sharpens the discussion of key design details. Pro-tip: share your design diagnoses and mock analyses before the study is launched to quickly build consensus around the study’s goals.\n\n\n\n\n\nGerber, Alan S., and Donald P. Green. 2012. Field Experiments: Design, Analysis, and Interpretation. New York: W.W. Norton.\n\n\nKing, Gary, Emmanuela Gakidou, Nirmala Ravishankar, Ryan T Moore, Jason Lakin, Manett Vargas, Martha Marı́a Téllez-Rojo, Juan Eugenio Hernández Ávila, Mauricio Hernández Ávila, and Héctor Hernández Llamas. 2007. “A ‘Politically Robust’ Experimental Design for Public Policy Evaluation, with Application to the Mexican Universal Health Insurance Program.” Journal of Policy Analysis and Management 26 (3): 479–506.\n\n\nLevine, Adam Seth. 2021. “How to Form Organizational Partnerships to Run Experiments.” In Advances in Experimental Political Science, 199–216. Cambridge, UK: Cambridge University Press.\n\n\n\n\nThis procedure is prone to bias for the average treatment effect among the “politically feasible” units if within some pairs, one unit is treatable but the other is not.↩︎"
  },
  {
    "objectID": "lifecycle/planning/funding.html",
    "href": "lifecycle/planning/funding.html",
    "title": "\n49  Funding\n",
    "section": "",
    "text": "To relax the budget constraint, researchers apply for funding. Funding applications have to communicate important features of the proposed research design. Funders want to know why the study would be useful, important, or interesting to scholars, the public, or policymakers. They also want to ensure that the research design provides credible answers to the question and that the research team is capable of executing the design. Since it’s their money on the line, funders also care that the design provides good value-for-money.\nResearchers and funders have an information problem. Applicants wish to obtain as large a grant as possible for their design but have difficulty credibly communicating the quality of their design given the subjectivity of the exercise. On the flip side, funders wish to get the most value-for-money in the set of proposals they decide to fund and have difficulty assessing the quality of proposed research. Design declaration and diagnosis provide a partial solution to the information problem. A common language for communicating the proposed design and its properties can communicate the value of the research under design assumptions that can be understood and interrogated by funders.\nFunding applications could usefully include a declaration and diagnosis of the proposed design. In addition to common diagnosands such as bias and efficiency, two special diagnosands may be valuable: cost and value-for-money. The cost can be included for each design variant as a function of design features such as sample size, the number of treated units, and the duration of survey interviews. Simulating the design across possible realizations of each variant explains how costs vary with choices the researcher makes. Value-for-money is a diagnosand that is a function of cost and the amount learned from the design.\nIn some cases, funders request applicants to provide multiple options and multiple price points or make clear how a design could be altered so that it could be funded at a lower level. Redesigning over differing sample sizes communicates how the researcher conceptualizes these options and provides the funder with an understanding of tradeoffs between the amount of learning and cost in these design variants. Applicants could use the redesign process to justify the high cost of their request directly in terms of the amount learned.\nEx-ante power analyses are required by an increasing number of funders. Current practice, however, illustrates the crux of the misaligned incentives between applicants and funders. Power calculators online have difficult-to-interrogate assumptions built in and cannot accommodate the specifics of many common designs (Blair et al. 2019). As a result, existing power analyses can demonstrate that almost any design is “sufficiently powered” by changing expected effect sizes and variances. Design declaration is a partial solution to this problem. By clarifying the assumptions of the design in code, applicants can more clearly link the assumptions of the power analysis to the specifics of the design setting.\nFinally, design declarations can , in principlpe, help funders compare applications on standard scales: root mean-squared-error, bias, and power. Moving design considerations onto a common scale takes some of the guesswork out of the process and reduces reliance on researcher claims about properties of designs.\n\n\n\n\nBlair, Graeme, Jasper Cooper, Alexander Coppock, and Macartan Humphreys. 2019. “Declaring and Diagnosing Research Designs.” American Political Science Review 113 (3): 838–59."
  },
  {
    "objectID": "lifecycle/planning/piloting.html",
    "href": "lifecycle/planning/piloting.html",
    "title": "\n50  Piloting\n",
    "section": "",
    "text": "We may have reasonably educated guesses about these parameters from past studies or theory. Our understanding of the nodes and edges in the causal graph of M, expected effect sizes, the distribution of outcomes, feasible randomization schemes, and many other features are directly selected from past research or chosen based on a literature review of past studies.\nEven so, we remain uncertain about these values. One reason for the uncertainty is that our research context and inquiries often differ subtly from previous work. Even when replicating an existing study as closely as possible, difficult-to-intuit features of the research setting may have serious consequences for the design. Moreover, our uncertainty about a design parameter is often the very reason for conducting a study. We run experiments because we are uncertain about the average treatment effect. Frustratingly, we always have to design under model uncertainty.\nThe main goal of pilot studies is to reduce this uncertainty. We would like use learn which models in M are more likely, so that the main study can be designed under beliefs that are closer to the truth. Pilots take many forms: focus groups, small-scale tests of measurement tools, even miniature versions of the main study on a smaller scale. We want to learn things like the distribution of outcomes, how covariates and outcomes might be correlated, or how feasible the assignment, sampling, and measurement strategies are.\nAlmost by definition, pilot studies are inferentially weaker than main studies. We turn to them in response to constraints on our time, money, and capacity. If we were not constrained, we would run a first full-size study, learn what is wrong with our design, then run a corrected full-size study. Since running multiple full studies is too expensive or otherwise unfeasible, we run either smaller mini-studies or test out only a subset of the elements of our planned design. Accordingly, the diagnosands of a pilot design will not measure up to those of the main design. Pilots have much lower statistical power and may suffer from higher measurement error and less generalizability. Accordingly, the goal of pilot studies should not be to obtain a preliminary answer to the main inquiry, but instead to learn the information that will make the main study a success.\nLike main studies, pilot studies can be declared and diagnosed – but importantly, the diagnosands for main and pilot studies need not be the same. Statistical power for an average treatment effect may be an essential diagnosand for the main study, but owing to their small size, power for pilot studies will typically be abysmal. Pilot studies should be diagnosed with respect to the decisions they imply for the main study.\nFigure 50.1 shows the relationship between effect size and the sample size required to achieve 80% statistical power for a two-arm trial using simple random assignment. Uncertainty about the true effect size has enormous design consequences. If the effect size is 0.17, we need about 1,100 subjects to achieve 80% power. If it’s 0.1, we need 3200.\n\n\nFigure 50.1: Minimum required sample sizes and uncertainty over effect size\n\n\nSuppose we have prior beliefs about the effect size that can be summarized as a normal distribution centered at 0.3 with a standard deviation of 0.1, as in the bottom panel of Figure 48.1. We could choose a design that corresponds to this best guess, the average of our prior belief distribution. If the true effect size is 0.3, then a study with 350 subjects will have 80% power.\nHowever, redesigning the study to optimize for the “best guess” is risky because the true effect could be much smaller than 0.3. Suppose we adopt the redesign heuristic of powering the study for an effect size at the 10th percentile of our prior belief distribution, which works out here to be an effect size of 0.17. Following this rule, we would select a design with 1100 subjects.\nNow suppose the true effect size is, in actuality, only 0.1, so we would need to sample 3200 subjects for 80% power. The power of our chosen 1100-subject design is a mere 38%. Here we see the consequences of having incorrect prior beliefs: our ex-ante guess of the effect size was too optimistic. Even taking what we thought of as a conservative choice – the 10th percentile redesign heuristic – we ended up with too small a study.\nA pilot study can help researchers update their priors about important design parameters. If we do a small scale pilot with 100 subjects, we’ll get a noisy but unbiased estimate of the true effect size. We can update prior beliefs by taking a precision weighted average of our priors and the estimate from the pilot, where the weights are the inverse of the variance of each guess. Our posterior beliefs will be closer to the truth, and our posterior uncertainty will be smaller. If we then follow the heuristic of powering the 10th percentile of our (now posterior) beliefs about effect size, we will have come closer to correctly powering our study. Figure 50.2 shows how large the studies would be, depending on how the pilot study came out if we were to follow the 10th percentile decision rule. On average, the pilot leads us to design the main study with 1800 subjects, sometimes more and sometimes less.\nThis exercise reveals that a pilot study can be quite valuable. Without a pilot study, we would chose to sample 1100 subjects, but since the true effect size is only 0.1 (not our best guess of 0.3), the experiment would be underpowered. The pilot study helps us correct our diffuse and incorrect prior beliefs. However, since the pilot is small, we don’t update our priors all the way to the truth. We still end up with a main study that is on average too small (1800), with a corresponding power of 56%. That said, a 56% chance of finding a statistically significant result is better than a 38% chance.\n\n\nFigure 50.2: Distribution of post-pilot sample size choices\n\n\nIn summary, pilots are most useful when we are uncertain – or outright wrong – about important design parameters. This uncertainty can often be shrunk by quite a bit without running pilot studies by meta-analyzing past empirical studies. Some things are hard to learn by reading others’ work; pilot studies are especially useful tools for learning about those things."
  },
  {
    "objectID": "lifecycle/planning/criticism.html",
    "href": "lifecycle/planning/criticism.html",
    "title": "\n51  Criticism\n",
    "section": "",
    "text": "The best moments to seek advice come before registering preanalysis plans or, if not writing a PAP, before implementing major data strategy elements. The point is not to seek advice exclusively on sampling, assignment, or measurement procedures; the important thing is that there’s still time to modify those design elements (Principle 3.4: Design early). Feedback about the design as a whole can inform changes to the data strategy before it is set in stone.\nFeedback will come in many forms. Sometimes the comments are directly about diagnosands. The critic may think the design has too many arms and won’t be well-powered for many inquiries. Or they may be concerned about bias due to excludability violations or selection issues. These comments are especially useful because they can easily be incorporated in design diagnosis and redesign exercises.\nOther comments are harder to pin down. A fruitful exercise in such cases is to understand how the criticism fits in to M, I, D, and A. Comments like, “I’m concerned about external validity here” might seem to be about the data strategy. If the units were not randomly sampled from some well-specified population, we can’t generalize from the sample to the population. But if the inquiry is not actually a population quantity, then this inability to use sample data to estimate a population quantity is irrelevant. The question then becomes whether knowing the answer to your sample inquiry helps make theoretical progress or whether we need to generalize – to switch the inquiry to the population quantity to make headway. Critics will not usually be specific about how their criticism relates to each element of design, so it is up to the criticism-seeker to understand the implications for design.\nSometimes we seek feedback from smart people, but they do not immediately understand the design setting. If the critic hasn’t absorbed or taken into account important features of the design, their recommendations and amendments may be off-base. For this reason, it’s important to communicate the design features – the model, inquiry, data strategy, and answer strategy – at a high enough level of detail that the critic is up to speed before passing judgment."
  },
  {
    "objectID": "lifecycle/planning/preanalysis-plan.html",
    "href": "lifecycle/planning/preanalysis-plan.html",
    "title": "\n52  Preanalysis Plan\n",
    "section": "",
    "text": "PAPs are sometimes misinterpreted as a binding commitment to report all pre-registered analyses and nothing but. This view is unrealistic and unnecessarily rigid. While we think that researchers should report all pre-registered analyses somewhere (see Section (populated-preanalysis-plan?) on “populated PAPs”), study write-ups inevitably deviate in some way from the PAP – and that’s a good thing. Researchers learn more by conducting research. This learning can and should be reflected in the finalized answer strategy. One guardrail against extensive post-PAP design changes can be a set of standard operating procedures that lays out what to do when circumstances change (Green and Lin 2016).\nOur hunch is that the main consequence of actually writing PAPs is that research designs improve. Just like design declaration forces us to think through the details of our model, inquiry, data strategy, and answer strategy, describing those choices in a publicly-posted document surely causes deeper reflection about the design. In this way, the main audience for a PAP is the study authors themselves.\nWhat belongs in a PAP? Recommendations for the set of decisions that should be specified in a PAP remain remarkably unclear and inconsistent across research communities. PAP templates and checklists are proliferating, and the number of items they suggest ranges from nine to sixty. PAPs themselves are becoming longer and more detailed. Some in the American Economic Association and Evidence in Governance and Politics (EGAP) study registries reach hundreds of pages as researchers seek to be ever more comprehensive. Some registries emphasize the registration of the hypotheses to be tested, while others emphasize the registration of the tests that will used. In a review of many PAPs, Ofosu and Posner (2022) find considerable variation in how often analytically-relevant pieces of information appear in posted plans.\nIn our view a PAP should center on a design declaration. Currently, most PAPs focus on the answer strategy A: what estimator to use, what covariates to condition on, and what subsets of the data to include. But of course, we also need to know the details of the data strategy D: how units will be sampled, how treatments will be assigned, and how the outcomes will be measured. We need these details to assess the properties of the design and gauge whether the principles of analysis respecting sampling, treatment assignment, and measurement procedures are being followed. We need to know about the inquiry I because we need to know the target of inference. A significant concern is “outcome switching,” wherein the eventual report focuses on different outcomes than initially intended. When we switch outcomes, we switch inquiries! We need enough of the model M in the plan to describe I in sufficient detail. In short, a design declaration is what belongs in a PAP because a design declaration specifies all of the analytically-relevant design decisions.\nIn addition to a design declaration, a PAP should include mock analyses conducted on simulated data. If the design declaration is made formally in code, creating simulated data that resemble the eventually realized data is straightforward. We think researchers should run their answer strategy on the mock data, creating mock figures and tables that will ultimately be made with real data. In our experience, this is the step that really causes researchers to think hard about all aspects of their design.\nPAPs can, optionally, include design diagnoses in addition to declarations, since it can be informative to describe why a particular design was chosen. For this reason, a PAP might include estimates of diagnosands like power, root-mean-squared-error, or bias. If a researcher writes in a PAP that the power to detect a very small effect is large, then if the study comes back null, the eventual write-up can much more credibly rule out “low precision” as an explanation for the null.\n\n52.0.1 Example Preanalysis Plan\nWe provide an example preanalysis plan in the appendix for Bonilla and Tillery (2020), a study of the effects of alternative framings of Black Lives Matter on support for the movement. The authors of that study posted a preanalysis plan to the As Predicted registry: link. These study authors are models of research transparency: they prominently link to the PAP in the published article, they conduct no non-preregistered analyses except those requested during the review process, and their replication archive includes all materials required to confirm their analyses, all of which we were able to reproduce exactly with minimal effort. Our goal with this alternative PAP is to show how design declaration can supplement and complement existing planning practices.\nWe show in section (populated-preanalysis-plan?) how to “populate” this PAP once the data have been realized and collected.\n\n\nExample preanalysis plan\n\n\n\n\n\n\nBonilla, Tabitha, and Alvin B. Tillery. 2020. “Which Identity Frames Boost Support for and Mobilization in the #BlackLivesMatter Movement? An Experimental Test.” American Political Science Review 114 (4): 947–62.\n\n\nCasey, Katherine, Rachel Glennerster, and Edward Miguel. 2012. “Reshaping Institutions: Evidence on Aid Impacts Using a Pre-Analysis Plan.” The Quarterly Journal of Economics 127 (4): 1755–1812.\n\n\nGreen, Donald P., and Winston Lin. 2016. “Standard Operating Procedures: A Safety Net for Pre-Analysis Plans.” PS: Political Science & Politics 49 (3): 495–99.\n\n\nHumphreys, Macartan, Raul de la Sierra, and Peter van der Windt. 2013. “Fishing, Commitment, and Communication: A Proposal for Comprehensive Nonbinding Research Registration.” Political Analysis 21 (1): 1–20.\n\n\nMiguel, Edward, Colin Camerer, Katherine Casey, Joshua Cohen, Kevin M Esterling, Alan S. Gerber, Rachel Glennerster, Donald P. Green, Macartan Humphreys, and Guido W. Imbens. 2014. “Promoting Transparency in Social Science Research.” Science 343 (6166): 30.\n\n\nOfosu, George, and Daniel Posner. 2022. “Pre-analysis Plans: An Early Stocktaking.” Perspectives on Politics."
  },
  {
    "objectID": "lifecycle/realization/index.html",
    "href": "lifecycle/realization/index.html",
    "title": "\n53  Realization\n",
    "section": "",
    "text": "When implementation is complete, the design preregistered in an analysis plan can be “populated” to report on analyses as planned and the realized design reconciled with the planned design. In writing up the study, the design forms the center: why we should believe the answers we report. The declared design can be used in the write-up to convince reviewers of the study’s quality, and also a tool to assess the impact of reviewer suggestions on the design."
  },
  {
    "objectID": "lifecycle/realization/pivoting.html",
    "href": "lifecycle/realization/pivoting.html",
    "title": "\n54  Pivoting\n",
    "section": "",
    "text": "We illustrate with three real choices we had to make, one in which criticism to a design by participants led to a simple improvement to the assignment mechanism (\\(D\\)) and two in which difficulties implementing assignments led to a change in inquiries (\\(I\\)) but for quite different reasons.\nThe first study was one in which one of us, in coordination with political parties in Uganda, a random selection of MPs were to be selected into a pilot political communications program. The design called for simple random assignment. Shortly before implementation, however, the political parties complained that the randomization scheme was one that could produce inequalities between the parties, with some parties getting more benefits than others just because of the luck of the draw. They asked whether it would be possible to “fix” things so that each party would have the same share participating.\nThe answer of course was yes: a change in \\(D\\) to employ blocked random assignment addressed the fairness concerns of the party but also led to a demonstrably better design. In the final design, members of each party pulled names out of a hat that contained names only of people from their own party. This is the rare pivot that both attends to an unanticipated complaint and improves the design in the offing.\nThe second study was one in which another of us faced a serious noncompliance problem. We launched a cluster-randomized, placebo-controlled 2x2 factorial trial in Nigeria of a film treatment and a text message blast treatment. Treatment was to be rolled out in 200 communities. A few days after treatment delivery began, we noticed that the number of replies was extremely similar in treatment and placebo communities, counter to expectations. We discovered that our research partner, the cell phone company, had delivered the treatment message to both groups! By that time, treatments had been delivered to 106 communities (about half the sample).\nWe faced the choice to abandon the study or pivot and adapt the study. We quickly agreed that we could not continue research in the 106 communities, because they had received at least partial treatment. We were left with 109 from our original sample of 200 plus 15 alternates that were selected in the same random sampling process. We determined we could not retain all four treatment conditions and the pure control. We decided that at most we could have two conditions, with about 50 units in each. But which ones? We were reticent to lose the text message or the film treatments, as both tested two distinct theoretical mechanisms for how to encourage prosocial behaviors. We decided to drop the pure control group, the fifth condition, as well as the placebo text message condition. In this way, we could learn about the effect of the film (compared to placebo) and about the effect of the text messages (compared to none).1 Thus we had to reduce the size of our inquiry set. Essentially we ended up salvaging a subpart of our design without having to materially change any design elements within this subpart.\nFinally, one of us was involved with, a get-out-the-vote canvassing-experiment-gone-wrong during the 2014 Senate race in New Hampshire. We randomly assigned 4,230 of 8,530 subjects to treatment. However, approximately two weeks before the election, canvassers had only attempted 746 subjects (17.6% of the treatment group) and delivered treatment to just 152 subjects (3.6%). In essence, the implementer had been overly optimistic about the number of subjects they would be able to contact in time for the election. In their revised assessment the organization estimated that they would only be able to attempt to contact 900 more voters. They also told us also that they believed that their efforts would be best spent on voters with above-median vote propensities.\nWe faced a choice: should we (1) stick to our design and continue trying to treat 900 of the remaining subjects that were allocated to treatment, knowing that we will have many non-compliers in this set or (2) alter D to conduct a whole new random assignment among above-median propensity voters only. A design diagnosis reveals a clear course of action. Even though it decreases the overall sample size, restricting the study to the above-median propensity voters substantially increases the precision of the design.2 Opting for this modification to D required thinking through whether we were willing to accept a change in I since the set of compliers for which we could generate estimates is different under the two designs.\nPivoting is sometimes hard and sometimes easy but, very often, assessing whether or how to pivot requires thinking through the full design to see which parts have to change as others change. If you do this through redesign your design becomes a living document and becomes a tool to guide you along the research path, not just as a document to write at the beginning of the study and revisit when you are writing up.\n\n\n\n\nWe randomized half of the communities to receive the treatment film and half the placebo. We then used an over-time stepped-wedge design to study the effect of the text message, randomizing how many days after the film was distributed the text message was sent.↩︎\nThis conclusion follows the logic of the placebo-controlled design described in Section (placebo-controlled-experiments?).↩︎"
  },
  {
    "objectID": "lifecycle/realization/populated-pap.html",
    "href": "lifecycle/realization/populated-pap.html",
    "title": "\n55  Populated preanalysis plan\n",
    "section": "",
    "text": "A preanalysis plan describes how study data will eventually be analyzed, but those plans may change in the during the process of producing a finished report, article, or book. Inevitably, authors of pre-analysis plans fail to anticipate how the data generated by the study will eventually be analyzed. Some reasons for discrepancies were discussed in the previous section on pivoting, but others intervene as well. A common reason is that PAPs promise too many analyses. In writing a concise paper, some analyses are dropped, others are combined, and still others are added during the writing and revision process. In the next section, we’ll describe how to reconcile analyses-as-planned with analyses-as-implemented, but this present section is about what to do with your analysis plan immediately after getting the data back.\nWe echo proposals made in Banerjee et al. (2020) and Alrababa’h et al. (2022) that researchers should produce short reports that fulfill the promises made in their PAPs. Banerjee et al. (2020) emphasize that writing PAPs is difficult and usually time-constrained, so it is natural that the final paper will reflect further thinking about the full set of empirical approaches. A “populated PAP” serves to communicate the results of the promised analyses. Alrababa’h et al. (2022) cite the tendency of researchers to abandon the publication of studies that return null results. To address the resulting publication bias, they recommend “null results reports” that share the results of the pre-registered analyses. We think these reports (whether the results come back null or otherwise) can easily be added to the appendix of the final write up or included in a replication archive.\nWe recommended in Section (preanalysis-plan?) that authors include mock analyses in their PAPs using simulated data. Doing so has the significant benefit of being specific about the details of the answer strategy. A further benefit comes when it is time to produce a populated PAP, since the realized data can quite straightforwardly be swapped in for the mock data. Given the time invested in building simulated analyses for the PAP, writing up a populated PAP takes only as much effort as is needed to clean the data (which will need to be done in any case)."
  },
  {
    "objectID": "lifecycle/realization/populated-pap.html#example",
    "href": "lifecycle/realization/populated-pap.html#example",
    "title": "\n55  Populated preanalysis plan\n",
    "section": "\n55.1 Example",
    "text": "55.1 Example\nIn Section (planning?), we declared the design for Bonilla and Tillery (2020) following their preanalysis plan. In doing so, we declared an answer strategy in code. In our populated PAP, we can run that same answer strategy code, but swap out the simulated data for the real data collected during the study.\n\n\nExample populated PAP\n\n\n\n\n\n\nAlrababa’h, Ala’, Scott Williamson, Andrea Dillon, Jens Hainmueller, Dominik Hangartner, Michael Hotard, David Laitin, Duncan Lawrence, and Jeremy Weinstein. 2022. “Learning from Null Effects: A Bottom-up Approach.” Political Analysis.\n\n\nBanerjee, Abhijit, Esther Duflo, Amy Finkelstein, Lawrence F Katz, Benjamin A Olken, and Anja Sautmann. 2020. “In Praise of Moderation: Suggestions for the Scope and Use of Pre-Analysis Plans for RCTs in Economics.” Working Paper 26993. Working Paper Series. National Bureau of Economic Research. https://doi.org/10.3386/w26993.\n\n\nBonilla, Tabitha, and Alvin B. Tillery. 2020. “Which Identity Frames Boost Support for and Mobilization in the #BlackLivesMatter Movement? An Experimental Test.” American Political Science Review 114 (4): 947–62."
  },
  {
    "objectID": "lifecycle/realization/reconciliation.html",
    "href": "lifecycle/realization/reconciliation.html",
    "title": "\n56  Reconciliation\n",
    "section": "",
    "text": "Reconciliation is the process of comparing the final design to the planned design. Understanding the differences between the two and the justifications for the changes can help us understand what to learn from the final results.\nSuppose the original design described a three-arm trial: one control and two treatments, but the design as implemented drops all subjects assigned to the second treatment. Sometimes, this is an entirely appropriate and reasonable design modification. Perhaps the second treatment was simply not delivered due to an implementation failure. Other times, these modifications are less benign. Perhaps the second treatment effect estimate did not achieve statistical significance, so the author omitted it from the analysis.\nFor this reason, we recommend that authors reconcile the design as planned with the design as implemented. A reconciliation can be a plain description of the deviations from the PAP, with justifications where appropriate. A more involved reconciliation would include a declaration of the planned design, a declaration of the implemented design, and a list of the differences. This “diff” of the designs can be automated through the declaration of both designs in computer code, then comparing the two design objects line-by-line (see the function compare_designs() in DeclareDesign).\nIn some cases, reconciliation will lead to additional learning beyond what can be inferred from the final design itself. When some units refuse to be included in the study sample or some units refuse measurement, we learn that important information about those units. Understanding sample exclusions, noncompliance, and attrition not only may inform future research design planning choices but contribute substantively to our understanding of the social setting.\nThere are no current standards for how to report reconciliation. We recommend however providing two types of tables, a qualitative Inquiry reconciliation table and a quantitative Answer strategy reconciliation table. In the latter case especially, guiding readers to relevant discussions or sections of replication code can make the reconciliation tables easier to evaluate. Such answer strategy reconciliation tables could also usefully include output from diagnosis comparisons to provide motivation for change.\nWe provide illustrations for both of these below:\nInquiry | Following A from the PAP| Following A from the paper | Notes |\n————–|:————————- :|:——————————-:|——————- |\nGender effect | estimate = 0.6, s.e = 0.31 | estimate = 0.6, s.e = 0.25 | Difference due to change in control variables [provide cross references to tables and code] |\nTable: (#tab:reconciliation2) Illustration of an Answer strategy reconciliation table."
  },
  {
    "objectID": "lifecycle/realization/reconciliation.html#example",
    "href": "lifecycle/realization/reconciliation.html#example",
    "title": "\n56  Reconciliation\n",
    "section": "\n56.1 Example",
    "text": "56.1 Example\nIn Section (planning?), we described the preanalysis plan registered by Bonilla and Tillery (2020). We reconcile the set of conditional average treatment effect (CATE) analyses planned in that PAP, the analyses reported in the paper, and those reported in the appendix at the request of reviewers in Table ?tbl-reconciliation. In column two, we see that the authors planned four CATE estimations: effects by familiarity with Black Lives Matter; by gender; LGBTQ status; and linked fate. Only two of those are reported in the paper; the others may have been excluded for space reasons. Another way to handle these uninteresting results would be to present them in a populated PAP posted on their Web site or in the paper’s appendix.\nIn their appendix, the authors report on a set of analyses requested by reviewers. We see this as an excellent example of transparently presenting the set of planned analyses and highlighting the analyses that were added afterward and why they were added. They write: “We have been asked to consider other pertinent moderations beyond gender and LGBTQ+ status. They are contained in the four following sections.”\nThis small Inquiry reconciliation table describes the heterogeneous effects analyses the researchers planned, those reported in the paper, and those reported in the appendix at the request of reviewers.\n\n(#tab:reconciliation) Inquiry reconciliation table from Bonilla and Tillery (2020) analysis with their preanalysis plan.\n\n\n\n\n\n\n\nCovariate\nIn the preanalysis plan\nIn the paper\nIn the appendix (at the request of reviewers)\n\n\n\nFamiliarity with BLM\nX\n\n\n\n\nGender\nX\nX\n\n\n\nLGBTQ status\nX\nX\n\n\n\nLinked fate\nX\n\n\n\n\nReligiosity\n\n\nX\n\n\nRegion\n\n\nX\n\n\nAge\n\n\nX\n\n\nEducation\n\n\nX\n\n\n\n\n\n\n\nBonilla, Tabitha, and Alvin B. Tillery. 2020. “Which Identity Frames Boost Support for and Mobilization in the #BlackLivesMatter Movement? An Experimental Test.” American Political Science Review 114 (4): 947–62."
  },
  {
    "objectID": "lifecycle/realization/writing.html",
    "href": "lifecycle/realization/writing.html",
    "title": "\n57  Writing\n",
    "section": "",
    "text": "Elements of MIDA may appear throughout a paper. In ?fig-midamousa below, we annotate Mousa (2020) by highlighting where in the article each design component is discussed. The study reports on the results of a randomized experiment in which Iraqi Christians were assigned either to an all-Christian soccer team or a team in which they would play alongside Muslims. The experiment tested whether being on a mixed team affected intergroup attitudes and behaviors, both among teammates and back at home after the games were over. We highlight in color areas discussing the model M in yellow, the inquiry I in green, the data strategy D in blue, and the answer strategy A in pink.\n\n\nPaper with MIDA elements highlighted (Mousa 2020).\n\n\nThe model and the inquiry largely appear in the abstract and introductory portion of the paper, though aspects of the model are discussed later on. Much of the first three pages are devoted to the data strategy, while the answer strategy only appears briefly. This division makes sense: in this paper, the action is all in the experimental design whereas the answer strategy follows straightforwardly from it. The paper mostly describes M and D, with only a small amount of text devoted to I and A. Finally, it is notable that the data strategy is interspersed with aspects of the model. The reason is that the author is justifying choices about randomization and measurement using features of the model.\n\n\n\n\nMousa, Salma. 2020. “Building Social Cohesion Between Christians and Muslims Through Soccer in Post-ISIS Iraq.” Science 369 (6505): 866–70."
  },
  {
    "objectID": "lifecycle/integration/index.html",
    "href": "lifecycle/integration/index.html",
    "title": "\n58  Integration\n",
    "section": "",
    "text": "Most immediately, authors share their findings with the public through the media and with decision-makers. Design information is useful for helping journalists to emphasize design quality rather than splashy findings. Decision-makers may act on evidence from studies, and researchers who want to influence policymaking and business decisions may wish to consider diagnosands about the decisions these actors make.\nResearchers can prepare for the integration of their studies into scholarly debates through better archiving practices and better reporting of research designs in the published article. Future researchers may build on the results of a past study in three ways. First, they may reanalyze the original data. Reanalysts must be cognizant of the original data strategy D when working with the realized data \\(d\\). Changes to the the answer strategy A must respect D, regardless of whether the purpose of the reanalysis is to answer the original inquiry I or to answer a different inquiry \\(I'\\). Second, future researchers may replicate the design. Typically, replicators provide a new answer to the same I with new data, possibly improving elements of D and A along the way. If the inquiry of the replication is too different from the inquiry of the original study, the fidelity of the replication study may be compromised. Lastly, future researchers may meta-analyze study’s answer with other past studies. Meta-analysis is most meaningful when all of the included studies target a similar enough inquiry and when all studies rely on credible design.\nAll three of these activities depend on an accurate understanding of the study design. Reanalysts, replicators, and meta-analysts all need access to the study data and materials, of course. They also need to be sure of the critical design information in M, I, D, and A. Later in this section, we outline how archiving procedures that preserve study data and study design can enable new scientific purposes and describe strategies for doing each of these three particular integration tasks.\n\n\n\n\nThis section does not apply to private research, which unfortunately does not get “integrated”.↩︎"
  },
  {
    "objectID": "lifecycle/integration/communicating.html",
    "href": "lifecycle/integration/communicating.html",
    "title": "\n59  Communicating\n",
    "section": "",
    "text": "Too often, a casualty of translating the study from academic to other audiences is the design information. Emphasis gets placed on the study results, not on the reasons why the results of the study are to be believed. In sharing the research for nonspecialist audiences, we revert to saying that we think the findings are true and not why we think the findings are true. Explaining why requires explaining the research design, which in our view ought to be part of any public-facing communication about research.\nLooking at recent studies published in The New York Times Well section on health and fitness, we found that two dimensions of design quality were commonly ignored. First, experimental studies on new fitness regimens with very small samples, sometimes fewer than 10 units, are commonly highlighted. When both academic journals and reporters promote tiny studies, the likely result is that the published (and public) record contains many statistical flukes; results reflecting noise rather than new discoveries. Second, very large studies that draw observational comparisons between large samples of dieters and non-dieters with millions of observations receive outsize attention. These designs are prone to bias from confounding, but these concerns are too often not described or discussed.\nHow can we improve scientific communication so that we better communicate the credibility of findings? The market incentives for both journalists and authors reward striking and surprising findings, so any real solution to the problem would likely requires addressing those incentives. Short of that, we recommend that authors who wish to communicate the high quality of their designs to the media do so by providing the design information in M, I, D, and A in lay terms. Science communicators can state the research question (I) and explain why applying the data and answer strategies is likely to yield a good answer to the question. The actual result is, of course, also important to communicate, but why it is a credible answer to the research question is just as important to share—specifically what has to be believed about M for the results to be on target (Principle 3.6: Design to share).\nHow can we as researchers communicate about other scholars’ work? Citations can’t covey the entirety of MIDA in one sentence, but they can give an inkling. Here’s an example of how we could cite a (hypothetical) study in a way that conveys at least some design information. “Using a randomized experiment, the researchers (Authors, Year) found that donating to a campaign causes a large increase in the number of subsequent donation requests from other candidates, which is consistent with theories of party behavior that predict intra-party cooperation.”.\nThe citation explains that the data strategy included some kind of randomized experiment (we don’t know how many treatment arms or subjects, among other details), and that the answer strategy probably compared the counts of donation requests from any campaign (email requests, or phone, we don’t know) among the groups of subjects that were assigned to donate to a particular campaign. The citation mentions the models described in an unspecified area of the scientific literature on party politics, which all predict cooperation like the sharing of donor lists. We can reason that, if the inquiry, “Is the population average treatment effect effect of donating to one campaign on the number of donation requests from other campaigns positive?” were put to each of these theories, they would all respond “Yes.” The citation serves as a useful shorthand for the reader of what the claim of the paper is and why they should think it’s credible. By contrast, a citation like “The researchers found that party members cooperate (Author, Year).” doesn’t communicate any design information at all."
  },
  {
    "objectID": "lifecycle/integration/archiving.html",
    "href": "lifecycle/integration/archiving.html",
    "title": "\n60  Archiving\n",
    "section": "",
    "text": "What belongs in a replication archive? Enough documentation, data, and design detail that those who wish to reanalyze, replicate, and meta-analyze results can do so without contacting the authors.\nData. First, the realized data \\(d\\) itself. Sometimes this is the raw data. Sometimes it is only the “cleaned” data that is actually used by analysis scripts. Where ethically possible, we think it is preferable to post as much of the raw data as possible after removing information like IP addresses and geographic locations that could be used to identify subjects. The output of cleaning scripts – the cleaned data – should also be included in the replication archive.\nReanalyses often reexamine and extend studies by exploring the use of alternative outcomes, varying sets of control variables, and new ways of grouping data. As a result, replication data ideally includes all data collected by the authors even if the variables are not used in the final published results. Sometimes authors exclude these to preserve their own ability to publish on these other variables or because they are worried alternative analyses will cast doubt on their results. We hope norms will change such that study authors instead want to enable future researchers to build on their research by being expansive in what information is shared.\nAnalysis code. Replication archives also include the answer strategy A, or the set of functions that produce results when applied to the data. We need the actual analysis code because the natural-language descriptions of A that are typically given in written reports are imprecise. As a small example, many articles describe their answer strategies as “ordinary least squares” but do not fully describe the set of covariates included or the particular approach to variance estimation. These choices can substantively affect the quality of the research design – and nothing makes these choices explicit like the actual analysis code. Analysis code is needed not only for reanalysis but also replication and meta-analysis. Replication practice today involves inferring most of these details from descriptions in text. Reanalyses may directly reuse or modify analysis code and replication projects need to know the exact details of analyses to ensure they can implement the same analyses on the data they collect. Meta-analysis authors may take the estimates from the past studies directly, so understanding the exact analysis procedure conducted is important. Other times, meta-analyses reanalyze data to ensure comparability in estimation. Conducting analyses with and without covariates, with clustering when it was appropriate, or with a single statistical model when they vary across studies all require having the exact analysis code.\nTo the extent possible we encourage you to think of analysis code as being a data-in data-out function: a function that takes in your dataset—or a future replication dataset—implements analysis and reports a dataset containing answers: estimates and estimates of uncertainty.\nData strategy materials. Increasingly, replication archives include the materials needed to implement treatments and measurement strategies. Without the survey questionnaires in their original languages and formats, we cannot exactly replicate them in future studies, hindering our ability to build on and adapt them. The treatment stimuli used in the study should also be included. Data strategies are needed for reanalyses and meta-analyses too: answer strategies should respect data strategies, so understanding the details of sampling, treatment assignment, and measurement can shape reanalysts’ decisions and meta-analysis authors’ decisions about what studies to include and which estimates to synthesize.\nTo the extent possible we encourage you to describe data strategies also as data-in data-out function. Functions that take in information about a known context, and use this, together with parameters that characterize your strategy, return a dataset similar in structure to the data you generated.\nDesign declaration. While typical replication archives include the data and code, we think that future replication archives should also have a design declaration that fully describes M, I, D, and A. This declaration should be done in code and words. A diagnosis can also be included, demonstrating the properties as understood by the author and indicating the diagnosands that the author considered in judging the quality of the design.\nDesign details help future scholars not only assess, but replicate, reanalyze, and extend the study. Reanalysts need to understand the answer strategy to modify or extend it and the data strategy used to ensure that their new analysis respects the details of the sampling, treatment assignment, and measurement procedures. Data and analysis sharing enables reanalysts to adopt or adapt the analysis strategy, but a declaration of the data strategy would help more. The same is true of meta-analysis authors, who need to understand the designs’ details to make good decisions about which studies to include and how to analyze them. Replicators who wish to exactly replicate or even just provide an answer to the same inquiry need to understand the inquiry, data strategy, and answer strategy.\n?fig-filestructure below shows the file structure for an example replication. Our view on replication archives shares much in common with the TIER protocol and the proposals in Alvarez, Key, and Núñez (2018). It includes raw data in a platform-independent format (.csv) and cleaned data in a language-specific format (.rds, a format for R data files). Data features like labels, attributes, and factor levels are preserved when imported by the analysis scripts. The analysis scripts are labeled by the outputs they create, such as figures and tables. A master script is included that runs the cleaning and analysis scripts in the correct order. The documents folder consists of the paper, the supplemental appendix, the pre-analysis plan, the populated analysis plan, and codebooks that describe the data. A README file explains each part of the replication archive. We also suggest that authors include a script that consists of a design declaration and diagnosis. Bowers (2011) offers one reason above and beyond research transparency to go to all this effort: good archiving is like collaborating with your future self.\n\n\nFile structure for archiving\n\n\n\n\n\n\nAlvarez, R. Michael, Ellen M. Key, and Lucas Núñez. 2018. “Research Replication: Practical Considerations.” PS: Political Science & Politics 51 (2): 422–26.\n\n\nBowers, Jake. 2011. “Six Steps to a Better Relationship with Your Future Self.” The Political Methodologist 18 (2): 2–8.\n\n\nGabelica, Mirko, Ružica Bojčić, and Livia Puljak. 2022. “Many Researchers Were Not Compliant with Their Published Data Sharing Statement: Mixed-Methods Study.” Journal of Clinical Epidemiology.\n\n\nPeer, Limor, Lilla Orr, and Alexander Coppock. 2021. “Active Maintenance: A Proposal for the Long-term Computational Reproducibility of Scientific Results.” PS: Political Science & Politics 54 (3): 462–66."
  },
  {
    "objectID": "lifecycle/integration/reanalysis.html",
    "href": "lifecycle/integration/reanalysis.html",
    "title": "\n61  Reanalysis\n",
    "section": "",
    "text": "We can learn from reanalyses in several ways. First, we can fix errors in the original answer strategy. Reanalyses fix simple mathematical errors, typos in data transcription, or failures to account for features of the data strategy when analyzing the data. These reanalyses show whether the original results do or do not depend on these corrections. Second, we can reassess the study in light of new information about the world learned after the original study was published. That is, sometimes M changes in ways that color our interpretation of past results. Perhaps we learned about new confounders or alternative causal channels that undermine the original design’s credibility. When reanalyzed, demonstrating the results do (or do not) change when new model features are incorporated improves our understanding of the inquiry. Third, reanalyses may also aim to answer new questions that were not considered by the original study but for which the realized data can provide useful answers.\nLastly, many reanalyses show that original findings are not “robust” to alternative answer strategies. These are better conceptualized as claims about robustness to alternative models: one model may imply one answer strategy, and a different model, with another confounder, suggests another. If both models are plausible, a good answer strategy should be robust to both and even help distinguish between them. A reanalysis could uncover robustness to these alternative models or lack thereof.\nReanalyses are themselves research designs. Just like any design, whether a reanalysis is a strong research design depends on possible realizations of the data (as determined by the data strategy), not just the realized data. Because the realized data is fixed in a reanalysis, analysts are often instead tempted to judge the reanalysis based on whether it overturns or confirms the original study’s results. A successful reanalysis in this way of thinking demonstrates, by showing that the original results are changed under an alternative answer strategy, that the results are not robust to other plausible models.\nThis way of thinking can lead to incorrect assessments of reanalyses. Instead, we should consider what answers we would obtain under the original answer strategy A and the reanalysis strategy A’ under many possible realizations of the data. A good reanalysis strategy reveals with high probability the set of models of the world under which we can make credible claims about the inquiry. Whether or not the results change under the answer strategies A and A’ tells us little about this probability because the realized data is only one draw.\n\n61.0.1 Example\nIn this section, we illustrate the flaw in assessing reanalyses based on changing significance of results alone. We demonstrate how to assess the properties of reanalysis plans, comparing the properties of original answer strategies to proposed reanalysis answer strategies.\nThe design we consider is an observational study with a binary treatment \\(Z\\) that may or may not be confounded by a covariate \\(X\\). Suppose that the original researcher had in mind a model in which \\(Z\\) is not confounded by \\(X\\):\n\n# X is not a confounder and is measured pretreatment\nmodel_1 <- \n  declare_model(\n    N = 100,\n    U = rnorm(N),\n    X = rnorm(N),\n    Z = rbinom(N, 1, prob = plogis(0.5)),\n    potential_outcomes(Y ~ 0.1 * Z + 0.25 * X + U),\n    Y = reveal_outcomes(Y ~ Z)\n  ) \n\nThe reanalyst has in mind a different model. In this second model, \\(X\\) confounds the relationship between \\(Z\\) and \\(Y\\):\n\n# X is a confounder and is measured pretreatment\nmodel_2 <- \n  declare_model(\n    N = 100,\n    U = rnorm(N),\n    X = rnorm(N),\n    Z = rbinom(N, 1, prob = plogis(0.5 + X)),\n    potential_outcomes(Y ~ 0.1 * Z + 0.25 * X + U),\n    Y = reveal_outcomes(Y ~ Z)\n  ) \n\nThe original answer strategy A is a regression of the outcome \\(Y\\) on the treatment \\(Z\\). The reanalyst collects the covariate \\(X\\) and proposes to control for it in a linear regression; call that strategy A_prime.\n\nA <- declare_estimator(Y ~ Z, .method = lm_robust, label = \"A\")\nA_prime <- declare_estimator(Y ~ Z + X, .method = lm_robust, label = \"A_prime\")"
  },
  {
    "objectID": "lifecycle/integration/replication.html",
    "href": "lifecycle/integration/replication.html",
    "title": "\n62  Replication\n",
    "section": "",
    "text": "So-called “exact” replications hold key features of I, D, and A fixed, but draw a new dataset from the data strategy and apply the same answer strategy A to the new data to produce a fresh answer. Replications are said to “succeed” when the new and old answer are similar and to “fail” when they are not. Dichotomizing replication attempts into successes and failures is usually not that helpful, and it would be better to simply characterize how similar the old and new answers are. Literally exact replication is impossible: at least some elements of \\(m^*\\) have changed between the first study and the replication. Specifying how they might have changed, e.g., how outcomes vary with time, will help judge differences observed between old and new answers.\nReplication studies can benefit enormously from the knowledge gains produced by the original studies. For example, we learn a large amount to inform the construction of M and we learn the value of the inquiry from the original study. The M of the replication study can and should incorporate this new information. For example, if we learn from the original study that the estimand is positive, but it might be small, the replication study could respond by changing D to increase the sample size. Design diagnosis can help you learn about how to change the replication study’s design in light of the original research.\nWhen changes to the data strategy D or answer strategy A can be made to produce more informative answers about the same inquiry I, exact replication may not be preferred. Holding the treatment and outcomes the same may be required to provide an answer to the same I, but increasing the sample size or sampling individuals rather than villages or other changes may be preferable to exact replication. Replication designs can also take advantage of new best practices in research design.\nSo-called “conceptual” replications alter both M and D, but keep I and A as similar as possible. That is, a conceptual replication tries to ascertain whether a relationship in one context also holds in a new context. The trouble and promise of conceptual replications lie in the designer’s success at holding I constant. Too often, a conceptual replication fails because in changing M, too much changes about I, muddying the “concept” under replication.\nA summary function is needed to interpret the difference between the original answer and the replication answer. This function might take the new one and throw out the old if design was poor in the first. It might be taking the average. It might be a precision-weighted average. Specifying this function ex ante may be useful to avoid the choice of summary depending on the replication results. This summary function will be reflected in A and in the discussion section of the replication paper.\n\n62.0.1 Example\nHere we have an original study design of size 1000. The original study design’s true sample average treatment effect (SATE) is 0.2 because the original authors happened to study a very treatment-responsive population. We seek to replicate the original results, whatever they may be. We want to characterize the probability of concluding that we “failed” to replicate the original results. We have four alternative metrics for assessing replication failure.\n\nAre the original and replication estimates statistically significantly different from each other? If the difference-in-SATEs is significant, we conclude that we failed to replicate the original results, and if not, we conclude that the study replicated.\nIs the replication estimate within the original 95% confidence interval?\nIs the original estimate within the replication 95% confidence interval?\nDo we fail to affirm equivalence1 between the replication and original estimate, using a tolerance of 0.2?\n\n?fig-replications shows that no matter how big we make the replication, we find that the rate of concluding the difference-in-SATEs is nonzero only occurs about 10% of the time. Similarly, the replication estimate is rarely outside of the original confidence interval, because it’s rare to be more extreme than a wide confidence interval. The relatively high variance of the original study means that it is so uncertain, it’s tough to distinguish it from any number in particular.\nTurning to the third metric (is the original outside the 95% confidence interval of the replication estimate), we that we become more and more likely to conclude that the original study fails to replicate as the quality replication study goes up. At very large sample sizes, the replication confidence intervals become extremely small, so in the limit, it will always exclude the original study estimate.\nThe last metric, equivalence testing, has the nice property that as the sample size grows, we get closer to the correct answer – the true SATEs are indeed within 0.2 standard units of each other. However, again because the original study is so noisy, it is difficult to affirm its equivalence with anything, even when the replication study is quite large.\nThe upshot of this exercise is that, curiously, when original studies are weak (in that they generate imprecise estimates), it becomes harder to conclusively affirm that they did not replicate. This set of incentives is somewhat perverse: designers of original studies benefit from a lack of precision if it means they can’t “fail to replicate.”\n\n\nFigure 62.1: Rates of ‘failure to replicate’ according to four diagnosands. Original study N = 1000; True original SATE: 0.2; True replication SATE: 0.15.\n\n\n\n\n\n\n\nHartman, Erin, and F. Daniel Hidalgo. 2018. “An Equivalence Approach to Balance and Placebo Tests.” American Journal of Political Science 62 (4): 1000–1013.\n\n\n\n\nFor an introduction to equivalence testing see Hartman and Hidalgo (2018)↩︎"
  },
  {
    "objectID": "lifecycle/integration/meta-analysis.html",
    "href": "lifecycle/integration/meta-analysis.html",
    "title": "\n63  Meta-analysis\n",
    "section": "",
    "text": "Research synthesis takes two basic forms. The first is meta-analysis, in which a series of estimates are analyzed together in order to better understand features of the distribution of answers obtained in the literature (see Section (meta-analysis?)). Studies can be averaged together in ways that are better and worse. Sometimes the answers are averaged together according to their precision. A precision-weighted average gives more weight to precise estimates and less weight to studies that are noisy. Sometimes studies are “averaged” by counting up how many of the estimates are positive and significant, how many are negative and significant, and how many are null. This is the typical approach taken in a literature review. Regardless of the averaging approach, the goal of this kind of synthesis is to learn as much as possible about a particular inquiry I by drawing on evidence from many studies.\nA second kind of synthesis is an attempt to bring together the results of many designs, each of which targets a different inquiry about a common model. This is the kind of synthesis that takes place across an entire research literature. Different scholars focus on different nodes and edges of the common model, so a synthesis needs to incorporate the diverse sources of evidence. Such synthesis strategies include for example Bayesian model averaging approaches and stacking approaches (see e.g. Yao et al. (2018)).\nHow can you best anticipate how your research findings will be synthesized? For the first kind of synthesis—meta-analysis—you must be cognizant of keeping a commonly understood I in mind. You want to select inquiries not for their novelty, but because of their commonly-understood importance. While the specifics of the model M might differ from study to study, the fact that the Is are all similar enough to be synthesized allows for a specific kind of knowledge accumulation.\nFor the second kind of synthesis—literature-wide progress on a full causal model—even greater care is required. Specific studies cannot make up bespoke models M but instead must understand how the specific M adopted in the study is a special case of some broader M that is in principle agreed to by a wider research community. Perhaps in this spirit, Samii (2016) sets role of “causal empiricists” apart from the role of theorists. The nonstop, never-ending proliferation of study-specific theories is a threat to this kind of knowledge accumulation. In a telling piece, McPhetres et al. (2020) document that in a decade of research articles published in Psychological Science, 359 specific theories were named. Of these 70% were named just once and a further 12% were named just twice.\nDesign then with a view to integration. In the ideal case the meta-analytic design will already exist and your job will be to design a new study that can demonstrably add value to the collective design.\n\n\n\n\nMcPhetres, Jonathon, Nihan Albayrak-Aydemir, Ana Barbosa Mendes, Elvina C. Chow, Patricio Gonzalez-Marquez, Erin Loukras, Annika Maus, et al. 2020. “A Decade of Theory as Reflected in Psychological Science (2009-2019).”\n\n\nSamii, Cyrus. 2016. “Causal Empiricism in Quantitative Research.” The Journal of Politics 78 (3): 941–55.\n\n\nYao, Yuling, Aki Vehtari, Daniel Simpson, and Andrew Gelman. 2018. “Using Stacking to Average Bayesian Predictive Distributions (with Discussion).” Bayesian Analysis 13 (3): 917–1007."
  },
  {
    "objectID": "epilogue.html",
    "href": "epilogue.html",
    "title": "64  Epilogue",
    "section": "",
    "text": "The first prong of the response to the upheaval was the so-called “credibility revolution” which sought new research designs that could deliver credible evidence to answer social scientific questions. Randomized experiments rapidly rose in popularity in economics and political science. Observational causal inference methods such as regression discontinuities and difference-in-differences designs have become popular in sociology, political science, and economics. And sample sizes have increased and new populations outside university students have been explored in psychology and experimental economics.\nThe second prong was the so-called “open science movement.” Open science practices are motivated by the idea that even when a credible general design for drawing inferences is adopted, myriad small design decisions may influence the validity of the results. Sharing the plans, computer code, and materials used to implement the research as well as the data that result allows peer reviewers and readers to assess the large and small decisions the authors made and come to a their own judgment about what was learned. Preregistration of plans before implementing research provides additional clarity: which of these decisions were made before seeing data and results and which were made after.\nThe two prongs are closely related to each other. Open science practices are meant to reinforce the work on credible designs: transparency of research methods incentivizes researchers to select credible research designs in the first place. Common to both is the idea that research design matters.\nStrikingly, however, these advances have been made without a clear common understanding of what a design is or how to evaluate one. In this book, we provided a flexible approach to defining a design and a procedure for assessing its qualities. We identified four generic elements of a research design: the Model, the Inquiry, the Data Strategy and the Answer strategy. “Declaring” these four elements makes it easier to communicate the most important analytic features of a design, enabling “diagnosis” the credibility of claims that depend on them.\nWe hope our effort adds two new steps to the workflow promoted by credibility and transparency revolutionaries. First, we want scholars to develop designs by declaring them in code, diagnosing their properties in terms of scientific, logistical, and ethical goals, and redesigning across feasible designs to select the final design. Second, we want scholars to share their designs so they can be more easily understood, more easily interrogated, and more easily built upon.\nWe see these steps as deeply complementary to the credibility revolution and the open science movement.\nDeclaring and diagnosing designs can make designs stronger. Many design choices can be made on the basis of analytic results, and these should be used when possible. But oftentimes analytic results provide incomplete answers. Sampling and eligibility procedures can interact with treatment allocation schemes, so causal identification results can be insufficient to assess the unbiasedness of the design for a sample average treatment effect. Moreover, many theoretical results about research design are conditional on certain sample sizes, correlations between variables, or the correctness of functional forms. Assessing how designs perform based on the specific research setting and its sample size and empirical correlations between variables augments the general theoretical guidance. Of course, the theoretical results guide how to set up the design itself: identifying what kinds of problems can emerge in a model is an exercise shaped by theoretical results.\nSharing research designs in code complements common open science practices in use today. By providing the design in code, the study can be replicated exactly in a new setting or a later time period, reanalyzed with the realized data but new estimators, and the diagnosands reassessed on the authors’ original terms and under new conjectures about the model. Declarations also complement current practices in preregistration. Considerable debates surrounds what should be included in a preanalysis plan. Declarations in code provide an answer: you should declare sufficient information to enable to diagnose the design in terms of study-relevant diagnosands.\nBetter software tools will come along to declare, diagnose, and redesign studies in code. A body of domain-specific knowledge will develop about what models design must be assessed against to assure robustness. What we hope will remain is the idea that research designs can be thought of as objects that can be interrogated, defined by the specific steps in the procedures used to generate data and analyze it to provide answers to specific inquiries that are themselves well defined with respect to specified representations of the world. We hope that these ideas and tools will enable scholars to better respond to changed incentives in social sciences to adopt credible research designs for the questions they are asking and to communicate that they have done so to reviewers and readers."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Abadie, Alberto, Susan Athey, Guido W. Imbens, and Jeffrey Wooldridge.\n2017. “When Should You Adjust Standard Errors for\nClustering?” NBER Working Paper 24003.\n\n\nAbell, Peter, and Ofer Engel. 2021. “Subjective Causality and\nCounterfactuals in the Social Sciences: Toward an Ethnographic\nCausality?” Sociological Methods & Research 50 (4):\n1842–62.\n\n\nAguilar, Rosario, Saul Cunow, and Scott Desposato. 2015. “Choice\nSets, Gender, and Candidate Choice in Brazil.” Electoral\nStudies 39: 230–42. https://doi.org/https://doi.org/10.1016/j.electstud.2015.03.011.\n\n\nAlrababa’h, Ala’, Scott Williamson, Andrea Dillon, Jens Hainmueller,\nDominik Hangartner, Michael Hotard, David Laitin, Duncan Lawrence, and\nJeremy Weinstein. 2022. “Learning from Null Effects: A Bottom-up\nApproach.” Political Analysis.\n\n\nAlvarez, R. Michael, Ellen M. Key, and Lucas Núñez. 2018.\n“Research Replication: Practical\nConsiderations.” PS: Political Science\n& Politics 51 (2): 422–26.\n\n\nAngrist, Joshua D., and Jörn-Steffen Pischke. 2008. Mostly Harmless\nEconometrics: An Empiricist’s Companion. Princeton: Princeton\nUniversity Press.\n\n\nAronow, Peter M., Jonathon Baron, and Lauren Pinson. 2019. “A Note\non Dropping Experimental Subjects Who Fail a Manipulation Check.”\nPolitical Analysis 27 (4): 572–89.\n\n\nAronow, Peter M., Donald P. Green, and Donald K. K. Lee. 2014.\n“Sharp Bounds On The Variance In Randomized\nExperiments.” The Annals of Statistics 42 (3):\n850–71.\n\n\nAronow, Peter M., and Benjamin T. Miller. 2019. Foundations of\nAgnostic Statistics. Cambridge, UK: Cambridge University Press.\n\n\nAronow, Peter M., and Cyrus Samii. 2017. “Estimating Average\nCausal Effects Under General Interference, with Application to a Social\nNetwork Experiment.” The Annals of Applied Statistics 11\n(4): 1912–47.\n\n\nAvdeenko, Alexandra, and Michael J. Gilligan. 2015. “International\nInterventions to Build Social Capital: Evidence from a Field Experiment\nin Sudan.” American Political Science Review 109 (3):\n427–49.\n\n\nBai, Yuehao. 2021. “Why Randomize? Minimax Optimality Under\nPermutation Invariance.” Journal of Econometrics.\n\n\nBaird, Sarah, J. Aislinn Bohren, Craig McIntosh, and Berk Ozler. 2018.\n“Optimal Design of Experiments in the\nPresence of Interference.” Review of Economics\n& Statistics 5 (100): 844–60.\n\n\nBalcells, Laia, Valeria Palanza, and Elsa Voytas. 2022. “Do\nTransitional Justice Museums Persuade Visitors? Evidence from a Field\nExperiment.” The Journal of Politics 84 (1).\n\n\nBanerjee, Abhijit, Esther Duflo, Amy Finkelstein, Lawrence F Katz,\nBenjamin A Olken, and Anja Sautmann. 2020. “In Praise of\nModeration: Suggestions for the Scope and Use of Pre-Analysis Plans for\nRCTs in Economics.” Working Paper 26993. Working Paper Series.\nNational Bureau of Economic Research. https://doi.org/10.3386/w26993.\n\n\nBansak, Kirk, Jens Hainmueller, Daniel J. Hopkins, and Teppei Yamamoto.\n2021. “Beyond the Breaking Point? Survey Satisficing in Conjoint\nExperiments.” Political Science Research and Methods 9\n(1): 53–71.\n\n\nBaron, Hannah, and Lauren E. Young. 2021. “From Principles to\nPractice: Methods to Increase the Transparency of Research Ethics in\nViolent Contexts.” Political Science Research and\nMethods, 1–8.\n\n\nBateson, Regina. 2012. “Crime Victimization and Political\nParticipation.” American Political Science Review 106\n(3): 570–87.\n\n\nBauer, Paul C., and Bernhard Clemm von Hohenberg. 2021. “Believing\nand Sharing Information by Fake Sources: An Experiment.”\nPolitical Communication 38 (6): 647–71.\n\n\nBenjamin, Daniel J., James O. Berger, Magnus Johannesson, Brian A.\nNosek, E. -J. Wagenmakers, Richard Berk, Kenneth A. Bollen, et al. 2018.\n“Redefine Statistical Significance.” Nature Human\nBehaviour 2 (1): 6–10.\n\n\nBennett, Andrew. 2015. “Appendix.” In Process Tracing:\nFrom Metaphor to Analytic Tool, edited by Andrew Bennett and\nJeffrey Checkel. New York: Cambridge University Press.\n\n\nBennett, Andrew, and Jeffrey Checkel. 2015. “Process Tracing: From\nPhilosophical Roots to Best Practices.” In Process Tracing:\nFrom Metaphor to Analytic Tool, edited by Andrew Bennett and\nJeffrey Checkel, 3–37. New York: Cambridge University Press.\n\n\nBirkelund, Gunn Elisabeth, Bram Lancee, Edvard Nergård Larsen, Javier G\nPolavieja, Jonas Radl, and Ruta Yemane. 2022. “Gender\nDiscrimination in Hiring: Evidence from a Cross-National Harmonized\nField Experiment.” European Sociological Review 38 (3):\n337–54.\n\n\nBisbee, James. 2019. “BARP: Improving Mister p Using Bayesian\nAdditive Regression Trees.” American Political Science\nReview 113 (4): 1060–65.\n\n\nBlair, Graeme, Darin Christensen, and Aaron Rudkin. 2021. “Do\nCommodity Price Shocks Cause Armed Conflict? A Meta-Analysis of Natural\nExperiments.” American Political Science Review 115 (2):\n709–16.\n\n\nBlair, Graeme, Jasper Cooper, Alexander Coppock, and Macartan Humphreys.\n2019. “Declaring and Diagnosing Research Designs.”\nAmerican Political Science Review 113 (3): 838–59.\n\n\nBlair, Graeme, Alexander Coppock, and Margaret Moor. 2020. “When\nto Worry about Sensitivity Bias: A Social Reference Theory and Evidence\nfrom 30 Years of List Experiments.” American Political\nScience Review 114 (4): 1297–1315.\n\n\nBlair, Robert A., Manuel Moscoso-Rojas, Andres Vargas Castillo, and\nMichael Weintraub. 2022. “Preventing Rebel Resurgence After Civil\nWar: A Field Experiment in Security and Justice Provision in Rural\nColombia.” American Political Science Review, 1–20.\n\n\nBonilla, Tabitha, and Alvin B. Tillery. 2020. “Which Identity\nFrames Boost Support for and Mobilization in the #BlackLivesMatter\nMovement? An Experimental Test.” American Political Science\nReview 114 (4): 947–62.\n\n\nBorenstein, Michael, Larry V Hedges, Julian PT Higgins, and Hannah R\nRothstein. 2021. Introduction to Meta-Analysis. John Wiley\n& Sons.\n\n\nBowers, Jake. 2011. “Six Steps to a Better Relationship with Your\nFuture Self.” The Political Methodologist 18 (2): 2–8.\n\n\nBradley, Valerie C., Shiro Kuriwaki, Michael Isakov, Dino Sejdinovic,\nXiao-Li Meng, and Seth Flaxman. 2021. “Unrepresentative Big\nSurveys Significantly Overestimated US Vaccine Uptake.”\nNature 600 (7890): 695–700.\n\n\nBrady, Henry E. 2004. “Data-Set Observations Versus Causal-Process\nObservations: The 2000 US Presidential Election.” In\nRethinking Social Inquiry: Diverse Tools, Shared Standards,\n267–72. Lanham: Rowman & Littlefield.\n\n\nBroockman, David E., Joshua L. Kalla, and Jasjeet S. Sekhon. 2017.\n“The Design of Field Experiments with Survey Outcomes: A Framework\nfor Selecting More Efficient, Robust, and Ethical Designs.”\nPolitical Analysis 25 (4): 435–64.\n\n\nBroockman, David, and Joshua Kalla. 2016. “Durably Reducing\nTransphobia: A Field Experiment on Door-to-Door Canvassing.”\nScience 352 (6282): 220–24.\n\n\nButler, Daniel M., and Charles Crabtree. 2017. “Moving Beyond\nMeasurement: Adapting Audit Studies to Test Bias-Reducing\nInterventions.” Journal of Experimental Political\nScience 4 (1): 57–67.\n\n\nCalonico, Sebastian, Matias D Cattaneo, and Rocio Titiunik. 2014.\n“Robust Nonparametric Confidence Intervals for\nRegression-Discontinuity Designs.” Econometrica 82 (6):\n2295–2326.\n\n\nCarnegie, Allison, and Cyrus Samii. 2019. “International\nInstitutions and Political Liberalization: Evidence from the World Bank\nLoans Program.” British Journal of Political Science 49\n(4): 1357–79.\n\n\nCarreri, Maria, and Oeindrila Dube. 2017. “Do Natural Resources\nInfluence Who Comes to Power, and How?” The Journal of\nPolitics 79 (2): 502–18.\n\n\nCasey, Katherine, Rachel Glennerster, and Edward Miguel. 2012.\n“Reshaping Institutions: Evidence on Aid Impacts Using a\nPre-Analysis Plan.” The Quarterly Journal of Economics\n127 (4): 1755–1812.\n\n\nChaisemartin, Clément de, and Xavier d’Haultfoeuille. 2020.\n“Two-Way Fixed Effects Estimators with Heterogeneous Treatment\nEffects.” American Economic Review 110 (9): 2964–96.\n\n\nCheema, Ali, Sarah Khan, Asad Liaqat, and Shandana Khan Mohmand. 2022.\n“Canvassing the Gatekeepers: A Field Experiment to Increase Women\nVoters’ Turnout in Pakistan.” American Political Science\nReview, 1–21.\n\n\nChopra, Felix, Ingar Haaland, Christopher Roth, and Andreas Stegmann.\n2022. “The Null Result Penalty.” Unpublished\nManuscript.\n\n\nClingingsmith, David, Asim Ijaz Khwaja, and Michael Kremer. 2009.\n“Estimating the Impact of the Hajj: Religion and Tolerance in\nIslam’s Global Gathering.” The Quarterly Journal of\nEconomics 124 (3): 1133–70.\n\n\nCollier, David, David A Freedman, James D Fearon, David D Laitin, John\nGerring, and Gary Goertz. 2008. “Symposium: Case Selection, Case\nStudies, and Causal Inference.” Qualitative & Multimethod\nResearch 6 (2): 2–16.\n\n\nCollins, Jonathan E. 2021. “Does the Meeting Style Matter? The\nEffects of Exposure to Participatory and Deliberative School Board\nMeetings.” American Political Science Review 115 (3):\n790–804.\n\n\nCoppock, Alexander. 2017. “Did Shy Trump Supporters Bias the 2016\nPolls? Evidence from a Nationally-Representative List\nExperiment.” Statistics, Politics and Policy 8 (1):\n29–40.\n\n\n———. 2019. “Avoiding Post-Treatment Bias in\nAudit Experiments.” Journal of Experimental Political\nScience 6 (1): 1–4.\n\n\nCoppock, Alexander, Alan S. Gerber, Donald P. Green, and Holger L. Kern.\n2017. “Combining Double Sampling and Bounds to Address\nNonignorable Missing Outcomes in Randomized Experiments.”\nPolitical Analysis 25 (2): 188–206.\n\n\nCoppock, Alexander, and Dipin Kaur. 2022. “Qualitative Imputation of Missing Potential\nOutcomes.” American Journal of Political Science.\n\n\nCreighton, Mathew J., and Amaney Jamal. 2015. “Does Islam Play a\nRole in Anti-Immigrant Sentiment? An Experimental Approach.”\nSocial Science Research 53: 89–103.\n\n\nCruz, Cesi. 2019. “Social Networks and the Targeting of Vote\nBuying.” Comparative Political Studies 52 (3): 382–411.\n\n\nDawid, Philip, Macartan Humphreys, and Monica Musio. 2022.\n“Bounding Causes of Effects with Mediators.”\nSociological Methods & Research.\n\n\nDeaton, Angus S. 2010. “Instruments, Randomization, and Learning\nabout Development.” Journal of Economic Literature 48\n(2): 424–55.\n\n\nDunning, Thad. 2012. Natural Experiments in the Social Sciences: A\nDesign-Based Approach. Cambridge, UK: Cambridge University Press.\n\n\nDunning, Thad, Guy Grossman, Macartan Humphreys, Susan D. Hyde, Craig\nMcIntosh, Gareth Nellis, Claire L. Adida, et al. 2019. “Voter\nInformation Campaigns and Political Accountability: Cumulative Findings\nfrom a Preregistered Meta-Analysis of Coordinated Trials.”\nScience Advances 5 (7): 1–10.\n\n\nEgami, Naoki, and Erin Hartman. 2022. “Elements of External\nValidity: Framework, Design, and Analysis.” American\nPolitical Science Review.\n\n\nEgger, Dennis, Johannes Haushofer, Edward Miguel, Paul Niehaus, and\nMichael W Walker. 2019. “General Equilibrium Effects of Cash\nTransfers: Experimental Evidence from Kenya.” Working Paper\n26600. National Bureau of Economic Research.\n\n\nFairfield, Tasha, and Andrew E Charman. 2017. “Explicit Bayesian\nAnalysis for Process Tracing: Guidelines, Opportunities, and\nCaveats.” Political Analysis 25 (3): 363–80.\n\n\nFang, Albert H, Andrew M Guess, and Macartan Humphreys. 2019. “Can\nthe Government Deter Discrimination? Evidence from a Randomized\nIntervention in New York City.” The Journal of Politics\n81 (1): 127–41.\n\n\nFearon, James, and David Laitin. 2008. “Integrating Qualitative\nand Quantitative Methods.” In Oxford Handbook of Political\nMethodology, edited by Janet M. Box-Steffenmeier, David Collier,\nand Henry E Brady, 756–76. London, England: Oxford University Press.\n\n\nFenno, Richard F. 1978. Home Style: House Members in Their\nDistricts. New York: Longman.\n\n\nFoos, Florian, Peter John, Christian Müller, and Kevin Cunningham. 2021.\n“Social Mobilization in Partisan Spaces.” The Journal\nof Politics 83 (3): 1190–97.\n\n\nFrancois, Patrick, Ilia Rainer, and Francesco Trebbi. 2015. “How\nIs Power Shared in Africa?” Econometrica 83 (2):\n465–503.\n\n\nFrangakis, Constantine E., and Donald B Rubin. 2002. “Principal\nStratification in Causal Inference.” Biometrics 58 (1):\n21–29.\n\n\nFrederiksen, Kristian Vrede Skaaning. 2022. “Does Competence Make\nCitizens Tolerate Undemocratic Behavior?” American Political\nScience Review, 1–7.\n\n\nFreedman, David A. 2008. “On Regression Adjustments to\nExperimental Data.” Advances in Applied Mathematics 40\n(2): 180–93.\n\n\nFrey, Anderson, Gabriel López-Moctezuma, and Sergio Montero. 2022.\n“Sleeping with the Enemy: Effective Representation Under Dynamic\nElectoral Competition.” American Journal of Political\nScience.\n\n\nGabelica, Mirko, Ružica Bojčić, and Livia Puljak. 2022. “Many\nResearchers Were Not Compliant with Their Published Data Sharing\nStatement: Mixed-Methods Study.” Journal of Clinical\nEpidemiology.\n\n\nGalos, Diana, and Alexander Coppock. 2022. “What Have Employment\nAudits Taught Us about the Effects of Gender on Hiring? A\nMeta-Analyis.” Unpublished Manuscript.\n\n\nGeddes, Barbara. 2003. Paradigms and Sand Castles: Theory Building\nand Research Design in Comparative Politics. Ann Arbor, Michigan:\nUniversity of Michigan Press.\n\n\nGelman, Andrew, and John Carlin. 2014. “Beyond Power Calculations\nAssessing Type s (Sign) and Type m (Magnitude) Errors.”\nPerspectives on Psychological Science 9 (6): 641–51.\n\n\nGerber, Alan S., James G. Gimpel, Donald P. Green, and Daron R. Shaw.\n2011. “How Large and Long-Lasting Are the Persuasive Effects of\nTelevised Campaign Ads? Results from a Randomized Field\nExperiment.” American Political Science Review 105 (1):\n135–50.\n\n\nGerber, Alan S., and Donald P. Green. 2012. Field Experiments: Design, Analysis, and\nInterpretation. New York: W.W. Norton.\n\n\nGerring, John, and Lee Cojocaru. 2016. “Selecting Cases for\nIntensive Analysis: A Diversity of Goals and Methods.”\nSociological Methods & Research 45 (3): 392–423.\n\n\nGoertz, Gary. 2008. “Choosing Cases for Case Studies: A\nQualitative Logic.” Newsletter of the APSA Section on\nQualitative & Multi-Method Research 6 (2): 11–14.\n\n\nGoertz, Gary, and James Mahoney. 2012. A Tale of Two Cultures:\nQualitative and Quantitative Research in the Social Sciences.\nPrinceton: Princeton University Press.\n\n\nGreen, Donald P., Jonathan S. Krasno, Alexander Coppock, Benjamin D.\nFarrer, Brandon Lenoir, and Joshua N. Zingher. 2016. “The Effects\nof Lawn Signs on Vote Outcomes: Results from Four Randomized Field\nExperiments.” Electoral Studies 41: 143–50.\n\n\nGreen, Donald P., and Winston Lin. 2016. “Standard Operating\nProcedures: A Safety Net for Pre-Analysis Plans.” PS:\nPolitical Science & Politics 49 (3): 495–99.\n\n\nGreen, Donald P., and Andrej Tusicisny. 2012. “Statistical\nAnalysis of Results from Laboratory Studies in Experimental Economics: A\nCritique of Current Practice.” Available at SSRN\n2181654.\n\n\nGreen, Jon, James N. Druckman, Matthew A. Baum, David Lazer, Katherine\nOgnyanova, Matthew Simonson, Jennifer Lin, Mauricio Santillana, and Roy\nH. Perlis. 2022. “Using General Messages to Persuade on a\nPoliticized Scientific Issue.” British Journal of Political\nScience.\n\n\nGulzar, Saad, and Muhammad Yasir Khan. 2021. “‘Good\nPoliticians:’ Experimental Evidence on Motivations for Political\nCandidacy and Government Performance.”\n\n\nHainmueller, Jens, Daniel J. Hopkins, and Teppei Yamamoto. 2014.\n“Causal Inference in Conjoint Analysis:\nUnderstanding Multidimensional Choices via Stated Preference\nExperiments.” Political Analysis 22 (1): 1–30.\n\n\nHalpern, Joseph Y. 2000. “Axiomatizing Causal Reasoning.”\nJournal of Artificial Intelligence Research 12: 317–37.\n\n\nHartman, Erin, and F. Daniel Hidalgo. 2018. “An Equivalence\nApproach to Balance and Placebo Tests.” American Journal of\nPolitical Science 62 (4): 1000–1013.\n\n\nHedayat, A. S., Hansheng Cheng, and Jennifer Pajda-De La O. 2019.\n“Existence of Unbiased Estimation for the Minimum, Maximum, and\nMedian in Finite Population Sampling.” Statistics &\nProbability Letters 153: 192–95.\n\n\nHerron, Michael C., and Kevin M. Quinn. 2016. “A Careful Look at\nModern Case Selection Methods.” Sociological Methods &\nResearch 45 (3): 458–92.\n\n\nHumphreys, Macartan. 2015. “Reflections on the Ethics of Social\nExperimentation.” Journal of Globalization and\nDevelopment 6 (1): 87–112.\n\n\nHumphreys, Macartan, and Alan Jacobs. forthcoming. Integrated\nInferences. Cambridge: Cambridge University Press.\n\n\nHumphreys, Macartan, and Alan M. Jacobs. 2015. “Mixing Methods: A\nBayesian Approach.” American Political Science Review\n109 (4): 653–73.\n\n\nImai, Kosuke. 2011. “Multivariate Regression\nAnalysis for the Item Count Technique.” Journal of the\nAmerican Statistical Association 106 (494): 407–16.\n\n\nImai, Kosuke, Gary King, and Clayton Nall. 2009. “The Essential\nRole of Pair Matching in Cluster-Randomized Experiments, with\nApplication to the Mexican Universal Health Insurance\nEvaluation.” Statistical Science 24 (1): 29–53.\n\n\nImai, Kosuke, Gary King, and Elizabeth A. Stuart. 2008.\n“Misunderstandings Between Experimentalists and Observationalists\nabout Causal Inference.” Journal of the Royal Statistical\nSociety: Series A (Statistics in Society) 171 (2): 481–502.\n\n\nImbens, Guido W. 2010. “Better LATE Than Nothing: Some Comments on\nDeaton (2009) and Heckman and Urzua (2009).” Journal of\nEconomic Literature 48 (2): 399–423.\n\n\nImbens, Guido W., and Donald B. Rubin. 2015. Causal Inference in\nStatistics, Social, and Biomedical Sciences. Cambridge: Cambridge\nUniversity Press.\n\n\nIyengar, Shanto, and Sean J. Westwood. 2015. “Fear and Loathing\nAcross Party Lines: New Evidence on Group Polarization.”\nAmerican Journal of Political Science 59 (3): 690–707.\n\n\nJamison, Julian C. 2019. “The Entry of Randomized Assignment into\nthe Social Sciences.” Journal of Causal Inference 7 (1):\n1–16.\n\n\nJefferson, Hakeem. 2022. “Respectability and the Politics of\nPunishment Among Black Americans.” Unpublished\nManuscript.\n\n\nJohnson, Noel D, and Alexandra A Mislin. 2011. “Trust Games: A\nMeta-Analysis.” Journal of Economic Psychology 32 (5):\n865–89.\n\n\nKalla, Joshua, Frances Rosenbluth, and Dawn Langan Teele. 2018.\n“Are You My Mentor? A Field Experiment on Gender, Ethnicity, and\nPolitical Self-Starters.” The Journal of Politics 80\n(1): 337–41.\n\n\nKao, Kristen, and Mara R. Revkin. 2022. “Retribution or\nReconciliation? Post-Conflict Attitudes Toward Enemy\nCollaborators.” American Journal of Political Science.\n\n\nKarpowitz, Christopher F., J. Quin Monson, and Jessica Robinson Preece.\n2017. “How to Elect More Women: Gender and Candidate Success in a\nField Experiment.” American Journal of Political Science\n61 (4): 927–43.\n\n\nKasy, Maximilian. 2016. “Why Experimenters Might Not Always Want\nto Randomize, and What They Could Do Instead.” Political\nAnalysis 24 (3): 324–38.\n\n\nKing, Gary. 1998. Unifying Political Methodology: The Likelihood\nTheory of Statistical Inference. Ann Arbor, Michigan: University of\nMichigan Press.\n\n\nKing, Gary, Emmanuela Gakidou, Nirmala Ravishankar, Ryan T Moore, Jason\nLakin, Manett Vargas, Martha Marı́a Téllez-Rojo, Juan Eugenio Hernández\nÁvila, Mauricio Hernández Ávila, and Héctor Hernández Llamas. 2007.\n“A ‘Politically Robust’ Experimental Design for\nPublic Policy Evaluation, with Application to the Mexican Universal\nHealth Insurance Program.” Journal of Policy Analysis and\nManagement 26 (3): 479–506.\n\n\nKing, Gary, and Melissa Sands. 2015. “How Human Subjects Research\nRules Mislead You and Your University, and What to Do about It.”\nUnpublished Manuscript.\n\n\nKirkland, Patricia A. 2021. “Business Owners and Executives as\nPoliticians: The Effect on Public Policy.” The Journal of\nPolitics 83 (4): 1652–68.\n\n\nKlar, Samara, and Thomas J Leeper. 2019. “Identities and\nIntersectionality: A Case for Purposive Sampling in Survey-Experimental\nResearch.” Experimental Methods in Survey Research:\nTechniques That Combine Random Sampling with Random Assignment,\n419–33.\n\n\nKling, Jeffrey R, Jeffrey B Liebman, and Lawrence F Katz. 2007.\n“Experimental Analysis of Neighborhood Effects.”\nEconometrica 75 (1): 83–119.\n\n\nLax, Jeffrey R., and Justin H. Phillips. 2009. “Gay Rights in the\nStates: Public Opinion and Policy Responsiveness.” American\nPolitical Science Review 103 (3): 367–86.\n\n\nLevine, Adam Seth. 2021. “How to Form Organizational Partnerships\nto Run Experiments.” In Advances in Experimental Political\nScience, 199–216. Cambridge, UK: Cambridge University Press.\n\n\nLevy, Jack S. 2008. “Case Studies: Types, Designs, and Logics of\nInference.” Conflict Management and Peace Science 25\n(1): 1–18.\n\n\nLieberman, Evan S. 2005. “Nested Analysis as a Mixed-Method\nStrategy for Comparative Research.” American Political\nScience Review 99 (3): 435–52.\n\n\nLin, Winston. 2013. “Agnostic Notes on Regression Adjustments to\nExperimental Data: Reexamining Freedman’s Critique.” Annals\nof Applied Statistics 7 (1): 295–318.\n\n\nLyall, Jason. 2022. “Preregister Your Ethical Redlines: Vulnerable\nPopulations, Policy Engagement, and the Perils of e-Hacking.”\nUnpublished Manuscript.\n\n\nLyall, Jason, Yang-Yang Zhou, and Kosuke Imai. 2020. “Can Economic\nAssistance Shape Combatant Support in Wartime? Experimental Evidence\nfrom Afghanistan.” American Political Science Review 114\n(1): 126–43.\n\n\nMartin, Lisa. 1992. Coercive Cooperation: Explaining\nMultilateral Economic Sanctions. Princeton: Princeton\nUniversity Press.\n\n\nMcPhetres, Jonathon, Nihan Albayrak-Aydemir, Ana Barbosa Mendes, Elvina\nC. Chow, Patricio Gonzalez-Marquez, Erin Loukras, Annika Maus, et al.\n2020. “A Decade of Theory as Reflected in Psychological Science\n(2009-2019).”\n\n\nMellon, Jonathan. 2021. “Rain, Rain, Go Away: 176 Potential\nExclusion-Restriction Violations for Studies Using Weather as an\nInstrumental Variable.” Unpublished Manuscript.\n\n\nMerkley, Eric, and Dominik A. Stecula. 2021. “Party Cues in the\nNews: Democratic Elites, Republican Backlash, and the Dynamics of\nClimate Skepticism.” British Journal of Political\nScience 51 (4): 1439–56.\n\n\nMiddleton, Joel A. 2008. “Bias of the Regression Estimator for\nExperiments Using Clustered Random Assignment.” Statistics\n& Probability Letters 78 (16): 2654–59.\n\n\nMill, John Stuart. 1869. A System of Logic, Ratiocinative and\nInductive: Being a Connected View of the Principles of Evidence and the\nMethods of Scientific Investigation. New York: Harper &\nBrothers.\n\n\nMiller, Judith Droitcour. 1984. “A New Survey Technique for\nStudying Deviant Behavior.” PhD thesis, George Washington\nUniversity.\n\n\nMontgomery, Jacob M., and Erin L. Rossiter. 2020. “So Many\nQuestions, so Little Time: Integrating Adaptive Inventories into Public\nOpinion Research.” Journal of Survey Statistics and\nMethodology 8 (4): 667–90.\n\n\nMorris, Tim P., Ian R. White, and Michael J. Crowther. 2021.\n“Using Simulation Studies to Evaluate Statistical Methods.”\nStatistics in Medicine 38 (11): 2074–2102.\n\n\nMousa, Salma. 2020. “Building Social Cohesion Between Christians\nand Muslims Through Soccer in Post-ISIS Iraq.” Science\n369 (6505): 866–70.\n\n\nNellis, Gareth, and Niloufer Siddiqui. 2018. “Secular Party Rule\nand Religious Violence in Pakistan.” American Political\nScience Review 112 (1): 49–67.\n\n\nOffer-Westort, Molly, Alexander Coppock, and Donald P. Green. 2021.\n“Adaptive Experimental Design: Prospects and\nApplications in Political Science.” American Journal\nof Political Science 65: 826–44.\n\n\nOfosu, George, and Daniel Posner. 2022. “Pre-analysis Plans: An Early Stocktaking.”\nPerspectives on Politics.\n\n\nPaglayan, Agustina S. 2019. “Public-Sector Unions and the Size of\nGovernment.” American Journal of Political Science 63\n(1): 21–36.\n\n\nPaler, Laura, Leslie Marshall, and Sami Atallah. 2018. “The Social\nCosts of Public Political Participation: Evidence from a Petition\nExperiment in Lebanon.” The Journal of Politics 80 (4):\n1405–10.\n\n\nPaluck, Elizabeth Levy, and Donald P. Green. 2009. “Deference,\nDissent, and Dispute Resolution: An Experimental Intervention Using Mass\nMedia to Change Norms and Behavior in Rwanda.” American\nPolitical Science Review 103 (4): 622–44.\n\n\nPaluck, Elizabeth Levy, Seth A. Green, and Donald P. Green. 2019.\n“The Contact Hypothesis Re-Evaluated.” Behavioural\nPublic Policy 3 (2): 129–58.\n\n\nPearl, Judea. 1999. “Probabilities of Causation: Three\nCounterfactual Interpretations and Their Identification.”\nSynthese 121 (1-2): 93–149.\n\n\n———. 2009. Causality: Models, Reasoning and Inference. Second\nEdition. Cambridge: Cambridge University Press.\n\n\nPearl, Judea, and Dana Mackenzie. 2018. The Book of Why: The New\nScience of Cause and Effect. New York: Basic Books.\n\n\nPeer, Limor, Lilla Orr, and Alexander Coppock. 2021. “Active Maintenance: A Proposal for the Long-term\nComputational Reproducibility of Scientific Results.”\nPS: Political Science & Politics 54 (3): 462–66.\n\n\nPennycook, Gordon, Ziv Epstein, Mohsen Mosleh, Antonio A. Arechar, Dean\nEckles, and David G. Rand. 2021. “Shifting Attention to Accuracy\nCan Reduce Misinformation Online.” Nature 592 (7855):\n590–95.\n\n\nPettigrew, Thomas F., and Linda R. Tropp. 2006. “A Meta-Analytic\nTest of Intergroup Contact Theory.” Journal of Personality\nand Social Psychology 90 (5): 751–83.\n\n\nPeyton, Kyle, Michael Sierra-Arévalo, and David G. Rand. 2019. “A\nField Experiment on Community Policing and Police Legitimacy.”\nProceedings of the National Academy of Sciences 116 (40):\n19894–98.\n\n\nPlümper, Thomas, Vera E. Troeger, and Eric Neumayer. 2019. “Case\nSelection and Causal Inference in Qualitative Research.” PloS\nOne 14 (7): 1–18.\n\n\nPorter, Ethan, and Yamil Velez. 2021. “Placebo Selection in Survey\nExperiments: An Agnostic Approach.” Political Analysis,\n1–14.\n\n\nPrillaman, Soledad Artiz. 2022. “Strength in Numbers: How Women’s\nGroups Close India’s Political Gender Gap.” American Journal\nof Political Science.\n\n\nReiss, Peter C, and Frank A Wolak. 2007. “Structural Econometric\nModeling: Rationales and Examples from Industrial Organization.”\nHandbook of Econometrics 6: 4277–4415.\n\n\nRevkin, Mara Redlich, and Ariel I. Ahram. 2020. “Perspectives on\nthe Rebel Social Contract: Exit, Voice, and Loyalty in the Islamic State\nin Iraq and Syria.” World Development 132: 104981.\n\n\nRubin, Donald B. 1980. “Randomization Analysis of Experimental\nData: The Fisher Randomization Test Comment.” Journal of the\nAmerican Statistical Association 75 (371): 591–93.\n\n\nRubinstein, Ariel. 1982. “Perfect Equilibrium in a Bargaining\nModel.” Econometrica: Journal of the Econometric\nSociety, 97–109.\n\n\nSamii, Cyrus. 2016. “Causal Empiricism in Quantitative\nResearch.” The Journal of Politics 78 (3): 941–55.\n\n\nSamii, Cyrus, and Peter M. Aronow. 2012. “On Equivalencies Between\nDesign-Based and Regression-Based Variance Estimators for Randomized\nExperiments.” Statistics & Probability Letters 82\n(2): 365–70.\n\n\nScacco, Alexandra, and Shana S. Warren. 2018. “Can Social Contact\nReduce Prejudice and Discrimination? Evidence from a Field Experiment in\nNigeria.” American Political Science Review 112 (3):\n654–77.\n\n\nSchwarz, Susanne, and Alexander Coppock. 2022. “What Have We Learned About Gender From Candidate Choice\nExperiments? A Meta-analysis of 67 Factorial Survey\nExperiments.” Journal of Politics 84 (2): 655–68.\n\n\nSeawright, Jason, and John Gerring. 2008. “Case Selection\nTechniques in Case Study Research: A Menu of Qualitative and\nQuantitative Options.” Political Research Quarterly 61\n(2): 294–308.\n\n\nShadish, William, Thomas D. Cook, and Donald Thomas Campbell. 2002.\nExperimental and Quasi-Experimental Designs for Generalized Causal\nInference. Boston: Houghton Mifflin.\n\n\nSinclair, Betsy, Margaret McConnell, and Donald P. Green. 2012.\n“Detecting Spillover Effects: Design and Analysis of Multilevel\nExperiments.” American Journal of Political Science 56\n(4): 1055–69.\n\n\nSkocpol, Theda. 1979. States and Social Revolutions: A Comparative\nAnalysis of France, Russia and China. Cambridge, UK: Cambridge\nUniversity Press.\n\n\nSlough, Tara. 2020. “The Ethics of Electoral Experimentation:\nDesign-Based Recommendations.” Unpublished Manuscript.\n\n\nSnyder, Jack, and Erica D. Borghard. 2011. “The Cost of Empty\nThreats: A Penny, Not a Pound.” American Political Science\nReview 105 (3): 437–56.\n\n\nStokes, Leah C. 2016. “Electoral Backlash Against Climate Policy:\nA Natural Experiment on Retrospective Voting and Local Resistance to\nPublic Policy.” American Journal of Political Science 60\n(4): 958–74.\n\n\nStokes, Susan C. 2005. “Perverse Accountability: A Formal Model of\nMachine Politics with Evidence from Argentina.” American\nPolitical Science Review 99 (3): 315–25.\n\n\nSwank, Duane. 2002. Global Capital, Political\nInstitutions, and Policy Change in Developed Welfare States.\nNew York: Cambridge University Press.\n\n\nSwire, Briony, Adam J. Berinsky, Stephan Lewandowsky, and Ullrich K. H.\nEcker. 2017. “Processing Political Misinformation: Comprehending\nthe Trump Phenomenon.” Royal Society Open Science 4 (3):\n1–21.\n\n\nTausanovitch, Chris, and Christopher Warshaw. 2013. “Measuring\nConstituent Policy Preferences in Congress, State Legislatures, and\nCities.” The Journal of Politics 75 (2): 330–42.\n\n\nTeele, Dawn. 2021. “Virtual Consent: A Bronze Standard for\nExperimental Ethics.” In Cambridge Handbook of Experimental\nPolitical Science, edited by Donald P. Green and James N. Druckman.\nNew York: Cambridge University Press.\n\n\nThistlethwaite, Donald L., and Donald T. Campbell. 1960.\n“Regression-Discontinuity Analysis: An Alternative to the Ex Post\nFacto Experiment.” Journal of Educational Psychology 51\n(6): 309.\n\n\nVan der Vaart, Aad W. 2000. Asymptotic Statistics. Vol. 3.\nCambridge university press.\n\n\nVan Evera, Stephen. 1997. Guide to Methods for Students of Political\nScience. Ithaca: Cornell University Press.\n\n\nWager, Stefan, and Susan Athey. 2018. “Estimation and Inference of\nHeterogeneous Treatment Effects Using Random Forests.”\nJournal of the American Statistical Association 113 (523):\n1228–42.\n\n\nWhite, Ariel R., Noah L. Nathan, and Julie K. Faller. 2015. “What Do I Need to Vote? Bureaucratic Discretion and\nDiscrimination by Local Election Officials.” American\nPolitical Science Review 109 (1): 129–42.\n\n\nWhite, Halbert. 1982. “Maximum Likelihood Estimation of\nMisspecified Models.” Econometrica: Journal of the\nEconometric Society 50 (1): 1–25.\n\n\nWilke, Anna M. 2021. “How Does the State Replace the Community?\nExperimental Evidence on Crime Control from South Africa.”\nUnpublished Manuscript.\n\n\nWilke, Anna M., Donald P. Green, and Jasper Cooper. 2020. “A\nPlacebo Design to Detect Spillovers from an Education–Entertainment\nExperiment in Uganda.” Journal of the Royal Statistical\nSociety: Series A (Statistics in Society) 183 (3): 1075–96.\n\n\nWilke, Anna M., and Macartan Humphreys. 2020. “Field Experiments,\nTheory, and External Validity.” SAGE Handbook of Research Methods\nin Political Science; International Relations.\n\n\nWood, Elisabeth Jean. 2006. “The Ethical\nChallenges of Field Research in Conflict Zones.”\nQualitative Sociology 29 (3): 373–86.\n\n\nYamamoto, Teppei. 2012. “Understanding the Past: Statistical\nAnalysis of Causal Attribution.” American Journal of\nPolitical Science 56 (1): 237–56.\n\n\nYao, Yuling, Aki Vehtari, Daniel Simpson, and Andrew Gelman. 2018.\n“Using Stacking to Average Bayesian Predictive Distributions (with\nDiscussion).” Bayesian Analysis 13 (3): 917–1007.\n\n\nZelizer, Adam. 2019. “Is Position-Taking Contagious? Evidence of\nCue-Taking from Two Field Experiments in a State Legislature.”\nAmerican Political Science Review 113 (2): 340–52."
  }
]